{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "19429a69-eaec-47ee-b3ad-4e150594af75",
      "metadata": {
        "id": "19429a69-eaec-47ee-b3ad-4e150594af75"
      },
      "source": [
        "# CSE5CV - Face Recognition\n",
        "In this lab we perform face recognition using PCA and KNN. We will also reuse some of the face detection work we completed in the previous face detection notebook.\n",
        "\n",
        "By the end of this lab, you should be able to:\n",
        "* Use PCA and KNN for facial recognition\n",
        "* Use both Face Detection and Face Recognition in a single pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Colab preparation\n",
        "\n",
        "Google Colab is a free online service for editing and running code in notebooks like this one. To get started, follow the steps below:\n",
        "\n",
        "1. Click the \"Copy to Drive\" button at the top of the page. This will open a new tab with the title \"Copy of...\". This is a copy of the lab notebook which is saved in your personal Google Drive. **Continue working in that copy, otherwise you will not be able to save your work**. You may close the original Colab page (the one which displays the \"Copy to Drive\" button).\n",
        "2. Run the code cell below to prepare the Colab coding environment by downloading sample files. Note that if you close this notebook and come back to work on it again later, you will need to run this cell again."
      ],
      "metadata": {
        "id": "qHrEAKS4h2Nm"
      },
      "id": "qHrEAKS4h2Nm"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ltu-cse5cv/cse5cv-labs.git\n",
        "%cd cse5cv-labs/Lab06"
      ],
      "metadata": {
        "id": "wM8UKEozh4MK"
      },
      "execution_count": null,
      "outputs": [],
      "id": "wM8UKEozh4MK"
    },
    {
      "cell_type": "markdown",
      "id": "701be84d-41f3-4bbb-86e2-cb53f2e19203",
      "metadata": {
        "id": "701be84d-41f3-4bbb-86e2-cb53f2e19203"
      },
      "source": [
        "## Packages\n",
        "In this lab we will be using the following packages:\n",
        "* *OpenCV* for face detection\n",
        "* *numpy* for interacting with image data\n",
        "* *sklearn* for PCA, KNN and metric computation\n",
        "* *matplotlib* for visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c952893-2663-4c4c-b4c3-fb036aa579e0",
      "metadata": {
        "id": "3c952893-2663-4c4c-b4c3-fb036aa579e0"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from urllib.request import urlopen"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "218558ad-021e-45f8-8a30-082040902fe7",
      "metadata": {
        "id": "218558ad-021e-45f8-8a30-082040902fe7"
      },
      "source": [
        "Refer to the `Packages` notebook for more information on packages we have used before."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bf63f74-5389-48e1-8e6a-e02bc3b61d89",
      "metadata": {
        "id": "1bf63f74-5389-48e1-8e6a-e02bc3b61d89"
      },
      "source": [
        "In this lab, we will reuse some of the functions we created previously. They are provided below for your convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b99de3c-9738-4629-9ea7-9fb6a764d228",
      "metadata": {
        "id": "1b99de3c-9738-4629-9ea7-9fb6a764d228"
      },
      "outputs": [],
      "source": [
        "# General Functions\n",
        "\n",
        "def display_image(image, title=None):\n",
        "    fig, axes = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    if image.ndim == 2:\n",
        "        axes.imshow(image, cmap='gray', vmin=0, vmax=255)\n",
        "    else:\n",
        "        axes.imshow(image)\n",
        "\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def load_image_from_url(url):\n",
        "    \"\"\"Given a URL, loads the image into a numpy\n",
        "\n",
        "    Image loaded in RGB, with HWC channel ordering\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the image to load\n",
        "\n",
        "    Returns:\n",
        "        (np.ndarray): The RGB, HWC ordered image\n",
        "    \"\"\"\n",
        "    with urlopen(url) as ur:\n",
        "        image = np.asarray(bytearray(ur.read()), dtype='uint8')\n",
        "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    return image\n",
        "\n",
        "\n",
        "def draw_rectangles_tlbr(image, rectangles, colour=(255, 0, 0), thickness=1):\n",
        "    \"\"\"Draws a list of rectangles in [tlx, tly, brx, bry] form onto an image\n",
        "\n",
        "    Args:\n",
        "        image (np.ndarray): The image to overlay rectangles on\n",
        "        rectangles (list of list/np.ndarray): A list of rectangles to overlay on the image.\n",
        "            rectangles should be in the form: [tlx, tly, brx, bry].\n",
        "        colour (3-tuple): The RGB colour to draw boxes in\n",
        "        thickness (int): The thickness of the rectangles\n",
        "\n",
        "    Returns:\n",
        "        (np.ndarray): A copy of the image with all rectangles overlaid\n",
        "    \"\"\"\n",
        "    # Copy the image to not mutate the original image\n",
        "    image = image.copy()\n",
        "\n",
        "    # Draw rectangles\n",
        "    for rectangle in rectangles:\n",
        "        tlx, tly, brx, bry = rectangle.astype(np.int32)\n",
        "        cv2.rectangle(\n",
        "            image, (tlx, tly), (brx, bry),\n",
        "            color=colour, thickness=thickness)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "# Haar Cascades\n",
        "\n",
        "def preprocess_image_haar(image):\n",
        "    return cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "\n",
        "def detect_faces_haar(image, haar_cascade_classifier, scale_factor, min_neighbours):\n",
        "    gray_image = preprocess_image_haar(image)\n",
        "    detections = haar_cascade_classifier.detectMultiScale(gray_image, scaleFactor=scale_factor, minNeighbors=min_neighbours)\n",
        "    if isinstance(detections, np.ndarray):\n",
        "        detections[:, 2] = detections[:, 0] + detections[:, 2]\n",
        "        detections[:, 3] = detections[:, 1] + detections[:, 3]\n",
        "    return detections"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ca0cff2-b23a-4520-9b7e-bb2ae2b9c6cd",
      "metadata": {
        "id": "0ca0cff2-b23a-4520-9b7e-bb2ae2b9c6cd"
      },
      "source": [
        "We will make use of a Haar cascade classifier in section 2.4 of this lab. For your convenience we provide the necessary code to create a Haar cascade classifier instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfe917e3-e6e1-4d98-8a65-b7ef5aea588e",
      "metadata": {
        "id": "cfe917e3-e6e1-4d98-8a65-b7ef5aea588e"
      },
      "outputs": [],
      "source": [
        "xml_filepath = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
        "haar_cascade_classifier = cv2.CascadeClassifier(xml_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db836c52-3d29-4b36-9885-24d2efc8fc63",
      "metadata": {
        "id": "db836c52-3d29-4b36-9885-24d2efc8fc63"
      },
      "source": [
        "# 1. Face Recognition\n",
        "Face recognition is the process of assigning an identity to a given image of a face.\n",
        "\n",
        "In this section we will look at using Principal Component Analysis (PCA) and applying the K-Nearest Neighbours (KNN) algorithm to perform face recognition in images.  \n",
        "Please refer to your lecture material for an in depth discussion on PCA."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e95e8288-18a0-48d9-a0ab-c17286e2948a",
      "metadata": {
        "id": "e95e8288-18a0-48d9-a0ab-c17286e2948a"
      },
      "source": [
        "## 1.1 Dataset\n",
        "\n",
        "To be able to perform face recognition, we will first need a labelled dataset of faces which includes their identities. This will enable us to fit our PCA and KNN algorithm to recognize images of faces on new data.\n",
        "\n",
        "Like earlier in this lab, we will make use of a subset of data from the [Labelled Faces in the Wild dataset](http://vis-www.cs.umass.edu/lfw/). This subset differs from the previous one we used in that we have many more images, the resolution of these images is larger, and that these images also contain a label representing their identity.\n",
        "\n",
        "In the code cell below we download the dataset and look at some properties of the data. Note that when downloading the dataset we specify that we want grayscale images.\n",
        "\n",
        "It may take a few moments for this code cell to run the first time you execute it, as the dataset takes some time to download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b27d14a3-e5ba-4a48-bab1-c3d88ee820b3",
      "metadata": {
        "id": "b27d14a3-e5ba-4a48-bab1-c3d88ee820b3"
      },
      "outputs": [],
      "source": [
        "lfw_people_dataset = fetch_lfw_people(min_faces_per_person=70, resize=0.4, color=False)\n",
        "print(f'The type of the dataset is: {type(lfw_people_dataset)}')\n",
        "print(f'The dataset has properties: {list(lfw_people_dataset.keys())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4310af4-5e50-4fa9-b775-a466a70f02b1",
      "metadata": {
        "id": "f4310af4-5e50-4fa9-b775-a466a70f02b1"
      },
      "source": [
        "We see above that the dataset is of type `sklearn.utils.Bunch`. This is a specific type of object that has various properties attached to it that we can access.\n",
        "\n",
        "From the print out above, we can see there are 5 different properties that this dataset has. A description of each of these is as follows:\n",
        "* `data`: A *(N, D)* *numpy* array with image data collapsed into a single dimension.\n",
        "* `images`: A *(N, H, W)* *numpy* array representing the image data\n",
        "* `target`: A *(N, )* *numpy* array representing the labels belonging to each image (We will look at this further soon)\n",
        "* `target_names`: A list of length *N* representing the name belonging to each image\n",
        "* `DESCR`: A text description of the dataset\n",
        "\n",
        "Where:\n",
        "* `N`: Represents the number of images in the dataset\n",
        "* `H`: Represents the height of images in the dataset\n",
        "* `W`: Represents the width of images in the dataset\n",
        "* `D`: Represents the number of dimensions per-image (i.e. `D` = `H` * `W`)\n",
        "\n",
        "The `data` and `images` properties currently represent pixels using values in the 0--1 range. We will now modify these values to range from 0--255 instead."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if lfw_people_dataset.data.max() <= 1:\n",
        "    lfw_people_dataset.data *= 255\n",
        "\n",
        "if lfw_people_dataset.images.max() <= 1:\n",
        "    lfw_people_dataset.images *= 255"
      ],
      "metadata": {
        "id": "IQex3K72khm1"
      },
      "id": "IQex3K72khm1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "54e45071-eb7f-4ff3-9494-10ee654fa56a",
      "metadata": {
        "id": "54e45071-eb7f-4ff3-9494-10ee654fa56a"
      },
      "source": [
        "Let's first start by printing out the `target_names` to see who is in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0133a74-1e66-4f58-b2c5-a94d46d1e3bb",
      "metadata": {
        "id": "c0133a74-1e66-4f58-b2c5-a94d46d1e3bb"
      },
      "outputs": [],
      "source": [
        "print(f'The people in our dataset are: {lfw_people_dataset.target_names}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f08d3fef-75c8-422a-b2c9-da7760f38937",
      "metadata": {
        "id": "f08d3fef-75c8-422a-b2c9-da7760f38937"
      },
      "source": [
        "It looks like there are 7 people in this dataset.  \n",
        "\n",
        "Next, let's inspect the images and target data in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9609f1ff-a951-4be6-b9f3-cbd3dd2f4008",
      "metadata": {
        "id": "9609f1ff-a951-4be6-b9f3-cbd3dd2f4008"
      },
      "outputs": [],
      "source": [
        "print(f'The shape of all images in our dataset is: {lfw_people_dataset.images.shape}')\n",
        "print(f'The shape of the targets in our dataset is: {lfw_people_dataset.target.shape}')\n",
        "print('-' * 50)\n",
        "print(f'The shape of the first image in our dataset is: {lfw_people_dataset.images[0].shape}')\n",
        "print(f'The target for the first image in our dataset is: {lfw_people_dataset.target[0]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "467ac21a-cb17-474f-b124-2756a11d39cd",
      "metadata": {
        "id": "467ac21a-cb17-474f-b124-2756a11d39cd"
      },
      "source": [
        "After running the above code cell, it should be quite clear that we have 1288 images in our dataset, with each image having a height and width of 50px and 37px respectively. That means that each image in our dataset can be flattened into a 1850 (50 * 37) dimensional vector.\n",
        "\n",
        "We can also see that the target of our first image is the integer value 5. To find the name of who this person is, we use this value as an index into the list of `target_names`.\n",
        "\n",
        "**Task**: In the code cell below, extract the first image and target value from the dataset. Convert the image to a `np.uint8` datatype, find the name associated to the target value, then display the image with the title set to the name of who is in the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ea89012-1612-4fec-b517-01c3ee025522",
      "metadata": {
        "id": "4ea89012-1612-4fec-b517-01c3ee025522"
      },
      "outputs": [],
      "source": [
        "# TODO: Extract the first image from the dataset\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Extract the target value from the dataset\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Find the name associated to the target value\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Display the image with the appropriate title\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ce192f6-d877-46e2-b208-cf974713f12d",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "8ce192f6-d877-46e2-b208-cf974713f12d"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "# TODO: Extract the first image from the dataset\n",
        "image = lfw_people_dataset.images[0]\n",
        "\n",
        "\n",
        "# TODO: Extract the target value from the dataset\n",
        "target_value = lfw_people_dataset.target[0]\n",
        "\n",
        "\n",
        "# TODO: Find the name associated to the target value\n",
        "target_name = lfw_people_dataset.target_names[target_value]\n",
        "\n",
        "\n",
        "# TODO: Display the image with the appropriate title\n",
        "display_image(image, target_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bc5c63e-1ae8-4862-942f-b7264fb94e8d",
      "metadata": {
        "id": "3bc5c63e-1ae8-4862-942f-b7264fb94e8d"
      },
      "source": [
        "## 1.2 Dimensionality Reduction with Principal Component Analysis (PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0bf6a5a-1988-43a0-b5ec-69b5b50e6857",
      "metadata": {
        "id": "f0bf6a5a-1988-43a0-b5ec-69b5b50e6857"
      },
      "source": [
        "We have our dataset, but how can we recognize who the faces belong to?\n",
        "\n",
        "As it stands, each image in our dataset is 50x37px, which when flattened, results in a 1850 dimensional vector. We could immediately try to use this vector for face recognition, however there will be a lot of redundant information in that vector, and it would be quite computationally expensive to use. A better approach would be to try reduce the dimensionality of the data, whilst preserving important information, then using the smaller data dimensionality representation for face recognition.\n",
        "\n",
        "Principal Component Analysis is well suited for performing dimensionality reduction on our data, and can do so in a fully unsupervised manner. This means it can work out how to reduce the dimensionality of our data whilst still preserving important information in a data-driven approach.\n",
        "\n",
        "This section focusses on fitting PCA to our dataset and does not discuss in detail how PCA works. Refer to your lecture notes for a detailed discussion of PCA."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a8bb4e3-92d4-4e06-bdc3-1bdd92cc8dbe",
      "metadata": {
        "id": "4a8bb4e3-92d4-4e06-bdc3-1bdd92cc8dbe"
      },
      "source": [
        "To apply PCA on our data, we first need to fit PCA to our dataset so that it can determine how to reduce the dimensionality of our data whilst preserving important information.\n",
        "\n",
        "To do this, let's first extract the set of input data and corresponding targets from our dataset. The set of input data will be an (N, D) dimensional array, and the targets will be an (N, ) dimensional vector.\n",
        "\n",
        "**Task**: Extract the inputs and targets from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c717f8e1-6170-41c9-bc9e-c8b143e32d2c",
      "metadata": {
        "id": "c717f8e1-6170-41c9-bc9e-c8b143e32d2c"
      },
      "outputs": [],
      "source": [
        "# TODO: Extract the inputs from the dataset\n",
        "# inputs = ...\n",
        "\n",
        "\n",
        "# TODO: Extract the targets from the dataset\n",
        "# targets = ...\n",
        "\n",
        "\n",
        "# Check the shape of the inputs and targets\n",
        "print(inputs.shape)        # Should be: (1288, 1850)\n",
        "print(targets.shape)       # Should be: (1288, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b57b33bb-f59a-47c5-adb8-536c69ac0ccc",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "b57b33bb-f59a-47c5-adb8-536c69ac0ccc"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "# TODO: Extract the inputs from the dataset\n",
        "inputs = lfw_people_dataset.data\n",
        "\n",
        "\n",
        "# TODO: Extract the targets from the dataset\n",
        "targets = lfw_people_dataset.target\n",
        "\n",
        "\n",
        "# Check the shape of the inputs and targets\n",
        "print(inputs.shape)\n",
        "print(targets.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64bef346-521c-45c9-a2a9-630c4e1935ac",
      "metadata": {
        "id": "64bef346-521c-45c9-a2a9-630c4e1935ac"
      },
      "source": [
        "To ensure we can properly evaluate our face recognition system, we should split our data up into a training and testing set. This is extremely common to do when training any machine learning algorithm as it ensures we can do fair evaluation on data that our algorithm has not used during training.\n",
        "\n",
        "**Task**: In the code cell below we split the inputs and targets into a training and testing set. Print the shape of each train/test input/target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "671fd845-9b11-4de1-a9aa-3f1841eb50c0",
      "metadata": {
        "id": "671fd845-9b11-4de1-a9aa-3f1841eb50c0"
      },
      "outputs": [],
      "source": [
        "# Split dataset into train/test (We will use an existing function)\n",
        "train_input, test_input, train_target, test_target = train_test_split(inputs, targets, test_size=0.25, random_state=42)\n",
        "\n",
        "# TODO: Print the shape of train/test input/target\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7627e8b2-c8cc-49d1-ac7b-ace5e87d7632",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "7627e8b2-c8cc-49d1-ac7b-ace5e87d7632"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "# Split dataset into train/test (We will use an existing function)\n",
        "train_input, test_input, train_target, test_target = train_test_split(inputs, targets, test_size=0.25, random_state=42)\n",
        "\n",
        "# TODO: Print the shape of train/test input/target\n",
        "print(train_input.shape, train_target.shape)\n",
        "print(test_input.shape, test_target.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4807e45b-55df-4d1f-a67d-28371f63bf05",
      "metadata": {
        "id": "4807e45b-55df-4d1f-a67d-28371f63bf05"
      },
      "source": [
        "You should see that we now have 966 images in our training set and 322 images in our testing set. We will fit our PCA using the 966 images in the training set.\n",
        "\n",
        "The next thing we need to do is decide how many principal components we want to reduce our data to. That is, our data currently has 1850 dimensions, so how many dimensions do we want to reduce that to.\n",
        "\n",
        "Let's choose 100. That is, we want our image data to be described by a 100 dimensional vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "732eb5b4-db06-486d-bc3c-42d69d315c9e",
      "metadata": {
        "id": "732eb5b4-db06-486d-bc3c-42d69d315c9e"
      },
      "outputs": [],
      "source": [
        "NUM_COMPONENTS = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d88d55b-82fe-4fd2-bd45-cc6ac3be39a1",
      "metadata": {
        "id": "3d88d55b-82fe-4fd2-bd45-cc6ac3be39a1"
      },
      "source": [
        "Now we know how many components we want to reduce our data to, the next step is to setup PCA and fit our training set of input data!\n",
        "\n",
        "**Task**: With reference to the [PCA documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html), create an instance of PCA, then fit it to the `train_input` data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "317ee714-a310-4416-8969-482aa09f8f24",
      "metadata": {
        "id": "317ee714-a310-4416-8969-482aa09f8f24"
      },
      "outputs": [],
      "source": [
        "# TODO: Create an instance of PCA\n",
        "# pca = ...\n",
        "\n",
        "\n",
        "# TODO: Fit PCA to the training data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e09be150-c333-4c53-9c4d-41f6bca238bf",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "e09be150-c333-4c53-9c4d-41f6bca238bf"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "# TODO: Create an instance of PCA\n",
        "pca = PCA(n_components=NUM_COMPONENTS)\n",
        "\n",
        "\n",
        "# TODO: Fit PCA to the training data\n",
        "pca.fit(train_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef2c6e8d-1166-4eaf-a9ef-4625429351af",
      "metadata": {
        "id": "ef2c6e8d-1166-4eaf-a9ef-4625429351af"
      },
      "source": [
        "What did that just do?\n",
        "\n",
        "By fitting PCA to our training dataset, our `pca` object can now reduce high dimensional image data to a relatively lower 100 dimensional vector. The value of each dimension corresponds to the contribution of each eigenface that can sum up to reproduce the original image.\n",
        "\n",
        "Let's explore this to develop a better understanding of what has occurred.\n",
        "\n",
        "In the code cell below, we display the first 12 eigenfaces from our PCA (That is, the first 12 components of our fitted PCA)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6ca9404-d3ef-42e8-ac57-86e5a6a11f4a",
      "metadata": {
        "id": "f6ca9404-d3ef-42e8-ac57-86e5a6a11f4a"
      },
      "outputs": [],
      "source": [
        "# Get the eigenfaces from components of pca (We reshape these to be (100x50x37 dimensional))\n",
        "eigenfaces = pca.components_.reshape((NUM_COMPONENTS, 50, 37))\n",
        "\n",
        "# Extract the first 12 eigenfaces\n",
        "eigenfaces = eigenfaces[:12]\n",
        "\n",
        "# Display the eigenfaces\n",
        "fig, axs = plt.subplots(3, 4, figsize=(19.2, 10.8))\n",
        "axs = np.ravel(axs)\n",
        "for idx, eigenface in enumerate(eigenfaces):\n",
        "    axs[idx].imshow(eigenface, cmap=plt.cm.gray)\n",
        "    axs[idx].set_title(f'Eigenface {idx}')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a028544c-4efd-4ee4-89c1-eb884ba772ba",
      "metadata": {
        "id": "a028544c-4efd-4ee4-89c1-eb884ba772ba"
      },
      "source": [
        "We can also use these eigenfaces to approximately reconstruct an original image.\n",
        "\n",
        "In the code cell below we extract the first image from the original dataset and transform it to the lower dimensionality space. We then attempt to reconstruct it using a number of eigenfaces. At the end of this code cell we display both the original and reconstructed images for visual comparison. Experiment with changing the number of eigenfaces used to reconstruct the image to see how many are required to produce a reasonable reconstruction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfb882dc-46fe-4215-80e9-c87122fbf3d7",
      "metadata": {
        "id": "dfb882dc-46fe-4215-80e9-c87122fbf3d7"
      },
      "outputs": [],
      "source": [
        "# Set how many eigenfaces we want to use to reconstruct the original image\n",
        "USE_N_EIGENFACES = 10\n",
        "\n",
        "# Extract the first image from the dataset\n",
        "first_image = lfw_people_dataset.images[0]\n",
        "print(f'The shape of this image is: {first_image.shape}')\n",
        "\n",
        "# Flatten the image to produce a vector, then transform it to the lower dimensionality space\n",
        "#    We reshape to (1, -1) which gives us data with dimensionality: (1, 1850)\n",
        "reduced_image = pca.transform(first_image.reshape(1, -1))\n",
        "print(f'The shape of this image with reduced dimensionality is: {reduced_image.shape}')\n",
        "\n",
        "# Extract the first N eigenvalues\n",
        "eigenvalues = reduced_image[0, :USE_N_EIGENFACES]\n",
        "\n",
        "# Print out the eigenvalues\n",
        "print(f'The eigenvalues we will use to reconstruct the image are: {eigenvalues}')\n",
        "\n",
        "# Extract the first N eigenfaces (Don't reshape as we need to combine with eigenvalues)\n",
        "eigenfaces = pca.components_[:USE_N_EIGENFACES]\n",
        "print(f'Using the first {USE_N_EIGENFACES} for image reconstruction')\n",
        "\n",
        "# Create the reconstructed image by combining eigenvalues and eigenfaces\n",
        "#    We also need to add the mean image to do this reconstruction\n",
        "reconstructed_image = np.dot(eigenvalues, eigenfaces) + pca.mean_\n",
        "reconstructed_image = reconstructed_image.reshape((50, 37))\n",
        "\n",
        "# Display the original image and reconstructed image\n",
        "display_image(first_image, 'Original Image')\n",
        "display_image(reconstructed_image, f'Reconstructed Image using first {USE_N_EIGENFACES} eigenfaces')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c5e6d9e-4589-46af-86b8-9e8eaba50eef",
      "metadata": {
        "id": "6c5e6d9e-4589-46af-86b8-9e8eaba50eef"
      },
      "source": [
        "You might see that using only the first 10 eigenvectors/eigenfaces doesn't really reproduce the image too well. As you bump this number up, you should see better resemblance between the original and reconstructed images.\n",
        "\n",
        "What is really impressive though is that now we can represent our original 50x37px images (with a total of 1850 values) by only 100 numbers!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8d27d75-0bb6-4f44-a93a-e87bfd8f48f6",
      "metadata": {
        "id": "a8d27d75-0bb6-4f44-a93a-e87bfd8f48f6"
      },
      "source": [
        "Now we have a bit of a better understanding on exactly *what* PCA did to our data, the next step is to transform all of our original training and testing data from their original dimensionality to the reduced dimensionality space.\n",
        "\n",
        "**Task**: In the below cell, transform `train_input` and `test_input` using PCA. Store these in variables `train_input_pca` and `test_input_pca` respectively. You saw an example of how we can do this in the previous code cell, alternatively refer to the PCA documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dda5296-e793-4a7e-97e6-eb711e240724",
      "metadata": {
        "id": "3dda5296-e793-4a7e-97e6-eb711e240724"
      },
      "outputs": [],
      "source": [
        "# TODO: Transform train_input and test_input into the lower dimensionality space with PCA\n",
        "\n",
        "\n",
        "\n",
        "# Test your solution\n",
        "print(train_input_pca.shape)     # Should be: (966, 100)\n",
        "print(test_input_pca.shape)      # Should be: (322, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ce40aa7-349e-422f-bc2f-2c55ff5b1504",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "6ce40aa7-349e-422f-bc2f-2c55ff5b1504"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "# TODO: Transform train_input and test_input into the lower dimensionality space with PCA\n",
        "train_input_pca = pca.transform(train_input)\n",
        "test_input_pca = pca.transform(test_input)\n",
        "\n",
        "# Test your solution\n",
        "print(train_input_pca.shape)     # Should be: (966, 100)\n",
        "print(test_input_pca.shape)      # Should be: (322, 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af7e0dcf-7756-49d9-9a2b-627a694fb717",
      "metadata": {
        "id": "af7e0dcf-7756-49d9-9a2b-627a694fb717"
      },
      "source": [
        "## 1.3 Face Recognition with K-Nearest Neighbour (KNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9e62be5-38df-4522-8d1f-a87b40edc406",
      "metadata": {
        "id": "d9e62be5-38df-4522-8d1f-a87b40edc406"
      },
      "source": [
        "We've successfully been able to reduce the dimensionality of our original image data, so the next thing we need to do is somehow use this reduced dimensionality representation to take a new image of a face and determine the identity of the face.\n",
        "\n",
        "To do this we will be using the K-Nearest Neighbour algorithm.\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Expand for KNN background</u></summary>\n",
        "\n",
        "K-Nearest Neighbour (KNN) is a machine learning classification algorithm that can classify a new piece of data based on an initial training set of data.\n",
        "    \n",
        "The process for KNN classification is:\n",
        "* Take a set of N-dimensional labelled data to initialize the algorithm\n",
        "* Given a new N-dimensional data to be classified:\n",
        "    * Compute the distance between the data to be classified and each example used to initialize the algorithm\n",
        "    * Based on the value of K (The number of neighbours), assign a classification label by voting based on the distance between the new data point and the labels of the K nearest data points\n",
        "    \n",
        "Given we used PCA to reduce the dimensionality of our dataset, our data is 100 dimensional.\n",
        "    \n",
        "You can visualize an example of KNN by looking at this useful [online KNN Demo](http://vision.stanford.edu/teaching/cs231n-demos/knn/). In this demo, the data is 2-dimensional (which enables us to visualize it on a plane).\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a224093-9e17-4c45-b236-717d864552ef",
      "metadata": {
        "id": "8a224093-9e17-4c45-b236-717d864552ef"
      },
      "source": [
        "When using KNN, we are able to choose the value of K (the number of neighbours). Let's set this to 6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79044d47-bd50-4bb6-bc6a-5e4db5a14824",
      "metadata": {
        "id": "79044d47-bd50-4bb6-bc6a-5e4db5a14824"
      },
      "outputs": [],
      "source": [
        "NUM_NEIGHBOURS = 6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fb4b7bf-92c7-4702-b660-25b7b416e0e4",
      "metadata": {
        "id": "8fb4b7bf-92c7-4702-b660-25b7b416e0e4"
      },
      "source": [
        "Now we've defined the number of neighbours we want, the next step is to setup our KNN algorithm and initialize it with our training set of input data and labels!\n",
        "\n",
        "**Task**: With reference to the [KNN documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), create an instance of KNN, then fit it to the `train_input_pca` and `train_target` data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e869c144-944f-4a68-8c66-635ec5dc1019",
      "metadata": {
        "id": "e869c144-944f-4a68-8c66-635ec5dc1019"
      },
      "outputs": [],
      "source": [
        "# TODO: Create an instance of KNN\n",
        "# knn = ...\n",
        "\n",
        "\n",
        "# TODO: Fit KNN to the training data (inputs and targets)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7e477e1-924d-4e4c-93a3-519f6aae8079",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "a7e477e1-924d-4e4c-93a3-519f6aae8079"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "# TODO: Create an instance of KNN\n",
        "knn = KNeighborsClassifier(n_neighbors=NUM_NEIGHBOURS)\n",
        "\n",
        "\n",
        "# TODO: Fit KNN to the training data (inputs and targets)\n",
        "knn.fit(train_input_pca, train_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31083600-3452-4b86-928b-7cea78750d56",
      "metadata": {
        "id": "31083600-3452-4b86-928b-7cea78750d56"
      },
      "source": [
        "Great work! It was as easy as that to setup our KNN classifier! Now it's ready for us to perform classification!\n",
        "\n",
        "Let's test this out on the first example of our dataset (In the next section we will do a proper performance evaluation)\n",
        "\n",
        "**Task**: In the code cell below, take the first reduced dimensionality example from `test_input_pca` and classify the identity of the face using your KNN classifier. Display the original image along with the predicted label and actual label. Look at the KNN documentation to see how you can use it to make a prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf6fb430-88e7-44b8-a0e2-618f7eff0028",
      "metadata": {
        "id": "bf6fb430-88e7-44b8-a0e2-618f7eff0028"
      },
      "outputs": [],
      "source": [
        "# TODO: Extract the first example from test_input_pca\n",
        "# pca_image = ...\n",
        "\n",
        "\n",
        "# TODO: Reshape your data to get it in a form usable to perform classification\n",
        "#       KNN expects a set of data to predict, so create an empty dimension\n",
        "pca_image = pca_image.reshape(1, -1)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Predict the label of this example\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Extract the original image (from test_input) and reshape it back into (50, 37)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Extract the actual label (from test_target)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Convert the predicted and actual labels into an text name (using lfw_people_dataset.target_names)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Display the original image, with the predicted label and actual label in the title\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c050dfbc-e074-48a4-8a55-1b01a67b23fc",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "c050dfbc-e074-48a4-8a55-1b01a67b23fc"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "# TODO: Extract the first example from test_input_pca\n",
        "pca_image = test_input_pca[0]\n",
        "\n",
        "\n",
        "# TODO: Reshape your data to get it in a form usable to perform classification\n",
        "#       KNN expects a set of data to predict, so create an empty dimension\n",
        "pca_image = pca_image.reshape(1, -1)\n",
        "\n",
        "\n",
        "# TODO: Predict the label of this example\n",
        "predicted_label = knn.predict(pca_image)\n",
        "\n",
        "\n",
        "# TODO: Extract the original image (from test_input) and reshape it back into (50, 37)\n",
        "original_image = test_input[0].reshape(50, 37)\n",
        "\n",
        "\n",
        "# TODO: Extract the actual label (from test_target)\n",
        "actual_label = test_target[0]\n",
        "\n",
        "\n",
        "# TODO: Convert the predicted and actual labels into an text name (using lfw_people_dataset.target_names)\n",
        "# NOTE: predicted_label is returned as a list of labels. We extract the first label (There is only 1 given we only predicted 1 image)\n",
        "predicted_label_name = lfw_people_dataset.target_names[predicted_label[0]]\n",
        "actual_label_name = lfw_people_dataset.target_names[actual_label]\n",
        "\n",
        "\n",
        "# TODO: Display the original image, with the predicted label and actual label in the title\n",
        "title = f'Prediction: {predicted_label_name}. Ground Truth: {actual_label_name}. Correct: {predicted_label_name == actual_label_name}'\n",
        "display_image(original_image, title)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3083ef8-3719-44f6-b89a-f939e564c7a3",
      "metadata": {
        "id": "c3083ef8-3719-44f6-b89a-f939e564c7a3"
      },
      "source": [
        "**Comprehension Question**\n",
        "\n",
        "You have a headshot photo of your face and want to test this face recognition system on your photo. What do you expect would happen?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "This face recognition system is only able to recognize faces that were used in the dataset to initialize KNN, so it would be impossible for it to recognize your face.  \n",
        "You would expect the KNN classifier to assign a label based on the faces that look most similar to yours from this dataset (Assuming the PCA dimensionality reduction worked well on your image).  \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9b45925-d6b3-4215-ae78-1ebf0c1cc4c0",
      "metadata": {
        "id": "f9b45925-d6b3-4215-ae78-1ebf0c1cc4c0"
      },
      "source": [
        "## 1.4 Evaluation\n",
        "If everything went well, you should have found the first face in the test set was correctly recognized.\n",
        "\n",
        "Before finishing this section on Face Recognition, it's very important that we evaluate exactly how well our PCA and KNN face recognition works on our test set! This will let us know if the system we have developed is reliable enough, or if we need to change some parts in our design.\n",
        "\n",
        "**Task**: In the code cell below, use your KNN instance to make predictions on all examples in `test_input_pca`. Store these predictions in the variable `test_prediction`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eae72a49-f82d-478b-9cb2-c1f1abcd6e76",
      "metadata": {
        "id": "eae72a49-f82d-478b-9cb2-c1f1abcd6e76"
      },
      "outputs": [],
      "source": [
        "# TODO: Make predictions on test_input_pca with your KNN instance\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e29dd872-c2b2-4de6-a110-7e878afa83fa",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "e29dd872-c2b2-4de6-a110-7e878afa83fa"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "# TODO: Make predictions on test_input_pca with your KNN instance\n",
        "test_prediction = knn.predict(test_input_pca)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eacc4552-3e1b-4055-82fa-d368e3cba126",
      "metadata": {
        "id": "eacc4552-3e1b-4055-82fa-d368e3cba126"
      },
      "source": [
        "In Lab 3 we looked at different evaluation metrics we could use to evaluate the performance of our image classification model, specifically we looked at per-class `precision`, `recall` and `f1 score`.\n",
        "\n",
        "Within `sklearn` we can produce a classification report that summarizes the precision, recall and f1 scores per-class and also gives us an averaged score for the whole dataset. Let's produce this report for our dataset!\n",
        "\n",
        "**Task**: With reference to the [*`classification_report()`* documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html), generate and print out the classification report given the predictions in `test_prediction` and ground truth in `test_target`. Make sure you specify the `target_names` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eded03b1-ead3-4f75-81e8-fd61c9ab45c8",
      "metadata": {
        "id": "eded03b1-ead3-4f75-81e8-fd61c9ab45c8"
      },
      "outputs": [],
      "source": [
        "# TODO: Generate the classification report\n",
        "# report = ...\n",
        "\n",
        "\n",
        "# TODO: Print out the report\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4017d6f2-9393-4823-a9e5-1a5e6f11c24a",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "4017d6f2-9393-4823-a9e5-1a5e6f11c24a"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "# TODO: Generate the classification report\n",
        "report = classification_report(test_target, test_prediction, target_names=lfw_people_dataset.target_names)\n",
        "\n",
        "\n",
        "# TODO: Print out the report\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d26a9ba6-c258-4379-b128-23504539938f",
      "metadata": {
        "id": "d26a9ba6-c258-4379-b128-23504539938f"
      },
      "source": [
        "You should see that the values in the report exactly match what you computed in each of the previous sections.\n",
        "\n",
        "Overall these results aren't really that great. The unweighted precision and recall of our face recognition was only *0.51* and *0.41* respectively.  \n",
        "\n",
        "Depending on what this system is being used for, these results may be acceptible, however in general you would ideally want better performance before using this system in a practical application.  \n",
        "**This is why evaluation is so important when designing any machine learning system.**\n",
        "\n",
        "We leave it up to you in `Challenge Task 1` to modify the PCA and KNN parameters to see if you can achieve better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40c9af59-da5a-4b17-8738-0dbacb9b9d8f",
      "metadata": {
        "id": "40c9af59-da5a-4b17-8738-0dbacb9b9d8f"
      },
      "source": [
        "# 2. Face Detection and Recognition with Haar Cascades, PCA and KNN\n",
        "\n",
        "To bring together what we have learned about both face detection and recognition, let's create a system that can take a whole image and detect and recognize faces within that image."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a829c8a-6a5a-43c5-8202-b9bc1bda900f",
      "metadata": {
        "id": "1a829c8a-6a5a-43c5-8202-b9bc1bda900f"
      },
      "source": [
        "## 2.1 Loading Sample Data\n",
        "\n",
        "Given our face recognition system was trained on a dataset with only 7 different identities, we need to choose an image that contains someone in that dataset.\n",
        "\n",
        "In the code cell below we load and display an image of George W. Bush."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "049751d2-e82c-43a6-8a9a-1c9780e28489",
      "metadata": {
        "id": "049751d2-e82c-43a6-8a9a-1c9780e28489"
      },
      "outputs": [],
      "source": [
        "url = 'https://upload.wikimedia.org/wikipedia/commons/c/cf/20081205_George_W_Bush_Economy.jpg'\n",
        "image = load_image_from_url(url)\n",
        "display_image(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65e4a884-e0ed-45b3-836b-f1af46444cda",
      "metadata": {
        "id": "65e4a884-e0ed-45b3-836b-f1af46444cda"
      },
      "source": [
        "## 2.2 Preprocessing\n",
        "\n",
        "We fit our face recognition system (PCA and KNN) on data contained in an existing dataset.\n",
        "\n",
        "Data in that dataset was grayscale with a size of 50x37px. If we want to reuse that face recognition system, it is important that we first preprocess any image data we want to perform image recognition on to the same form.\n",
        "\n",
        "**Task**: Write a function `preprocess_image_face_recognition` that takes an *image* as an argument (assumed to be in RGB), converts it to grayscale, resizes it to 50x37px, then returns the preprocessed image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aeb5e9e-9352-4eff-b285-fb6132af6c45",
      "metadata": {
        "id": "5aeb5e9e-9352-4eff-b285-fb6132af6c45"
      },
      "outputs": [],
      "source": [
        "# TODO: Your function here\n",
        "\n",
        "\n",
        "\n",
        "# Use this to test your function (You should see a gray image with height 50px and width 37px)\n",
        "preprocessed_image = preprocess_image_face_recognition(image)\n",
        "display_image(preprocessed_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fdea9f1-da47-4983-8d36-2754b7adebd0",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "7fdea9f1-da47-4983-8d36-2754b7adebd0"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "def preprocess_image_face_recognition(image):\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    image = cv2.resize(image, (37, 50), interpolation=cv2.INTER_AREA)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "382781ec-1ad4-441b-a3ff-1bccd21241a3",
      "metadata": {
        "id": "382781ec-1ad4-441b-a3ff-1bccd21241a3"
      },
      "source": [
        "**Comprehension Question**\n",
        "\n",
        "With reference to the visualization of the preprocessed image, what issues can you foresee when using it with our face recognition system?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "The dataset we used to fit PCA and KNN contained images of faces that only contained faces. When we preprocessed the whole image, we see that the face is only about 5-10 pixels wide and 10-15 pixels high, with no recognizable face features. This looks entirely different to all examples that were contained in our dataset.  \n",
        "    \n",
        "We cannot rely on the identity that our face recognition system assigns this image.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d925c995-7936-4227-8f1a-83fd5268fcf7",
      "metadata": {
        "id": "d925c995-7936-4227-8f1a-83fd5268fcf7"
      },
      "source": [
        "## 2.3 Recognition\n",
        "Let's now look at performing face recognition on image data using our PCA/KNN recognition system. To make things easier, we will create a function that can take an image and produce a face identity corresponding to that image.\n",
        "\n",
        "**Task**: Write a function named `recognize_face` that:\n",
        "* Takes an *image*, *pca* instance, *knn* instance and a list of *names*\n",
        "* Preprocesses the image ready for processing through the PCA/KNN recognition system\n",
        "* Reshapes the image to create an empty dimension (with *.reshape(1, -1)*)\n",
        "* Reduces the dimensionality of the preprocessed image with the *pca* instance\n",
        "* Predicts the face identity of the reduced dimensionality image using the *knn* instance\n",
        "* Converts the predicted label from *knn* into a name, using the list of *names*\n",
        "* Returns the name associated to the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcbf4a19-fb03-44a3-9a4b-267d45c30d85",
      "metadata": {
        "id": "fcbf4a19-fb03-44a3-9a4b-267d45c30d85"
      },
      "outputs": [],
      "source": [
        "# TODO: Your function here\n",
        "\n",
        "\n",
        "# Test your solution\n",
        "name = recognize_face(image, pca, knn, lfw_people_dataset.target_names)\n",
        "display_image(image, name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36f2d35d-0ec6-4080-8e6b-781a83c8aef6",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "36f2d35d-0ec6-4080-8e6b-781a83c8aef6"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "def recognize_face(image, pca, knn, target_names):\n",
        "    # Preprocess image\n",
        "    image = preprocess_image_face_recognition(image)\n",
        "\n",
        "    # Reshape\n",
        "    image = image.reshape((1, -1))\n",
        "\n",
        "    # Dimensionality reduction\n",
        "    pca_image = pca.transform(image)\n",
        "\n",
        "    # Face recognition\n",
        "    label = knn.predict(pca_image)\n",
        "\n",
        "    # Label idx -> name\n",
        "    name = target_names[label[0]]\n",
        "\n",
        "    return name"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab045ef0-8161-4444-9041-0e3b1df27d75",
      "metadata": {
        "id": "ab045ef0-8161-4444-9041-0e3b1df27d75"
      },
      "source": [
        "Looks like our system got it wrong!\n",
        "\n",
        "However, given what the input to our face recognition system looked like, it's not really all that surprising.\n",
        "\n",
        "How can we fix this so that the input to the recognition system looks better?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbe30e70-0931-44ef-9108-906c2864363a",
      "metadata": {
        "id": "dbe30e70-0931-44ef-9108-906c2864363a"
      },
      "source": [
        "## 2.4 Detection and Recognition Pipeline\n",
        "It turns out that face detection and face recognition can go hand-in-hand to create a single complete pipeline that addresses the issue we ran into above.\n",
        "\n",
        "What if instead of feeding in the whole image to our face recognition system, we first *detect* faces in the image, extract *crops* of those faces, then use our face recognition system on those face crops?\n",
        "\n",
        "Let's give this a try!\n",
        "\n",
        "**Task**: Using your `haar_cascade_classifier`, detect the faces within the `image` of George W. Bush and display the image with face detections overlaid. Use a *1.3* `scaleFactor` and *6* `minNeighbors`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9205cb31-f66c-4c4d-ba1a-55ece7bb6d33",
      "metadata": {
        "id": "9205cb31-f66c-4c4d-ba1a-55ece7bb6d33"
      },
      "outputs": [],
      "source": [
        "# TODO: Your solution here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efea7bb6-ae21-44ef-8cd8-c8ae49f5bdbc",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "efea7bb6-ae21-44ef-8cd8-c8ae49f5bdbc"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "face_detections = detect_faces_haar(image, haar_cascade_classifier, 1.3, 6)\n",
        "overlaid_image = draw_rectangles_tlbr(image, face_detections, thickness=3)\n",
        "display_image(overlaid_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14c75b6d-ab26-41bb-bde9-a8c634ef17a5",
      "metadata": {
        "id": "14c75b6d-ab26-41bb-bde9-a8c634ef17a5"
      },
      "source": [
        "So far so good! Our Haar cascade classifier was successfully able to detect the face in the image!\n",
        "\n",
        "Next, let's crop out the image of the face in preparation for recognition.\n",
        "\n",
        "**Task**: Using the detected face bounding rectangle from the previous code cell, crop out the image data bounded by that rectangle and display it (Store the cropped image in the variable `face_crop`). Refer to Lab 1 for examples of how to crop an image.\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Hint</u></summary>\n",
        "\n",
        "The bounding rectangle is in the form: top-left, width, height. You will need to transform this into the form: top-left, bottom-right to crop your image. When cropping, these coordinates **must** be integers.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f3ee2dc-88d0-4e46-acda-71ac820c38fe",
      "metadata": {
        "id": "5f3ee2dc-88d0-4e46-acda-71ac820c38fe"
      },
      "outputs": [],
      "source": [
        "# TODO: Your solution here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34545349-cc9e-414c-b07c-40275fd09a17",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "34545349-cc9e-414c-b07c-40275fd09a17"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "# Extract the face detection\n",
        "tlx, tly, brx, bry = face_detections[0]\n",
        "\n",
        "# Crop the image (Cast variables to integers to ensure integer crop coordinates)\n",
        "face_crop = image[tly:bry, tlx:brx, :]\n",
        "\n",
        "# Display the cropped image\n",
        "display_image(face_crop)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1d6fe40-3f20-462b-b7c4-f9181e59e9ec",
      "metadata": {
        "id": "e1d6fe40-3f20-462b-b7c4-f9181e59e9ec"
      },
      "source": [
        "This is looking better! Before performing recognition, let's check what the preprocessed image looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e165c995-6704-4a64-b2cc-736c88582d4e",
      "metadata": {
        "tags": [],
        "id": "e165c995-6704-4a64-b2cc-736c88582d4e"
      },
      "outputs": [],
      "source": [
        "preprocessed_image = preprocess_image_face_recognition(face_crop)\n",
        "display_image(preprocessed_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb00ec6d-6fbd-4d2b-800a-e0ac24a5b60e",
      "metadata": {
        "id": "bb00ec6d-6fbd-4d2b-800a-e0ac24a5b60e"
      },
      "source": [
        "This looks infinitely better than the preprocessed whole image! We can actually make out face features in this crop, and now this crop looks much more similar to examples in our dataset.\n",
        "\n",
        "Let's try recognize the face from this crop!\n",
        "\n",
        "**Task**: Recognize the face in the `face_crop` image and display the `face_crop` image with the title set to the name of the recognized face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dccfe3c-ea9c-4908-943e-5ea2e7ed45ad",
      "metadata": {
        "id": "8dccfe3c-ea9c-4908-943e-5ea2e7ed45ad"
      },
      "outputs": [],
      "source": [
        "# TODO: Your solution here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52168282-f6a9-4e79-bd46-d90c076f96ca",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "52168282-f6a9-4e79-bd46-d90c076f96ca"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "name = recognize_face(face_crop, pca, knn, lfw_people_dataset.target_names)\n",
        "display_image(face_crop, name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cfcab90-a11e-4d02-9f9f-41d52a251594",
      "metadata": {
        "id": "1cfcab90-a11e-4d02-9f9f-41d52a251594"
      },
      "source": [
        "If everything worked well, you should see that our face recognition system has now successfully recognized this face as George W. Bush!\n",
        "\n",
        "This is a great example to show how we can combine multiple computer vision applications together in a single pipeline to produce results that we otherwise would not be able to achieve."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5901d98f-a749-4a8e-96cb-0bead3e2367d",
      "metadata": {
        "id": "5901d98f-a749-4a8e-96cb-0bead3e2367d"
      },
      "source": [
        "# 3. Additional Tasks\n",
        "\n",
        "This section contains additional tasks that you should complete within this lab to further your understanding of face detection and recognition."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f964313-4698-43f0-ab8b-b194e52eb57f",
      "metadata": {
        "id": "2f964313-4698-43f0-ab8b-b194e52eb57f"
      },
      "source": [
        "## 3.1 Multiple Faces Example\n",
        "As another example for you to pull everything together again, let's try recognize faces in an image with multiple people.\n",
        "\n",
        "**Task**: In the code cell below:\n",
        "* Load the image from the given URL (George W. Bush is on the left, Donald Rumsfeld is on the right)\n",
        "* Display the image\n",
        "* Detect, overlay and display all faces in the image using your Haar cascade classifier with *1.3* `scaleFactor` and *6* `minNeighbors` *(Refer to Lab 5 - Section 2.4 - Classification)*\n",
        "* For every detected face:\n",
        "    * Crop out the face *(Refer to Section 2.4 - Detection and Recognition Pipeline)*\n",
        "    * Use the face recognition system to determine the name belonging to the face *(Refer to Section 2.3 - Recognition)*\n",
        "    * Display the face along with the name of who is in the image\n",
        "    \n",
        "If you do this successfully, you should see George W. Bush misrecognized as Colin Powell and Donald Rumsfeld correctly recognized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60dd454b-6b99-41c9-b8a6-7669fa9f4853",
      "metadata": {
        "id": "60dd454b-6b99-41c9-b8a6-7669fa9f4853"
      },
      "outputs": [],
      "source": [
        "url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/01/US_Navy_061108-F-5586B-137_President_George_W._Bush_looks_on_as_Secretary_of_Defense_Donald_H._Rumsfeld_addresses_the_nation_during_a_news_conference_from_the_Oval_Office.jpg/1280px-thumbnail.jpg'\n",
        "\n",
        "# TODO: Your solution here\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d57e1deb-a2f4-4f40-b8af-2cbc42e0fd08",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "d57e1deb-a2f4-4f40-b8af-2cbc42e0fd08"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/01/US_Navy_061108-F-5586B-137_President_George_W._Bush_looks_on_as_Secretary_of_Defense_Donald_H._Rumsfeld_addresses_the_nation_during_a_news_conference_from_the_Oval_Office.jpg/1280px-thumbnail.jpg'\n",
        "\n",
        "# Load and display the image\n",
        "image = load_image_from_url(url)\n",
        "display_image(image)\n",
        "\n",
        "# Detect, overlay and display faces in the image\n",
        "face_detections = detect_faces_haar(image, haar_cascade_classifier, 1.3, 6)\n",
        "overlaid_image = draw_rectangles_tlbr(image, face_detections, thickness=3)\n",
        "display_image(overlaid_image)\n",
        "\n",
        "# Iterate through all face detections\n",
        "for tlx, tly, brx, bry in face_detections:\n",
        "    face_crop = image[tly:bry, tlx:brx, :]\n",
        "\n",
        "    # Determine the name belonging to the face\n",
        "    name = recognize_face(face_crop, pca, knn, lfw_people_dataset.target_names)\n",
        "\n",
        "    # Display the face along with the recognized name\n",
        "    display_image(face_crop, name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72847826-a77f-4ce2-b480-ad77f57368f8",
      "metadata": {
        "id": "72847826-a77f-4ce2-b480-ad77f57368f8"
      },
      "source": [
        "## 3.2 PCA Dimensions and K Neighbours Experimentation\n",
        "In sections 1.2 and 1.3 of this lab, we arbitrarily chose to reduce the dataset to 100 dimensions and use 6 nearest neighbours for classification.\n",
        "\n",
        "Your task is to experiment with changing the number of dimensions and nearest neighbours and evaluate how this impacts the recognition accuracy on the dataset.\n",
        "\n",
        "Summarize your results in a table with columns for `PCA dimensions`, `K-Neighbours` and the macro avg `precision`, `recall` and `f1 score` (We generated this table in section 1.4). What combination gave you the best results?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "576f1814-08e0-4396-8733-1849d600e431",
      "metadata": {
        "id": "576f1814-08e0-4396-8733-1849d600e431"
      },
      "source": [
        "# 4. Challenge Tasks\n",
        "These tasks are meant to help pull together everything you have covered in this lab or extend on other exercises previously covered. It is highly recommended that you give these tasks a go, but only try to once you've finished the Lab Exercises section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd535baa-e284-41f0-8d75-35237e8815ed",
      "metadata": {
        "id": "dd535baa-e284-41f0-8d75-35237e8815ed"
      },
      "source": [
        "## Challenge 1 - Creating Your Own Face Recognition System\n",
        "This challenge task should be seen as a potential idea for a project to further develop your understanding of the content in this lab. It is expected that this project could take multiple days to complete.\n",
        "\n",
        "We saw in the Face Recognition section of this lab how we could take a dataset of cropped faces and using PCA for dimensionality reduction and KNN for classification, we could create a facial recognition system. Unfortunately, this system can only recognize 7 different faces.\n",
        "\n",
        "Your task is to:\n",
        "* Create your own image recognition task (e.g. to recognize your friends or members of your family)\n",
        "* Construct a custom dataset of face images of those in your image recognition task\n",
        "    * You can make use of the face detection approaches we have seen in this lab to take crops of faces from a larger image\n",
        "* Using the PCA that was fit to the dataset in this lab:\n",
        "    * Use PCA to reduce the dimensionality of your custom dataset\n",
        "    * Fit your own KNN algorithm using your custom dataset\n",
        "    * Evaluate the performance on your custom dataset\n",
        "* Fit PCA to your custom dataset\n",
        "    * Fit your own KNN algorithm using your custom dataset\n",
        "    * Evaluate the performance on your custom dataset\n",
        "\n",
        "You're likely to find that if you fit PCA to your custom dataset the results are better. This is expected if in general, the faces in your dataset look different than the faces in the dataset we explored in this lab.\n",
        "\n",
        "Once you've done this, you will have a system that is able to recognize faces of those people who were in your custom dataset.\n",
        "\n",
        "To extend this even further, you could write code to:\n",
        "* Take an image\n",
        "* Detect the faces in the image\n",
        "* Crop out the faces and perform face recognition\n",
        "* Overlay the face detection boxes and the name of the person who was recognized onto the original image\n",
        "* Display the overlaid image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58c18baf-1d90-418b-9364-9081480e724f",
      "metadata": {
        "id": "58c18baf-1d90-418b-9364-9081480e724f"
      },
      "source": [
        "# Summary\n",
        "In this lab we looked into face recognition using PCA and KNN, and saw how we could combine both face detection and face recognition into a single pipeline."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "toc-autonumbering": false,
    "toc-showcode": false,
    "toc-showmarkdowntxt": false,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}