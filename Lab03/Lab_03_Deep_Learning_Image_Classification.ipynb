{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "19429a69-eaec-47ee-b3ad-4e150594af75",
      "metadata": {
        "id": "19429a69-eaec-47ee-b3ad-4e150594af75"
      },
      "source": [
        "# CSE5CV - Deep Learning Image Classification\n",
        "In this lab we classify image data using a pretrained Convolutional Neural Network (CNN).\n",
        "\n",
        "By the end of this lab, you should be able to:\n",
        "* Classify image data using a pretrained CNN\n",
        "* Understand how to interpret the output of a CNN\n",
        "* Implement and interpret various evaluation metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Colab preparation\n",
        "\n",
        "Google Colab is a free online service for editing and running code in notebooks like this one. To get started, follow the steps below:\n",
        "\n",
        "1. Click the \"Copy to Drive\" button at the top of the page. This will open a new tab with the title \"Copy of...\". This is a copy of the lab notebook which is saved in your personal Google Drive. **Continue working in that copy, otherwise you will not be able to save your work**. You may close the original Colab page (the one which displays the \"Copy to Drive\" button).\n",
        "2. Run the code cell below to prepare the Colab coding environment by downloading sample files. Note that if you close this notebook and come back to work on it again later, you will need to run this cell again."
      ],
      "metadata": {
        "id": "qHrEAKS4h2Nm"
      },
      "id": "qHrEAKS4h2Nm"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ltu-cse5cv/cse5cv-labs.git\n",
        "%cd cse5cv-labs/Lab03"
      ],
      "metadata": {
        "id": "wM8UKEozh4MK"
      },
      "execution_count": null,
      "outputs": [],
      "id": "wM8UKEozh4MK"
    },
    {
      "cell_type": "markdown",
      "id": "701be84d-41f3-4bbb-86e2-cb53f2e19203",
      "metadata": {
        "id": "701be84d-41f3-4bbb-86e2-cb53f2e19203"
      },
      "source": [
        "## Packages\n",
        "In this lab we will be using the following packages:\n",
        "* *PyTorch* to work with deep learning models\n",
        "* *Torchvision* to download pre-trained models and apply transformations to image data\n",
        "* *Scikit-learn* to compute various evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c952893-2663-4c4c-b4c3-fb036aa579e0",
      "metadata": {
        "id": "3c952893-2663-4c4c-b4c3-fb036aa579e0"
      },
      "outputs": [],
      "source": [
        "# Packages\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms.functional as tvtf\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, ConfusionMatrixDisplay, accuracy_score,\n",
        "    recall_score, precision_score, f1_score)\n",
        "from urllib.request import urlopen"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdb4dabd-37c0-47c0-8f73-3444ceb69d92",
      "metadata": {
        "tags": [],
        "id": "cdb4dabd-37c0-47c0-8f73-3444ceb69d92"
      },
      "source": [
        "### PyTorch\n",
        "PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.\n",
        "\n",
        "Package Homepage: https://pytorch.org/    \n",
        "Python Documentation (latest): https://pytorch.org/docs/stable/index.html\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>More Details</u></summary>\n",
        "\n",
        "We will be making extensive use of *PyTorch* to:  \n",
        "\n",
        "- Create Tensors (Like *numpy* arrays, but can be used with the CPU or GPU)\n",
        "- Work with Deep Learning models (performing forward passes through those models)\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21c6e686-a2a6-413d-9ca6-5ff94c4bca73",
      "metadata": {
        "id": "21c6e686-a2a6-413d-9ca6-5ff94c4bca73"
      },
      "source": [
        "### Torchvision\n",
        "This library is part of the PyTorch project. PyTorch is an open source machine learning framework. The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision.\n",
        "\n",
        "Python Documentation (latest): https://pytorch.org/vision/stable/index.html  \n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>More Details</u></summary>\n",
        "\n",
        "\n",
        "We will be making use of *torchvision* to:  \n",
        "\n",
        "- Download pre-trained models (These are Deep Learning models that have already been defined and trained on other datasets)\n",
        "- Apply transformations to our image data\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38b4c0cd-49c6-40e1-a411-551081a1c902",
      "metadata": {
        "id": "38b4c0cd-49c6-40e1-a411-551081a1c902"
      },
      "source": [
        "### Scikit-learn\n",
        "Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities.\n",
        "\n",
        "Package Homepage: https://scikit-learn.org/stable/    \n",
        "Python Documentation (latest): https://scikit-learn.org/stable/modules/classes.html\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>More Details</u></summary>\n",
        "\n",
        "We will be making use of *Scikit-learn* to compute evaluation metrics on the predictions of our neural network. Using scikit-learn means we do not need to write the evaluation computation code ourselves.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b9e9a6c-f992-4e65-86b1-dd6351b0174b",
      "metadata": {
        "id": "2b9e9a6c-f992-4e65-86b1-dd6351b0174b"
      },
      "source": [
        "## CPU vs. GPU\n",
        "\n",
        "A Central Processing Unit (CPU) is a piece of hardware in every computer that handles interpreting instructions and performing processing operations. On a typical machine, the CPU would have around 4-8 cores, meaning it can process up to 4-8 things in parallel.\n",
        "\n",
        "A Graphics Processing Unit (GPU) is a piece of hardware that handles rendering of graphics. Not every computer will have a GPU as the CPU can also handle these tasks. GPUs are designed with parallelism in mind, and as such have a huge number of cores. For example, the NVIDIA GTX 1080 GPU has 2560 cores (supporting a lot of parallelism!)\n",
        "\n",
        "It turns out that performing forward and backward passes through a neural network can be greatly optimized by parallelizing the operations that take place. This is something perfectly suited for a GPU!\n",
        "\n",
        "The *PyTorch* package supports both CPU and GPU for training and evaluating deep learning models. We will be using the CPU only for these labs. The great thing about *PyTorch* is that the code you write works for both the CPU and GPU, and to change between the two you simply need to tell *PyTorch* to put data on the respective ***device***."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "998f3da6-5876-4a03-a5df-d1e9aec49672",
      "metadata": {
        "id": "998f3da6-5876-4a03-a5df-d1e9aec49672"
      },
      "source": [
        "# 1. Classification\n",
        "\n",
        "In this section we will develop code to classify an image using a Convolutional Neural Network (CNN)!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "098ce237-f8d2-4d85-8e68-107a8ebdb56c",
      "metadata": {
        "id": "098ce237-f8d2-4d85-8e68-107a8ebdb56c"
      },
      "source": [
        "## 1.1 Pre-Trained Network\n",
        "\n",
        "To train a neural network for classification, we need a huge amount of training data from all of the different classes we want our network to predict.\n",
        "\n",
        "For this lab, this presents two problems:\n",
        "* How can we get access to so much training data?\n",
        "* How can we train the network, given we likely don't have access to a GPU, nor do we have the time to train something.\n",
        "\n",
        "Luckily for us the *torchvision* package has our back!  \n",
        "\n",
        "The *torchvision* package consists of popular datasets, model architectures, and common image transformations for computer vision. With *torchvision*, we can very easily choose a popular model architecture and download model weights that have been already been trained on a large dataset.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2366ca86-a9a7-4f9a-8b3b-f8cc574f04a8",
      "metadata": {
        "id": "2366ca86-a9a7-4f9a-8b3b-f8cc574f04a8"
      },
      "source": [
        "### Downloading a Pre-Trained Network\n",
        "You can see the torchvision documentation for a [list of available model architectures](https://pytorch.org/vision/stable/models.html). In this lab we are looking at performing classification, and to get us started we will work with the [MobileNetV3 Small](https://arxiv.org/abs/1905.02244) architecture.  \n",
        "\n",
        "The classification models available through *torchvision* have been trained on the [ImageNet dataset](https://image-net.org/). This is a massive dataset that contains a huge number of images across 1000 different classes.\n",
        "\n",
        "In the cell below we create a pretrained MobileNetV3 model. If you are interested, you can find more information in the torchvision documentation on [creating pretrained models](https://pytorch.org/vision/stable/models.html#:~:text=We%20provide%20pre-trained%20models%2C%20using%20the%20PyTorch%20torch.utils.model_zoo.%20These%20can%20be%20constructed%20by%20passing%20pretrained%3DTrue%3A). It may take a few moments the first time you run this cell as *torchvision* needs to download the trained weights to your machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95826326-df4e-43f3-8489-44ad10de60bf",
      "metadata": {
        "id": "95826326-df4e-43f3-8489-44ad10de60bf"
      },
      "outputs": [],
      "source": [
        "mobilenet_v3 = models.mobilenet_v3_small(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3898ee3-5d53-4508-895c-60179527f165",
      "metadata": {
        "id": "a3898ee3-5d53-4508-895c-60179527f165"
      },
      "source": [
        "### Inspecting the Pre-Trained Network\n",
        "Models downloaded with *torchvision* are written using *PyTorch*. They consist of many different *PyTorch* layers (e.g. Convolution, Linear, Pooling, etc.) connected together to create a pipeline.\n",
        "\n",
        "You have performed convolutions in earlier labs with manually defined kernels. The basic idea of a Convolutional Neural Network is that it *learns* good kernels to extract image features and classify images at the same time. Of course that's not all there is to it.\n",
        "\n",
        "In the cell below we print out the layers used in our MobileNetV3 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22f5b698-c10c-4561-958b-f5e850ddd3bc",
      "metadata": {
        "id": "22f5b698-c10c-4561-958b-f5e850ddd3bc"
      },
      "outputs": [],
      "source": [
        "print(mobilenet_v3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d903aab6-85d7-4255-92dd-20cab3e461d8",
      "metadata": {
        "id": "d903aab6-85d7-4255-92dd-20cab3e461d8"
      },
      "source": [
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Expand for Discussion</u></summary>\n",
        "\n",
        "As you can see in the printed summary there are quite a number of layers in the network.\n",
        "  \n",
        "The inner-most layers that you can see relate directly to a layer you can create in *PyTorch*. The names of these are self-explanatory to what type of layer they are (for example, a Conv2D layer is a 2D convolution layer). You can take a look at the [*PyTorch* *nn* documentation] (https://pytorch.org/docs/stable/nn.html) for a detailed description of each type of layer/activation.\n",
        "\n",
        "You'll see that the printed summary also shows some of the properties of each layer. To briefly discuss the first Conv2D layer in the printed summary:  \n",
        "`Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)`\n",
        "* The first number (3) describes the expected number if input channels/feature maps. This is set to 3 (Usually the first Conv layer in a network expects 3 channels - RGB!)\n",
        "* The second number (16) describes how many feature maps will be generated.\n",
        "* The *kernel_size* argument (3, 3) describes the spatial size of the convolution kernel, here it is a 3x3 kernel.\n",
        "* The *stride* argument (2, 2) describes how the kernel moves across the input.\n",
        "* The *padding* argument (1, 1) describes the padding applied around the input.\n",
        "* The *bias* argument (False) describes if the layer should also include a bias parameter.\n",
        "\n",
        "You can always refer back to the [*nn* documentation](https://pytorch.org/docs/stable/nn.html) for a full description of what arguments can be given to each layer.  \n",
        "    \n",
        "Something else that is good to be aware of - If we take a look at the MobileNetV3 Small table in the [MobileNetV3 paper](https://arxiv.org/abs/1905.02244) which describes the layers in the network (see below), we can see that the layers in the table match the layers that are printed in the summary. This helps give us some reassurance that the *torchvision* implementation does match what the paper describes.  \n",
        "    \n",
        "See if you can find the relationships between the printed model summary and the model as described in the paper.\n",
        "    \n",
        "![MobileNet V3 Small Architecture.png](attachment:8ed85952-76bc-4925-84a5-c953decf4d01.png)    \n",
        "  \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dca280de-3a63-457c-adf0-295210ffdb28",
      "metadata": {
        "id": "dca280de-3a63-457c-adf0-295210ffdb28"
      },
      "source": [
        "## 1.2 Tensors\n",
        "Up until now we have stored our image data in *numpy* **arrays**. However, *PyTorch* models only accept **tensors**.\n",
        "\n",
        "> A PyTorch Tensor is basically the same as a numpy array: it does not know anything about deep learning or computational graphs or gradients, and is just a generic n-dimensional array to be used for arbitrary numeric computation.  \n",
        "The biggest difference between a numpy array and a PyTorch Tensor is that a PyTorch Tensor can run on either CPU or GPU.\n",
        "\n",
        "From: https://pytorch.org/tutorials/beginner/examples_tensor/two_layer_net_tensor.html\n",
        "\n",
        "The *PyTorch* package has functions that let us switch between *PyTorch* **tensors** and *numpy* **ndarrays**.\n",
        "\n",
        "**An important note on tensors**: *PyTorch* expects the dimensions of tensors to be ordered as CHW (C = channels, H = height, W = width). This is different from *numpy* where we used HWC ordering."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "970f9851-7ce5-4633-9ef7-68ea058adbb7",
      "metadata": {
        "id": "970f9851-7ce5-4633-9ef7-68ea058adbb7"
      },
      "source": [
        "### Creating Tensors\n",
        "We are able to create tensors directly through *PyTorch*.\n",
        "\n",
        "In the code cell below, we create a 3x244x244 tensor filled with random values in the range \\[0, 1) using the [*`torch.rand()`* function](https://pytorch.org/docs/stable/generated/torch.rand.html), then print out some properties of the tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e41e1184-5de9-4163-995e-58a3072c2770",
      "metadata": {
        "id": "e41e1184-5de9-4163-995e-58a3072c2770"
      },
      "outputs": [],
      "source": [
        "# Create the random tensor, specifying a float32 datatype\n",
        "random_tensor = torch.rand([3, 224, 224], dtype=torch.float32)\n",
        "\n",
        "# Print some properties of the tensor\n",
        "print(f'The type of our tensor is: {type(random_tensor)}')\n",
        "print('=' * 50)\n",
        "print(f'The shape of our tensor is: {random_tensor.shape}')\n",
        "print('=' * 50)\n",
        "print(f'The datatype of our tensor is: {random_tensor.dtype}')\n",
        "print('=' * 50)\n",
        "print(f'Our tensor is on the: {random_tensor.device} device')\n",
        "print('=' * 50)\n",
        "print(f'The minimum value is: {random_tensor.min()}')\n",
        "print(f'The minimum value is: {random_tensor.max()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a809557-c7c9-462a-84f4-b8397d2802ee",
      "metadata": {
        "id": "6a809557-c7c9-462a-84f4-b8397d2802ee"
      },
      "source": [
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Expand for Discussion</u></summary>\n",
        "\n",
        "You may notice that the properties of our tensor that we accessed are essentially the same as what we have done in the past with a *numpy* array. The *type* of our tensor is a *torch.Tensor* as expected. The *shape* and *datatype* are also exactly as we expect, given we specified them when creating the tensor.\n",
        "\n",
        "A property that we haven't come across before with *numpy* arrays is the *device* property. Tensors can be placed on the CPU or GPU, and the way we can tell which one our tensor is on is through the *device* property. For this lab we will be working exclusively with the CPU.\n",
        "\n",
        "We also printed out the minimum and maximum values of the tensor. You should be able to see that the minimum/maximum values are very close to 0/1 respectively.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f8a2e71-2735-4dd5-97f4-8a87448f35f2",
      "metadata": {
        "id": "1f8a2e71-2735-4dd5-97f4-8a87448f35f2"
      },
      "source": [
        "### Tensors from Numpy Arrays\n",
        "It's also possible to create a tensor directly from a numpy array. This is very useful when we load our image data into a *numpy* array and later want to do something with it with *PyTorch*.\n",
        "\n",
        "In the code cell below, we create a *numpy* array of random values and convert it to a tensor using the [*`torch.as_tensor()`* function](https://pytorch.org/docs/stable/generated/torch.as_tensor.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e85b8f1a-0435-453e-90cc-23ee49cdf0ca",
      "metadata": {
        "id": "e85b8f1a-0435-453e-90cc-23ee49cdf0ca"
      },
      "outputs": [],
      "source": [
        "# Create a random numpy ndarray with dimensionality 3x224x224\n",
        "random_ndarray = np.random.rand(3, 224, 224)\n",
        "\n",
        "# Convert the ndarray to a torch tensor\n",
        "converted_torch = torch.as_tensor(random_ndarray)\n",
        "\n",
        "# Validate the types are different\n",
        "print(type(converted_torch))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6407d29b-751e-4d09-93a5-e24a2fcb142c",
      "metadata": {
        "id": "6407d29b-751e-4d09-93a5-e24a2fcb142c"
      },
      "source": [
        "We mentioned above that we represent image data with dimensionality HWC in a *numpy* array, but as CHW in a *PyTorch* tensor.\n",
        "\n",
        "To swap the ordering of our *numpy* array we can use the [*`np.transpose()`* function](https://numpy.org/doc/stable/reference/generated/numpy.transpose.html). An example is provided for you here:\n",
        "\n",
        "```\n",
        "my_array = np.random.rand(3, 4, 5)\n",
        "print(my_array.shape)\n",
        ">>> (3, 4, 5)\n",
        "transposed_array = np.transpose(my_array, (2, 0, 1))\n",
        "print(transposed_array.shape)\n",
        ">>> (5, 3, 4)\n",
        "```\n",
        "\n",
        "**Task**: Create a 200x100x3 (HWC) *numpy* array filled with random values, transpose it to get CHW dimensionality, then convert it to a tensor.\n",
        "\n",
        "At the bottom of the code cell is some code that will print out the shape of your resulting tensor. Before moving on, verify that your tensor has shape (3, 100, 200) and the datatype is *torch.Tensor*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af01b1e5-726b-4b28-85f3-410793e33606",
      "metadata": {
        "id": "af01b1e5-726b-4b28-85f3-410793e33606"
      },
      "outputs": [],
      "source": [
        "# TODO: Create a random numpy array with shape: (200, 100, 3)\n",
        "# random_ndarray = ...\n",
        "\n",
        "# TODO: Transpose the numpy array to get dimensionality: (3, 100, 200)\n",
        "# transposed_ndarray = ...\n",
        "\n",
        "# TODO: Convert the numpy array to a torch tensor\n",
        "# converted_torch = ...\n",
        "\n",
        "# Test your code!\n",
        "print(converted_torch.shape)       # Expected: (3, 100, 200)\n",
        "print(type(converted_torch))       # Expected: torch.Tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task solution"
      ],
      "metadata": {
        "id": "1iKZFHNjagLG"
      },
      "id": "1iKZFHNjagLG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a58aa63f-4ded-442e-8a30-d174197958dd",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "a58aa63f-4ded-442e-8a30-d174197958dd"
      },
      "outputs": [],
      "source": [
        "# TODO: Create a random numpy array with shape: (200, 100, 3)\n",
        "random_ndarray = np.random.rand(200, 100, 3)\n",
        "\n",
        "# TODO: Transpose the numpy array to get dimensionality: (3, 100, 200)\n",
        "transposed_ndarray = np.transpose(random_ndarray, (2, 1, 0))\n",
        "\n",
        "# TODO: Convert the numpy array to a torch tensor\n",
        "converted_torch = torch.as_tensor(transposed_ndarray)\n",
        "\n",
        "# Test your code!\n",
        "print(converted_torch.shape)       # Expected: (3, 100, 200)\n",
        "print(type(converted_torch))       # Expected: torch.Tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7d24d28-2513-4e42-8d33-ad346db47e56",
      "metadata": {
        "id": "f7d24d28-2513-4e42-8d33-ad346db47e56"
      },
      "source": [
        "### Numpy Arrays from Tensors\n",
        "We often need to convert tensors back into numpy arrays. For example, for visualisation using `matplotlib`.\n",
        "\n",
        "In the code cell below, we create a tensor of random values and convert it to a *numpy* array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba5d14a3-1b34-45d9-b222-21196d8a9e9d",
      "metadata": {
        "tags": [],
        "id": "ba5d14a3-1b34-45d9-b222-21196d8a9e9d"
      },
      "outputs": [],
      "source": [
        "# Create a random tensor with dimensionality 3x224x224\n",
        "random_tensor = torch.rand([3, 224, 224], dtype=torch.float32)\n",
        "\n",
        "# Convert the tensor to an ndarray\n",
        "# NOTE: Calling .detach() is important when we are using a tensor that has been passed through a neural network\n",
        "#       Calling .cpu() is important incase our tensor is on the GPU (numpy arrays can only be on the CPU)\n",
        "#       In general we will leave both of these calls in as part of conversion\n",
        "converted_ndarray = random_tensor.detach().cpu().numpy()\n",
        "\n",
        "# Validate the type has changed\n",
        "print(type(converted_ndarray))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ed293d9-ee3e-4bf6-ba7d-c4830f7f5208",
      "metadata": {
        "id": "4ed293d9-ee3e-4bf6-ba7d-c4830f7f5208"
      },
      "source": [
        "### Displaying Tensors\n",
        "As we saw in previous labs, it's extremely useful to be able to visualize our image data.\n",
        "\n",
        "We saw in Lab 1 that we can use *matplotlib* to display image data. However *matplotlib* cannot display data stored in tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ab6c738-4051-4c2e-9ddb-bd3f1cb930f2",
      "metadata": {
        "id": "1ab6c738-4051-4c2e-9ddb-bd3f1cb930f2"
      },
      "source": [
        "To consolidate what you have seen so far in this lab, and to reuse some of the code you have written in previous labs, let's write a function to handle displaying a tensor.\n",
        "\n",
        "**Task**: Write a function named *display_tensor* that:\n",
        "* Takes an image stored in a tensor\n",
        "* Converts the tensor to a *numpy* array\n",
        "* Transposes the numpy array to get channel ordering HWC (from CHW)\n",
        "* Displays the image using *matplotlib* *(Refer to the code you wrote in Lab 1. Do not worry about handling grayscale)*\n",
        "\n",
        "At the bottom of the code cell is some code that will call your display function. Use this to verify that the tensor is displayed correctly (The tensor is filled with random pixels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e94dccb-a27f-44af-8cfb-47ce14e31d2f",
      "metadata": {
        "id": "5e94dccb-a27f-44af-8cfb-47ce14e31d2f"
      },
      "outputs": [],
      "source": [
        "# TODO Write your function here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Test your function!\n",
        "random_tensor = torch.rand([3, 224, 224], dtype=torch.float32)\n",
        "display_tensor(random_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task solution"
      ],
      "metadata": {
        "id": "7wm-zCX7avMy"
      },
      "id": "7wm-zCX7avMy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d99c444-02d4-43b1-bba2-d6eb465cdd15",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "5d99c444-02d4-43b1-bba2-d6eb465cdd15"
      },
      "outputs": [],
      "source": [
        "def display_tensor(tensor):\n",
        "    # Convert to numpy ndarray\n",
        "    tensor_as_numpy = tensor.detach().cpu().numpy()\n",
        "\n",
        "    # Transpose from CHW to HWC\n",
        "    tensor_as_numpy = np.transpose(tensor_as_numpy, (1, 2, 0))\n",
        "\n",
        "    # Display the image\n",
        "    fig, axes = plt.subplots(figsize=(12, 8))\n",
        "    axes.imshow(tensor_as_numpy)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ce99d24-5d38-4cb5-98a6-ac6965699f41",
      "metadata": {
        "id": "4ce99d24-5d38-4cb5-98a6-ac6965699f41"
      },
      "source": [
        "### Batching Tensors\n",
        "The final thing to be aware of when dealing with tensors is that when we want to pass them through a neural network, we do so with *batches* of tensors. That is, instead of only passing tensors through our network 1 by 1, *PyTorch* explicitly expects us to pass a collection of tensors through a network at once.  \n",
        "\n",
        "When we batch tensors together, we end up with a single tensor that has an extra **B**atch dimension. This means the expected dimensionality of a tensor for a neural network is: **B**x**C**x**H**x**W**.\n",
        "\n",
        "There are two cases we might come across that require us to batch up data into a tensor for passing through a neural network:\n",
        "- We have a single tensor and we need to create a batch dimension (The size of the batch dimension is 1). Here we make use of the [*`unsqueeze()`* function](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html).\n",
        "- We have *N* tensors and we need to combine them into a single tensor (The size of the batch is *N*). Here we make use of the [*`stack()`* function](https://pytorch.org/docs/stable/generated/torch.stack.html).\n",
        "\n",
        "The code below shows how we handle each case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db743d46-5a7b-42bf-a954-5ba70851d5c6",
      "metadata": {
        "tags": [],
        "id": "db743d46-5a7b-42bf-a954-5ba70851d5c6"
      },
      "outputs": [],
      "source": [
        "# Case 1 - We have a single tensor and need to create a batch dimension\n",
        "random_tensor = torch.rand([3, 224, 224], dtype=torch.float32)\n",
        "print(random_tensor.shape)     # 3, 224, 224\n",
        "\n",
        "# Insert a dimension of size 1\n",
        "batched_tensor = random_tensor.unsqueeze(dim=0)\n",
        "print(batched_tensor.shape)\n",
        "# NOTE: We can also do this with: batched_tensor = torch.unsqueeze(random_tensor, dim=0)\n",
        "\n",
        "print('-' * 50)\n",
        "\n",
        "# Case 2 - We have multiple tensors and want to combine them into a single tensor along the batch dimension\n",
        "random_tensor_a = torch.rand([3, 224, 224], dtype=torch.float32)\n",
        "random_tensor_b = torch.rand([3, 224, 224], dtype=torch.float32)\n",
        "print(random_tensor_a.shape, random_tensor_b.shape)\n",
        "\n",
        "# Stack the tensors\n",
        "batched_tensor = torch.stack([random_tensor_a, random_tensor_b], dim=0)\n",
        "print(batched_tensor.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5359ab4a-d77f-438f-b837-533ff7e841d5",
      "metadata": {
        "id": "5359ab4a-d77f-438f-b837-533ff7e841d5"
      },
      "source": [
        "## 1.3 Making Predictions\n",
        "That was a lot of work! But now we know:\n",
        "- How to create a pretrained network (We have chosen MobileNet V3)\n",
        "- How to create random tensors, and importantly, how to put them into a batch\n",
        "\n",
        "All that's left is passing some data through the network and looking at the classification!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e3d1912-19b7-4656-9136-5614dafb5670",
      "metadata": {
        "id": "1e3d1912-19b7-4656-9136-5614dafb5670"
      },
      "source": [
        "### The Forward Pass\n",
        "In this lab we are interested in taking a pretrained model, passing some data through that network, then analyzing the classification output of the network. Because of this, we are only interested in the **forward pass** of the network.\n",
        "\n",
        "The code below shows how to pass data through a neural network (Using the pretrained MobileNet V3 we loaded earlier and a tensor filled with random values)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35c1053b-f3d0-49a1-a4ef-b647dc9e30dc",
      "metadata": {
        "id": "35c1053b-f3d0-49a1-a4ef-b647dc9e30dc"
      },
      "outputs": [],
      "source": [
        "# Define a random tensor (including a batch dimension)\n",
        "random_tensor = torch.rand([1, 3, 224, 224], dtype=torch.float32)\n",
        "\n",
        "# Perform the forward pass through our mobilenet_v3 model\n",
        "with torch.no_grad():\n",
        "    output = mobilenet_v3(random_tensor)\n",
        "\n",
        "# Inspect the output!\n",
        "print(f'The type of the output is: {type(output)}')\n",
        "print(f'The shape of the output is: {output.shape}')\n",
        "print(f'The output data is:\\n\\t {output}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7397bb9f-453e-43d0-b07b-df305ec4193c",
      "metadata": {
        "id": "7397bb9f-453e-43d0-b07b-df305ec4193c"
      },
      "source": [
        "As you can see from the code cell above, performing the forward pass through our neural network is actually quite simple! Once our model is created (and stored in a variable), we can use it like a function, where we use our batched tensor as the argument. Pay attention to the `with torch.no_grad()` context manager. We use this around the forward pass of our model when we are not interested in computing gradients (we compute gradients when we are training a network). For your interest, you can read more in the [*`torch.no_grad()`* documentation](https://pytorch.org/docs/stable/generated/torch.no_grad.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "312ebaf7-6c76-4487-9924-667263218a90",
      "metadata": {
        "id": "312ebaf7-6c76-4487-9924-667263218a90"
      },
      "source": [
        "#### 1000D Output\n",
        "Something that might not be immediately clear is what the output of our network represents. We know it is some form of prediction. After running that code cell you should have seen a huge tensor filled with seemingly random numbers being printed as the output (with dimensionality 1x1000).  \n",
        "\n",
        "Let's start by talking about the dimensionality of the data.  \n",
        "The first dimension (of size 1) represents the batch dimension. Given we only had a single tensor in our batch, the size of this dimension is 1. If we had *N* tensors in our batch, then this dimension would be *N*.  \n",
        "\n",
        "When we talked about the pretrained model at the start of this lab, we mentioned that the model was pretrained on ImageNet which consisted of 1000 different classes. This is no coincidence that it exactly matches the size of the second dimension!  \n",
        "The second batch dimension (of size 1000) corresponds to a score *per-class* based on the data we trained the model on. This means to assign the input to a class, we just need to determine which index into the 1000 dimensional vector produced the highest score.  \n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Why should it be the highest?</u></summary>\n",
        "\n",
        "Why not the lowest?  \n",
        "\n",
        "When training a neural network it is convention that the goal is for the network to output a higher score for the correct class and we must match how the model was trained.\n",
        "</details>\n",
        "\n",
        "The final question is then, if we know the index of the 1000 dimensional vector that produced the highest score, how do we map that back to the name of a class?  \n",
        "The answer is that we need to lookup the name of the class corresponding to that index!\n",
        "\n",
        "The below cell contains the definition for a dictionary containing the class index and corresponding label for the ImageNet dataset. We will use this dictionary to help us map from class index producing highest score to class label. Have a brief look through the classes then run the code cell to define the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a233963d-7c8c-492d-b7c5-034049b008a4",
      "metadata": {
        "tags": [],
        "id": "a233963d-7c8c-492d-b7c5-034049b008a4"
      },
      "outputs": [],
      "source": [
        "# Classes were taken from: https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n",
        "IMAGENET_CLASSES = {\n",
        "    0: 'tench, Tinca tinca',\n",
        "    1: 'goldfish, Carassius auratus',\n",
        "    2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n",
        "    3: 'tiger shark, Galeocerdo cuvieri',\n",
        "    4: 'hammerhead, hammerhead shark',\n",
        "    5: 'electric ray, crampfish, numbfish, torpedo',\n",
        "    6: 'stingray',\n",
        "    7: 'cock',\n",
        "    8: 'hen',\n",
        "    9: 'ostrich, Struthio camelus',\n",
        "    10: 'brambling, Fringilla montifringilla',\n",
        "    11: 'goldfinch, Carduelis carduelis',\n",
        "    12: 'house finch, linnet, Carpodacus mexicanus',\n",
        "    13: 'junco, snowbird',\n",
        "    14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n",
        "    15: 'robin, American robin, Turdus migratorius',\n",
        "    16: 'bulbul',\n",
        "    17: 'jay',\n",
        "    18: 'magpie',\n",
        "    19: 'chickadee',\n",
        "    20: 'water ouzel, dipper',\n",
        "    21: 'kite',\n",
        "    22: 'bald eagle, American eagle, Haliaeetus leucocephalus',\n",
        "    23: 'vulture',\n",
        "    24: 'great grey owl, great gray owl, Strix nebulosa',\n",
        "    25: 'European fire salamander, Salamandra salamandra',\n",
        "    26: 'common newt, Triturus vulgaris',\n",
        "    27: 'eft',\n",
        "    28: 'spotted salamander, Ambystoma maculatum',\n",
        "    29: 'axolotl, mud puppy, Ambystoma mexicanum',\n",
        "    30: 'bullfrog, Rana catesbeiana',\n",
        "    31: 'tree frog, tree-frog',\n",
        "    32: 'tailed frog, bell toad, ribbed toad, tailed toad, Ascaphus trui',\n",
        "    33: 'loggerhead, loggerhead turtle, Caretta caretta',\n",
        "    34: 'leatherback turtle, leatherback, leathery turtle, Dermochelys coriacea',\n",
        "    35: 'mud turtle',\n",
        "    36: 'terrapin',\n",
        "    37: 'box turtle, box tortoise',\n",
        "    38: 'banded gecko',\n",
        "    39: 'common iguana, iguana, Iguana iguana',\n",
        "    40: 'American chameleon, anole, Anolis carolinensis',\n",
        "    41: 'whiptail, whiptail lizard',\n",
        "    42: 'agama',\n",
        "    43: 'frilled lizard, Chlamydosaurus kingi',\n",
        "    44: 'alligator lizard',\n",
        "    45: 'Gila monster, Heloderma suspectum',\n",
        "    46: 'green lizard, Lacerta viridis',\n",
        "    47: 'African chameleon, Chamaeleo chamaeleon',\n",
        "    48: 'Komodo dragon, Komodo lizard, dragon lizard, giant lizard, Varanus komodoensis',\n",
        "    49: 'African crocodile, Nile crocodile, Crocodylus niloticus',\n",
        "    50: 'American alligator, Alligator mississipiensis',\n",
        "    51: 'triceratops',\n",
        "    52: 'thunder snake, worm snake, Carphophis amoenus',\n",
        "    53: 'ringneck snake, ring-necked snake, ring snake',\n",
        "    54: 'hognose snake, puff adder, sand viper',\n",
        "    55: 'green snake, grass snake',\n",
        "    56: 'king snake, kingsnake',\n",
        "    57: 'garter snake, grass snake',\n",
        "    58: 'water snake',\n",
        "    59: 'vine snake',\n",
        "    60: 'night snake, Hypsiglena torquata',\n",
        "    61: 'boa constrictor, Constrictor constrictor',\n",
        "    62: 'rock python, rock snake, Python sebae',\n",
        "    63: 'Indian cobra, Naja naja',\n",
        "    64: 'green mamba',\n",
        "    65: 'sea snake',\n",
        "    66: 'horned viper, cerastes, sand viper, horned asp, Cerastes cornutus',\n",
        "    67: 'diamondback, diamondback rattlesnake, Crotalus adamanteus',\n",
        "    68: 'sidewinder, horned rattlesnake, Crotalus cerastes',\n",
        "    69: 'trilobite',\n",
        "    70: 'harvestman, daddy longlegs, Phalangium opilio',\n",
        "    71: 'scorpion',\n",
        "    72: 'black and gold garden spider, Argiope aurantia',\n",
        "    73: 'barn spider, Araneus cavaticus',\n",
        "    74: 'garden spider, Aranea diademata',\n",
        "    75: 'black widow, Latrodectus mactans',\n",
        "    76: 'tarantula',\n",
        "    77: 'wolf spider, hunting spider',\n",
        "    78: 'tick',\n",
        "    79: 'centipede',\n",
        "    80: 'black grouse',\n",
        "    81: 'ptarmigan',\n",
        "    82: 'ruffed grouse, partridge, Bonasa umbellus',\n",
        "    83: 'prairie chicken, prairie grouse, prairie fowl',\n",
        "    84: 'peacock',\n",
        "    85: 'quail',\n",
        "    86: 'partridge',\n",
        "    87: 'African grey, African gray, Psittacus erithacus',\n",
        "    88: 'macaw',\n",
        "    89: 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n",
        "    90: 'lorikeet',\n",
        "    91: 'coucal',\n",
        "    92: 'bee eater',\n",
        "    93: 'hornbill',\n",
        "    94: 'hummingbird',\n",
        "    95: 'jacamar',\n",
        "    96: 'toucan',\n",
        "    97: 'drake',\n",
        "    98: 'red-breasted merganser, Mergus serrator',\n",
        "    99: 'goose',\n",
        "    100: 'black swan, Cygnus atratus',\n",
        "    101: 'tusker',\n",
        "    102: 'echidna, spiny anteater, anteater',\n",
        "    103: 'platypus, duckbill, duckbilled platypus, duck-billed platypus, Ornithorhynchus anatinus',\n",
        "    104: 'wallaby, brush kangaroo',\n",
        "    105: 'koala, koala bear, kangaroo bear, native bear, Phascolarctos cinereus',\n",
        "    106: 'wombat',\n",
        "    107: 'jellyfish',\n",
        "    108: 'sea anemone, anemone',\n",
        "    109: 'brain coral',\n",
        "    110: 'flatworm, platyhelminth',\n",
        "    111: 'nematode, nematode worm, roundworm',\n",
        "    112: 'conch',\n",
        "    113: 'snail',\n",
        "    114: 'slug',\n",
        "    115: 'sea slug, nudibranch',\n",
        "    116: 'chiton, coat-of-mail shell, sea cradle, polyplacophore',\n",
        "    117: 'chambered nautilus, pearly nautilus, nautilus',\n",
        "    118: 'Dungeness crab, Cancer magister',\n",
        "    119: 'rock crab, Cancer irroratus',\n",
        "    120: 'fiddler crab',\n",
        "    121: 'king crab, Alaska crab, Alaskan king crab, Alaska king crab, Paralithodes camtschatica',\n",
        "    122: 'American lobster, Northern lobster, Maine lobster, Homarus americanus',\n",
        "    123: 'spiny lobster, langouste, rock lobster, crawfish, crayfish, sea crawfish',\n",
        "    124: 'crayfish, crawfish, crawdad, crawdaddy',\n",
        "    125: 'hermit crab',\n",
        "    126: 'isopod',\n",
        "    127: 'white stork, Ciconia ciconia',\n",
        "    128: 'black stork, Ciconia nigra',\n",
        "    129: 'spoonbill',\n",
        "    130: 'flamingo',\n",
        "    131: 'little blue heron, Egretta caerulea',\n",
        "    132: 'American egret, great white heron, Egretta albus',\n",
        "    133: 'bittern',\n",
        "    134: 'crane',\n",
        "    135: 'limpkin, Aramus pictus',\n",
        "    136: 'European gallinule, Porphyrio porphyrio',\n",
        "    137: 'American coot, marsh hen, mud hen, water hen, Fulica americana',\n",
        "    138: 'bustard',\n",
        "    139: 'ruddy turnstone, Arenaria interpres',\n",
        "    140: 'red-backed sandpiper, dunlin, Erolia alpina',\n",
        "    141: 'redshank, Tringa totanus',\n",
        "    142: 'dowitcher',\n",
        "    143: 'oystercatcher, oyster catcher',\n",
        "    144: 'pelican',\n",
        "    145: 'king penguin, Aptenodytes patagonica',\n",
        "    146: 'albatross, mollymawk',\n",
        "    147: 'grey whale, gray whale, devilfish, Eschrichtius gibbosus, Eschrichtius robustus',\n",
        "    148: 'killer whale, killer, orca, grampus, sea wolf, Orcinus orca',\n",
        "    149: 'dugong, Dugong dugon',\n",
        "    150: 'sea lion',\n",
        "    151: 'Chihuahua',\n",
        "    152: 'Japanese spaniel',\n",
        "    153: 'Maltese dog, Maltese terrier, Maltese',\n",
        "    154: 'Pekinese, Pekingese, Peke',\n",
        "    155: 'Shih-Tzu',\n",
        "    156: 'Blenheim spaniel',\n",
        "    157: 'papillon',\n",
        "    158: 'toy terrier',\n",
        "    159: 'Rhodesian ridgeback',\n",
        "    160: 'Afghan hound, Afghan',\n",
        "    161: 'basset, basset hound',\n",
        "    162: 'beagle',\n",
        "    163: 'bloodhound, sleuthhound',\n",
        "    164: 'bluetick',\n",
        "    165: 'black-and-tan coonhound',\n",
        "    166: 'Walker hound, Walker foxhound',\n",
        "    167: 'English foxhound',\n",
        "    168: 'redbone',\n",
        "    169: 'borzoi, Russian wolfhound',\n",
        "    170: 'Irish wolfhound',\n",
        "    171: 'Italian greyhound',\n",
        "    172: 'whippet',\n",
        "    173: 'Ibizan hound, Ibizan Podenco',\n",
        "    174: 'Norwegian elkhound, elkhound',\n",
        "    175: 'otterhound, otter hound',\n",
        "    176: 'Saluki, gazelle hound',\n",
        "    177: 'Scottish deerhound, deerhound',\n",
        "    178: 'Weimaraner',\n",
        "    179: 'Staffordshire bullterrier, Staffordshire bull terrier',\n",
        "    180: 'American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier',\n",
        "    181: 'Bedlington terrier',\n",
        "    182: 'Border terrier',\n",
        "    183: 'Kerry blue terrier',\n",
        "    184: 'Irish terrier',\n",
        "    185: 'Norfolk terrier',\n",
        "    186: 'Norwich terrier',\n",
        "    187: 'Yorkshire terrier',\n",
        "    188: 'wire-haired fox terrier',\n",
        "    189: 'Lakeland terrier',\n",
        "    190: 'Sealyham terrier, Sealyham',\n",
        "    191: 'Airedale, Airedale terrier',\n",
        "    192: 'cairn, cairn terrier',\n",
        "    193: 'Australian terrier',\n",
        "    194: 'Dandie Dinmont, Dandie Dinmont terrier',\n",
        "    195: 'Boston bull, Boston terrier',\n",
        "    196: 'miniature schnauzer',\n",
        "    197: 'giant schnauzer',\n",
        "    198: 'standard schnauzer',\n",
        "    199: 'Scotch terrier, Scottish terrier, Scottie',\n",
        "    200: 'Tibetan terrier, chrysanthemum dog',\n",
        "    201: 'silky terrier, Sydney silky',\n",
        "    202: 'soft-coated wheaten terrier',\n",
        "    203: 'West Highland white terrier',\n",
        "    204: 'Lhasa, Lhasa apso',\n",
        "    205: 'flat-coated retriever',\n",
        "    206: 'curly-coated retriever',\n",
        "    207: 'golden retriever',\n",
        "    208: 'Labrador retriever',\n",
        "    209: 'Chesapeake Bay retriever',\n",
        "    210: 'German short-haired pointer',\n",
        "    211: 'vizsla, Hungarian pointer',\n",
        "    212: 'English setter',\n",
        "    213: 'Irish setter, red setter',\n",
        "    214: 'Gordon setter',\n",
        "    215: 'Brittany spaniel',\n",
        "    216: 'clumber, clumber spaniel',\n",
        "    217: 'English springer, English springer spaniel',\n",
        "    218: 'Welsh springer spaniel',\n",
        "    219: 'cocker spaniel, English cocker spaniel, cocker',\n",
        "    220: 'Sussex spaniel',\n",
        "    221: 'Irish water spaniel',\n",
        "    222: 'kuvasz',\n",
        "    223: 'schipperke',\n",
        "    224: 'groenendael',\n",
        "    225: 'malinois',\n",
        "    226: 'briard',\n",
        "    227: 'kelpie',\n",
        "    228: 'komondor',\n",
        "    229: 'Old English sheepdog, bobtail',\n",
        "    230: 'Shetland sheepdog, Shetland sheep dog, Shetland',\n",
        "    231: 'collie',\n",
        "    232: 'Border collie',\n",
        "    233: 'Bouvier des Flandres, Bouviers des Flandres',\n",
        "    234: 'Rottweiler',\n",
        "    235: 'German shepherd, German shepherd dog, German police dog, alsatian',\n",
        "    236: 'Doberman, Doberman pinscher',\n",
        "    237: 'miniature pinscher',\n",
        "    238: 'Greater Swiss Mountain dog',\n",
        "    239: 'Bernese mountain dog',\n",
        "    240: 'Appenzeller',\n",
        "    241: 'EntleBucher',\n",
        "    242: 'boxer',\n",
        "    243: 'bull mastiff',\n",
        "    244: 'Tibetan mastiff',\n",
        "    245: 'French bulldog',\n",
        "    246: 'Great Dane',\n",
        "    247: 'Saint Bernard, St Bernard',\n",
        "    248: 'Eskimo dog, husky',\n",
        "    249: 'malamute, malemute, Alaskan malamute',\n",
        "    250: 'Siberian husky',\n",
        "    251: 'dalmatian, coach dog, carriage dog',\n",
        "    252: 'affenpinscher, monkey pinscher, monkey dog',\n",
        "    253: 'basenji',\n",
        "    254: 'pug, pug-dog',\n",
        "    255: 'Leonberg',\n",
        "    256: 'Newfoundland, Newfoundland dog',\n",
        "    257: 'Great Pyrenees',\n",
        "    258: 'Samoyed, Samoyede',\n",
        "    259: 'Pomeranian',\n",
        "    260: 'chow, chow chow',\n",
        "    261: 'keeshond',\n",
        "    262: 'Brabancon griffon',\n",
        "    263: 'Pembroke, Pembroke Welsh corgi',\n",
        "    264: 'Cardigan, Cardigan Welsh corgi',\n",
        "    265: 'toy poodle',\n",
        "    266: 'miniature poodle',\n",
        "    267: 'standard poodle',\n",
        "    268: 'Mexican hairless',\n",
        "    269: 'timber wolf, grey wolf, gray wolf, Canis lupus',\n",
        "    270: 'white wolf, Arctic wolf, Canis lupus tundrarum',\n",
        "    271: 'red wolf, maned wolf, Canis rufus, Canis niger',\n",
        "    272: 'coyote, prairie wolf, brush wolf, Canis latrans',\n",
        "    273: 'dingo, warrigal, warragal, Canis dingo',\n",
        "    274: 'dhole, Cuon alpinus',\n",
        "    275: 'African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus',\n",
        "    276: 'hyena, hyaena',\n",
        "    277: 'red fox, Vulpes vulpes',\n",
        "    278: 'kit fox, Vulpes macrotis',\n",
        "    279: 'Arctic fox, white fox, Alopex lagopus',\n",
        "    280: 'grey fox, gray fox, Urocyon cinereoargenteus',\n",
        "    281: 'tabby, tabby cat',\n",
        "    282: 'tiger cat',\n",
        "    283: 'Persian cat',\n",
        "    284: 'Siamese cat, Siamese',\n",
        "    285: 'Egyptian cat',\n",
        "    286: 'cougar, puma, catamount, mountain lion, painter, panther, Felis concolor',\n",
        "    287: 'lynx, catamount',\n",
        "    288: 'leopard, Panthera pardus',\n",
        "    289: 'snow leopard, ounce, Panthera uncia',\n",
        "    290: 'jaguar, panther, Panthera onca, Felis onca',\n",
        "    291: 'lion, king of beasts, Panthera leo',\n",
        "    292: 'tiger, Panthera tigris',\n",
        "    293: 'cheetah, chetah, Acinonyx jubatus',\n",
        "    294: 'brown bear, bruin, Ursus arctos',\n",
        "    295: 'American black bear, black bear, Ursus americanus, Euarctos americanus',\n",
        "    296: 'ice bear, polar bear, Ursus Maritimus, Thalarctos maritimus',\n",
        "    297: 'sloth bear, Melursus ursinus, Ursus ursinus',\n",
        "    298: 'mongoose',\n",
        "    299: 'meerkat, mierkat',\n",
        "    300: 'tiger beetle',\n",
        "    301: 'ladybug, ladybeetle, lady beetle, ladybird, ladybird beetle',\n",
        "    302: 'ground beetle, carabid beetle',\n",
        "    303: 'long-horned beetle, longicorn, longicorn beetle',\n",
        "    304: 'leaf beetle, chrysomelid',\n",
        "    305: 'dung beetle',\n",
        "    306: 'rhinoceros beetle',\n",
        "    307: 'weevil',\n",
        "    308: 'fly',\n",
        "    309: 'bee',\n",
        "    310: 'ant, emmet, pismire',\n",
        "    311: 'grasshopper, hopper',\n",
        "    312: 'cricket',\n",
        "    313: 'walking stick, walkingstick, stick insect',\n",
        "    314: 'cockroach, roach',\n",
        "    315: 'mantis, mantid',\n",
        "    316: 'cicada, cicala',\n",
        "    317: 'leafhopper',\n",
        "    318: 'lacewing, lacewing fly',\n",
        "    319: \"dragonfly, darning needle, devil's darning needle, sewing needle, snake feeder, snake doctor, mosquito hawk, skeeter hawk\",\n",
        "    320: 'damselfly',\n",
        "    321: 'admiral',\n",
        "    322: 'ringlet, ringlet butterfly',\n",
        "    323: 'monarch, monarch butterfly, milkweed butterfly, Danaus plexippus',\n",
        "    324: 'cabbage butterfly',\n",
        "    325: 'sulphur butterfly, sulfur butterfly',\n",
        "    326: 'lycaenid, lycaenid butterfly',\n",
        "    327: 'starfish, sea star',\n",
        "    328: 'sea urchin',\n",
        "    329: 'sea cucumber, holothurian',\n",
        "    330: 'wood rabbit, cottontail, cottontail rabbit',\n",
        "    331: 'hare',\n",
        "    332: 'Angora, Angora rabbit',\n",
        "    333: 'hamster',\n",
        "    334: 'porcupine, hedgehog',\n",
        "    335: 'fox squirrel, eastern fox squirrel, Sciurus niger',\n",
        "    336: 'marmot',\n",
        "    337: 'beaver',\n",
        "    338: 'guinea pig, Cavia cobaya',\n",
        "    339: 'sorrel',\n",
        "    340: 'zebra',\n",
        "    341: 'hog, pig, grunter, squealer, Sus scrofa',\n",
        "    342: 'wild boar, boar, Sus scrofa',\n",
        "    343: 'warthog',\n",
        "    344: 'hippopotamus, hippo, river horse, Hippopotamus amphibius',\n",
        "    345: 'ox',\n",
        "    346: 'water buffalo, water ox, Asiatic buffalo, Bubalus bubalis',\n",
        "    347: 'bison',\n",
        "    348: 'ram, tup',\n",
        "    349: 'bighorn, bighorn sheep, cimarron, Rocky Mountain bighorn, Rocky Mountain sheep, Ovis canadensis',\n",
        "    350: 'ibex, Capra ibex',\n",
        "    351: 'hartebeest',\n",
        "    352: 'impala, Aepyceros melampus',\n",
        "    353: 'gazelle',\n",
        "    354: 'Arabian camel, dromedary, Camelus dromedarius',\n",
        "    355: 'llama',\n",
        "    356: 'weasel',\n",
        "    357: 'mink',\n",
        "    358: 'polecat, fitch, foulmart, foumart, Mustela putorius',\n",
        "    359: 'black-footed ferret, ferret, Mustela nigripes',\n",
        "    360: 'otter',\n",
        "    361: 'skunk, polecat, wood pussy',\n",
        "    362: 'badger',\n",
        "    363: 'armadillo',\n",
        "    364: 'three-toed sloth, ai, Bradypus tridactylus',\n",
        "    365: 'orangutan, orang, orangutang, Pongo pygmaeus',\n",
        "    366: 'gorilla, Gorilla gorilla',\n",
        "    367: 'chimpanzee, chimp, Pan troglodytes',\n",
        "    368: 'gibbon, Hylobates lar',\n",
        "    369: 'siamang, Hylobates syndactylus, Symphalangus syndactylus',\n",
        "    370: 'guenon, guenon monkey',\n",
        "    371: 'patas, hussar monkey, Erythrocebus patas',\n",
        "    372: 'baboon',\n",
        "    373: 'macaque',\n",
        "    374: 'langur',\n",
        "    375: 'colobus, colobus monkey',\n",
        "    376: 'proboscis monkey, Nasalis larvatus',\n",
        "    377: 'marmoset',\n",
        "    378: 'capuchin, ringtail, Cebus capucinus',\n",
        "    379: 'howler monkey, howler',\n",
        "    380: 'titi, titi monkey',\n",
        "    381: 'spider monkey, Ateles geoffroyi',\n",
        "    382: 'squirrel monkey, Saimiri sciureus',\n",
        "    383: 'Madagascar cat, ring-tailed lemur, Lemur catta',\n",
        "    384: 'indri, indris, Indri indri, Indri brevicaudatus',\n",
        "    385: 'Indian elephant, Elephas maximus',\n",
        "    386: 'African elephant, Loxodonta africana',\n",
        "    387: 'lesser panda, red panda, panda, bear cat, cat bear, Ailurus fulgens',\n",
        "    388: 'giant panda, panda, panda bear, coon bear, Ailuropoda melanoleuca',\n",
        "    389: 'barracouta, snoek',\n",
        "    390: 'eel',\n",
        "    391: 'coho, cohoe, coho salmon, blue jack, silver salmon, Oncorhynchus kisutch',\n",
        "    392: 'rock beauty, Holocanthus tricolor',\n",
        "    393: 'anemone fish',\n",
        "    394: 'sturgeon',\n",
        "    395: 'gar, garfish, garpike, billfish, Lepisosteus osseus',\n",
        "    396: 'lionfish',\n",
        "    397: 'puffer, pufferfish, blowfish, globefish',\n",
        "    398: 'abacus',\n",
        "    399: 'abaya',\n",
        "    400: \"academic gown, academic robe, judge's robe\",\n",
        "    401: 'accordion, piano accordion, squeeze box',\n",
        "    402: 'acoustic guitar',\n",
        "    403: 'aircraft carrier, carrier, flattop, attack aircraft carrier',\n",
        "    404: 'airliner',\n",
        "    405: 'airship, dirigible',\n",
        "    406: 'altar',\n",
        "    407: 'ambulance',\n",
        "    408: 'amphibian, amphibious vehicle',\n",
        "    409: 'analog clock',\n",
        "    410: 'apiary, bee house',\n",
        "    411: 'apron',\n",
        "    412: 'ashcan, trash can, garbage can, wastebin, ash bin, ash-bin, ashbin, dustbin, trash barrel, trash bin',\n",
        "    413: 'assault rifle, assault gun',\n",
        "    414: 'backpack, back pack, knapsack, packsack, rucksack, haversack',\n",
        "    415: 'bakery, bakeshop, bakehouse',\n",
        "    416: 'balance beam, beam',\n",
        "    417: 'balloon',\n",
        "    418: 'ballpoint, ballpoint pen, ballpen, Biro',\n",
        "    419: 'Band Aid',\n",
        "    420: 'banjo',\n",
        "    421: 'bannister, banister, balustrade, balusters, handrail',\n",
        "    422: 'barbell',\n",
        "    423: 'barber chair',\n",
        "    424: 'barbershop',\n",
        "    425: 'barn',\n",
        "    426: 'barometer',\n",
        "    427: 'barrel, cask',\n",
        "    428: 'barrow, garden cart, lawn cart, wheelbarrow',\n",
        "    429: 'baseball',\n",
        "    430: 'basketball',\n",
        "    431: 'bassinet',\n",
        "    432: 'bassoon',\n",
        "    433: 'bathing cap, swimming cap',\n",
        "    434: 'bath towel',\n",
        "    435: 'bathtub, bathing tub, bath, tub',\n",
        "    436: 'beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon',\n",
        "    437: 'beacon, lighthouse, beacon light, pharos',\n",
        "    438: 'beaker',\n",
        "    439: 'bearskin, busby, shako',\n",
        "    440: 'beer bottle',\n",
        "    441: 'beer glass',\n",
        "    442: 'bell cote, bell cot',\n",
        "    443: 'bib',\n",
        "    444: 'bicycle-built-for-two, tandem bicycle, tandem',\n",
        "    445: 'bikini, two-piece',\n",
        "    446: 'binder, ring-binder',\n",
        "    447: 'binoculars, field glasses, opera glasses',\n",
        "    448: 'birdhouse',\n",
        "    449: 'boathouse',\n",
        "    450: 'bobsled, bobsleigh, bob',\n",
        "    451: 'bolo tie, bolo, bola tie, bola',\n",
        "    452: 'bonnet, poke bonnet',\n",
        "    453: 'bookcase',\n",
        "    454: 'bookshop, bookstore, bookstall',\n",
        "    455: 'bottlecap',\n",
        "    456: 'bow',\n",
        "    457: 'bow tie, bow-tie, bowtie',\n",
        "    458: 'brass, memorial tablet, plaque',\n",
        "    459: 'brassiere, bra, bandeau',\n",
        "    460: 'breakwater, groin, groyne, mole, bulwark, seawall, jetty',\n",
        "    461: 'breastplate, aegis, egis',\n",
        "    462: 'broom',\n",
        "    463: 'bucket, pail',\n",
        "    464: 'buckle',\n",
        "    465: 'bulletproof vest',\n",
        "    466: 'bullet train, bullet',\n",
        "    467: 'butcher shop, meat market',\n",
        "    468: 'cab, hack, taxi, taxicab',\n",
        "    469: 'caldron, cauldron',\n",
        "    470: 'candle, taper, wax light',\n",
        "    471: 'cannon',\n",
        "    472: 'canoe',\n",
        "    473: 'can opener, tin opener',\n",
        "    474: 'cardigan',\n",
        "    475: 'car mirror',\n",
        "    476: 'carousel, carrousel, merry-go-round, roundabout, whirligig',\n",
        "    477: \"carpenter's kit, tool kit\",\n",
        "    478: 'carton',\n",
        "    479: 'car wheel',\n",
        "    480: 'cash machine, cash dispenser, automated teller machine, automatic teller machine, automated teller, automatic teller, ATM',\n",
        "    481: 'cassette',\n",
        "    482: 'cassette player',\n",
        "    483: 'castle',\n",
        "    484: 'catamaran',\n",
        "    485: 'CD player',\n",
        "    486: 'cello, violoncello',\n",
        "    487: 'cellular telephone, cellular phone, cellphone, cell, mobile phone',\n",
        "    488: 'chain',\n",
        "    489: 'chainlink fence',\n",
        "    490: 'chain mail, ring mail, mail, chain armor, chain armour, ring armor, ring armour',\n",
        "    491: 'chain saw, chainsaw',\n",
        "    492: 'chest',\n",
        "    493: 'chiffonier, commode',\n",
        "    494: 'chime, bell, gong',\n",
        "    495: 'china cabinet, china closet',\n",
        "    496: 'Christmas stocking',\n",
        "    497: 'church, church building',\n",
        "    498: 'cinema, movie theater, movie theatre, movie house, picture palace',\n",
        "    499: 'cleaver, meat cleaver, chopper',\n",
        "    500: 'cliff dwelling',\n",
        "    501: 'cloak',\n",
        "    502: 'clog, geta, patten, sabot',\n",
        "    503: 'cocktail shaker',\n",
        "    504: 'coffee mug',\n",
        "    505: 'coffeepot',\n",
        "    506: 'coil, spiral, volute, whorl, helix',\n",
        "    507: 'combination lock',\n",
        "    508: 'computer keyboard, keypad',\n",
        "    509: 'confectionery, confectionary, candy store',\n",
        "    510: 'container ship, containership, container vessel',\n",
        "    511: 'convertible',\n",
        "    512: 'corkscrew, bottle screw',\n",
        "    513: 'cornet, horn, trumpet, trump',\n",
        "    514: 'cowboy boot',\n",
        "    515: 'cowboy hat, ten-gallon hat',\n",
        "    516: 'cradle',\n",
        "    517: 'crane',\n",
        "    518: 'crash helmet',\n",
        "    519: 'crate',\n",
        "    520: 'crib, cot',\n",
        "    521: 'Crock Pot',\n",
        "    522: 'croquet ball',\n",
        "    523: 'crutch',\n",
        "    524: 'cuirass',\n",
        "    525: 'dam, dike, dyke',\n",
        "    526: 'desk',\n",
        "    527: 'desktop computer',\n",
        "    528: 'dial telephone, dial phone',\n",
        "    529: 'diaper, nappy, napkin',\n",
        "    530: 'digital clock',\n",
        "    531: 'digital watch',\n",
        "    532: 'dining table, board',\n",
        "    533: 'dishrag, dishcloth',\n",
        "    534: 'dishwasher, dish washer, dishwashing machine',\n",
        "    535: 'disk brake, disc brake',\n",
        "    536: 'dock, dockage, docking facility',\n",
        "    537: 'dogsled, dog sled, dog sleigh',\n",
        "    538: 'dome',\n",
        "    539: 'doormat, welcome mat',\n",
        "    540: 'drilling platform, offshore rig',\n",
        "    541: 'drum, membranophone, tympan',\n",
        "    542: 'drumstick',\n",
        "    543: 'dumbbell',\n",
        "    544: 'Dutch oven',\n",
        "    545: 'electric fan, blower',\n",
        "    546: 'electric guitar',\n",
        "    547: 'electric locomotive',\n",
        "    548: 'entertainment center',\n",
        "    549: 'envelope',\n",
        "    550: 'espresso maker',\n",
        "    551: 'face powder',\n",
        "    552: 'feather boa, boa',\n",
        "    553: 'file, file cabinet, filing cabinet',\n",
        "    554: 'fireboat',\n",
        "    555: 'fire engine, fire truck',\n",
        "    556: 'fire screen, fireguard',\n",
        "    557: 'flagpole, flagstaff',\n",
        "    558: 'flute, transverse flute',\n",
        "    559: 'folding chair',\n",
        "    560: 'football helmet',\n",
        "    561: 'forklift',\n",
        "    562: 'fountain',\n",
        "    563: 'fountain pen',\n",
        "    564: 'four-poster',\n",
        "    565: 'freight car',\n",
        "    566: 'French horn, horn',\n",
        "    567: 'frying pan, frypan, skillet',\n",
        "    568: 'fur coat',\n",
        "    569: 'garbage truck, dustcart',\n",
        "    570: 'gasmask, respirator, gas helmet',\n",
        "    571: 'gas pump, gasoline pump, petrol pump, island dispenser',\n",
        "    572: 'goblet',\n",
        "    573: 'go-kart',\n",
        "    574: 'golf ball',\n",
        "    575: 'golfcart, golf cart',\n",
        "    576: 'gondola',\n",
        "    577: 'gong, tam-tam',\n",
        "    578: 'gown',\n",
        "    579: 'grand piano, grand',\n",
        "    580: 'greenhouse, nursery, glasshouse',\n",
        "    581: 'grille, radiator grille',\n",
        "    582: 'grocery store, grocery, food market, market',\n",
        "    583: 'guillotine',\n",
        "    584: 'hair slide',\n",
        "    585: 'hair spray',\n",
        "    586: 'half track',\n",
        "    587: 'hammer',\n",
        "    588: 'hamper',\n",
        "    589: 'hand blower, blow dryer, blow drier, hair dryer, hair drier',\n",
        "    590: 'hand-held computer, hand-held microcomputer',\n",
        "    591: 'handkerchief, hankie, hanky, hankey',\n",
        "    592: 'hard disc, hard disk, fixed disk',\n",
        "    593: 'harmonica, mouth organ, harp, mouth harp',\n",
        "    594: 'harp',\n",
        "    595: 'harvester, reaper',\n",
        "    596: 'hatchet',\n",
        "    597: 'holster',\n",
        "    598: 'home theater, home theatre',\n",
        "    599: 'honeycomb',\n",
        "    600: 'hook, claw',\n",
        "    601: 'hoopskirt, crinoline',\n",
        "    602: 'horizontal bar, high bar',\n",
        "    603: 'horse cart, horse-cart',\n",
        "    604: 'hourglass',\n",
        "    605: 'iPod',\n",
        "    606: 'iron, smoothing iron',\n",
        "    607: \"jack-o'-lantern\",\n",
        "    608: 'jean, blue jean, denim',\n",
        "    609: 'jeep, landrover',\n",
        "    610: 'jersey, T-shirt, tee shirt',\n",
        "    611: 'jigsaw puzzle',\n",
        "    612: 'jinrikisha, ricksha, rickshaw',\n",
        "    613: 'joystick',\n",
        "    614: 'kimono',\n",
        "    615: 'knee pad',\n",
        "    616: 'knot',\n",
        "    617: 'lab coat, laboratory coat',\n",
        "    618: 'ladle',\n",
        "    619: 'lampshade, lamp shade',\n",
        "    620: 'laptop, laptop computer',\n",
        "    621: 'lawn mower, mower',\n",
        "    622: 'lens cap, lens cover',\n",
        "    623: 'letter opener, paper knife, paperknife',\n",
        "    624: 'library',\n",
        "    625: 'lifeboat',\n",
        "    626: 'lighter, light, igniter, ignitor',\n",
        "    627: 'limousine, limo',\n",
        "    628: 'liner, ocean liner',\n",
        "    629: 'lipstick, lip rouge',\n",
        "    630: 'Loafer',\n",
        "    631: 'lotion',\n",
        "    632: 'loudspeaker, speaker, speaker unit, loudspeaker system, speaker system',\n",
        "    633: \"loupe, jeweler's loupe\",\n",
        "    634: 'lumbermill, sawmill',\n",
        "    635: 'magnetic compass',\n",
        "    636: 'mailbag, postbag',\n",
        "    637: 'mailbox, letter box',\n",
        "    638: 'maillot',\n",
        "    639: 'maillot, tank suit',\n",
        "    640: 'manhole cover',\n",
        "    641: 'maraca',\n",
        "    642: 'marimba, xylophone',\n",
        "    643: 'mask',\n",
        "    644: 'matchstick',\n",
        "    645: 'maypole',\n",
        "    646: 'maze, labyrinth',\n",
        "    647: 'measuring cup',\n",
        "    648: 'medicine chest, medicine cabinet',\n",
        "    649: 'megalith, megalithic structure',\n",
        "    650: 'microphone, mike',\n",
        "    651: 'microwave, microwave oven',\n",
        "    652: 'military uniform',\n",
        "    653: 'milk can',\n",
        "    654: 'minibus',\n",
        "    655: 'miniskirt, mini',\n",
        "    656: 'minivan',\n",
        "    657: 'missile',\n",
        "    658: 'mitten',\n",
        "    659: 'mixing bowl',\n",
        "    660: 'mobile home, manufactured home',\n",
        "    661: 'Model T',\n",
        "    662: 'modem',\n",
        "    663: 'monastery',\n",
        "    664: 'monitor',\n",
        "    665: 'moped',\n",
        "    666: 'mortar',\n",
        "    667: 'mortarboard',\n",
        "    668: 'mosque',\n",
        "    669: 'mosquito net',\n",
        "    670: 'motor scooter, scooter',\n",
        "    671: 'mountain bike, all-terrain bike, off-roader',\n",
        "    672: 'mountain tent',\n",
        "    673: 'mouse, computer mouse',\n",
        "    674: 'mousetrap',\n",
        "    675: 'moving van',\n",
        "    676: 'muzzle',\n",
        "    677: 'nail',\n",
        "    678: 'neck brace',\n",
        "    679: 'necklace',\n",
        "    680: 'nipple',\n",
        "    681: 'notebook, notebook computer',\n",
        "    682: 'obelisk',\n",
        "    683: 'oboe, hautboy, hautbois',\n",
        "    684: 'ocarina, sweet potato',\n",
        "    685: 'odometer, hodometer, mileometer, milometer',\n",
        "    686: 'oil filter',\n",
        "    687: 'organ, pipe organ',\n",
        "    688: 'oscilloscope, scope, cathode-ray oscilloscope, CRO',\n",
        "    689: 'overskirt',\n",
        "    690: 'oxcart',\n",
        "    691: 'oxygen mask',\n",
        "    692: 'packet',\n",
        "    693: 'paddle, boat paddle',\n",
        "    694: 'paddlewheel, paddle wheel',\n",
        "    695: 'padlock',\n",
        "    696: 'paintbrush',\n",
        "    697: \"pajama, pyjama, pj's, jammies\",\n",
        "    698: 'palace',\n",
        "    699: 'panpipe, pandean pipe, syrinx',\n",
        "    700: 'paper towel',\n",
        "    701: 'parachute, chute',\n",
        "    702: 'parallel bars, bars',\n",
        "    703: 'park bench',\n",
        "    704: 'parking meter',\n",
        "    705: 'passenger car, coach, carriage',\n",
        "    706: 'patio, terrace',\n",
        "    707: 'pay-phone, pay-station',\n",
        "    708: 'pedestal, plinth, footstall',\n",
        "    709: 'pencil box, pencil case',\n",
        "    710: 'pencil sharpener',\n",
        "    711: 'perfume, essence',\n",
        "    712: 'Petri dish',\n",
        "    713: 'photocopier',\n",
        "    714: 'pick, plectrum, plectron',\n",
        "    715: 'pickelhaube',\n",
        "    716: 'picket fence, paling',\n",
        "    717: 'pickup, pickup truck',\n",
        "    718: 'pier',\n",
        "    719: 'piggy bank, penny bank',\n",
        "    720: 'pill bottle',\n",
        "    721: 'pillow',\n",
        "    722: 'ping-pong ball',\n",
        "    723: 'pinwheel',\n",
        "    724: 'pirate, pirate ship',\n",
        "    725: 'pitcher, ewer',\n",
        "    726: \"plane, carpenter's plane, woodworking plane\",\n",
        "    727: 'planetarium',\n",
        "    728: 'plastic bag',\n",
        "    729: 'plate rack',\n",
        "    730: 'plow, plough',\n",
        "    731: \"plunger, plumber's helper\",\n",
        "    732: 'Polaroid camera, Polaroid Land camera',\n",
        "    733: 'pole',\n",
        "    734: 'police van, police wagon, paddy wagon, patrol wagon, wagon, black Maria',\n",
        "    735: 'poncho',\n",
        "    736: 'pool table, billiard table, snooker table',\n",
        "    737: 'pop bottle, soda bottle',\n",
        "    738: 'pot, flowerpot',\n",
        "    739: \"potter's wheel\",\n",
        "    740: 'power drill',\n",
        "    741: 'prayer rug, prayer mat',\n",
        "    742: 'printer',\n",
        "    743: 'prison, prison house',\n",
        "    744: 'projectile, missile',\n",
        "    745: 'projector',\n",
        "    746: 'puck, hockey puck',\n",
        "    747: 'punching bag, punch bag, punching ball, punchball',\n",
        "    748: 'purse',\n",
        "    749: 'quill, quill pen',\n",
        "    750: 'quilt, comforter, comfort, puff',\n",
        "    751: 'racer, race car, racing car',\n",
        "    752: 'racket, racquet',\n",
        "    753: 'radiator',\n",
        "    754: 'radio, wireless',\n",
        "    755: 'radio telescope, radio reflector',\n",
        "    756: 'rain barrel',\n",
        "    757: 'recreational vehicle, RV, R.V.',\n",
        "    758: 'reel',\n",
        "    759: 'reflex camera',\n",
        "    760: 'refrigerator, icebox',\n",
        "    761: 'remote control, remote',\n",
        "    762: 'restaurant, eating house, eating place, eatery',\n",
        "    763: 'revolver, six-gun, six-shooter',\n",
        "    764: 'rifle',\n",
        "    765: 'rocking chair, rocker',\n",
        "    766: 'rotisserie',\n",
        "    767: 'rubber eraser, rubber, pencil eraser',\n",
        "    768: 'rugby ball',\n",
        "    769: 'rule, ruler',\n",
        "    770: 'running shoe',\n",
        "    771: 'safe',\n",
        "    772: 'safety pin',\n",
        "    773: 'saltshaker, salt shaker',\n",
        "    774: 'sandal',\n",
        "    775: 'sarong',\n",
        "    776: 'sax, saxophone',\n",
        "    777: 'scabbard',\n",
        "    778: 'scale, weighing machine',\n",
        "    779: 'school bus',\n",
        "    780: 'schooner',\n",
        "    781: 'scoreboard',\n",
        "    782: 'screen, CRT screen',\n",
        "    783: 'screw',\n",
        "    784: 'screwdriver',\n",
        "    785: 'seat belt, seatbelt',\n",
        "    786: 'sewing machine',\n",
        "    787: 'shield, buckler',\n",
        "    788: 'shoe shop, shoe-shop, shoe store',\n",
        "    789: 'shoji',\n",
        "    790: 'shopping basket',\n",
        "    791: 'shopping cart',\n",
        "    792: 'shovel',\n",
        "    793: 'shower cap',\n",
        "    794: 'shower curtain',\n",
        "    795: 'ski',\n",
        "    796: 'ski mask',\n",
        "    797: 'sleeping bag',\n",
        "    798: 'slide rule, slipstick',\n",
        "    799: 'sliding door',\n",
        "    800: 'slot, one-armed bandit',\n",
        "    801: 'snorkel',\n",
        "    802: 'snowmobile',\n",
        "    803: 'snowplow, snowplough',\n",
        "    804: 'soap dispenser',\n",
        "    805: 'soccer ball',\n",
        "    806: 'sock',\n",
        "    807: 'solar dish, solar collector, solar furnace',\n",
        "    808: 'sombrero',\n",
        "    809: 'soup bowl',\n",
        "    810: 'space bar',\n",
        "    811: 'space heater',\n",
        "    812: 'space shuttle',\n",
        "    813: 'spatula',\n",
        "    814: 'speedboat',\n",
        "    815: \"spider web, spider's web\",\n",
        "    816: 'spindle',\n",
        "    817: 'sports car, sport car',\n",
        "    818: 'spotlight, spot',\n",
        "    819: 'stage',\n",
        "    820: 'steam locomotive',\n",
        "    821: 'steel arch bridge',\n",
        "    822: 'steel drum',\n",
        "    823: 'stethoscope',\n",
        "    824: 'stole',\n",
        "    825: 'stone wall',\n",
        "    826: 'stopwatch, stop watch',\n",
        "    827: 'stove',\n",
        "    828: 'strainer',\n",
        "    829: 'streetcar, tram, tramcar, trolley, trolley car',\n",
        "    830: 'stretcher',\n",
        "    831: 'studio couch, day bed',\n",
        "    832: 'stupa, tope',\n",
        "    833: 'submarine, pigboat, sub, U-boat',\n",
        "    834: 'suit, suit of clothes',\n",
        "    835: 'sundial',\n",
        "    836: 'sunglass',\n",
        "    837: 'sunglasses, dark glasses, shades',\n",
        "    838: 'sunscreen, sunblock, sun blocker',\n",
        "    839: 'suspension bridge',\n",
        "    840: 'swab, swob, mop',\n",
        "    841: 'sweatshirt',\n",
        "    842: 'swimming trunks, bathing trunks',\n",
        "    843: 'swing',\n",
        "    844: 'switch, electric switch, electrical switch',\n",
        "    845: 'syringe',\n",
        "    846: 'table lamp',\n",
        "    847: 'tank, army tank, armored combat vehicle, armoured combat vehicle',\n",
        "    848: 'tape player',\n",
        "    849: 'teapot',\n",
        "    850: 'teddy, teddy bear',\n",
        "    851: 'television, television system',\n",
        "    852: 'tennis ball',\n",
        "    853: 'thatch, thatched roof',\n",
        "    854: 'theater curtain, theatre curtain',\n",
        "    855: 'thimble',\n",
        "    856: 'thresher, thrasher, threshing machine',\n",
        "    857: 'throne',\n",
        "    858: 'tile roof',\n",
        "    859: 'toaster',\n",
        "    860: 'tobacco shop, tobacconist shop, tobacconist',\n",
        "    861: 'toilet seat',\n",
        "    862: 'torch',\n",
        "    863: 'totem pole',\n",
        "    864: 'tow truck, tow car, wrecker',\n",
        "    865: 'toyshop',\n",
        "    866: 'tractor',\n",
        "    867: 'trailer truck, tractor trailer, trucking rig, rig, articulated lorry, semi',\n",
        "    868: 'tray',\n",
        "    869: 'trench coat',\n",
        "    870: 'tricycle, trike, velocipede',\n",
        "    871: 'trimaran',\n",
        "    872: 'tripod',\n",
        "    873: 'triumphal arch',\n",
        "    874: 'trolleybus, trolley coach, trackless trolley',\n",
        "    875: 'trombone',\n",
        "    876: 'tub, vat',\n",
        "    877: 'turnstile',\n",
        "    878: 'typewriter keyboard',\n",
        "    879: 'umbrella',\n",
        "    880: 'unicycle, monocycle',\n",
        "    881: 'upright, upright piano',\n",
        "    882: 'vacuum, vacuum cleaner',\n",
        "    883: 'vase',\n",
        "    884: 'vault',\n",
        "    885: 'velvet',\n",
        "    886: 'vending machine',\n",
        "    887: 'vestment',\n",
        "    888: 'viaduct',\n",
        "    889: 'violin, fiddle',\n",
        "    890: 'volleyball',\n",
        "    891: 'waffle iron',\n",
        "    892: 'wall clock',\n",
        "    893: 'wallet, billfold, notecase, pocketbook',\n",
        "    894: 'wardrobe, closet, press',\n",
        "    895: 'warplane, military plane',\n",
        "    896: 'washbasin, handbasin, washbowl, lavabo, wash-hand basin',\n",
        "    897: 'washer, automatic washer, washing machine',\n",
        "    898: 'water bottle',\n",
        "    899: 'water jug',\n",
        "    900: 'water tower',\n",
        "    901: 'whiskey jug',\n",
        "    902: 'whistle',\n",
        "    903: 'wig',\n",
        "    904: 'window screen',\n",
        "    905: 'window shade',\n",
        "    906: 'Windsor tie',\n",
        "    907: 'wine bottle',\n",
        "    908: 'wing',\n",
        "    909: 'wok',\n",
        "    910: 'wooden spoon',\n",
        "    911: 'wool, woolen, woollen',\n",
        "    912: 'worm fence, snake fence, snake-rail fence, Virginia fence',\n",
        "    913: 'wreck',\n",
        "    914: 'yawl',\n",
        "    915: 'yurt',\n",
        "    916: 'web site, website, internet site, site',\n",
        "    917: 'comic book',\n",
        "    918: 'crossword puzzle, crossword',\n",
        "    919: 'street sign',\n",
        "    920: 'traffic light, traffic signal, stoplight',\n",
        "    921: 'book jacket, dust cover, dust jacket, dust wrapper',\n",
        "    922: 'menu',\n",
        "    923: 'plate',\n",
        "    924: 'guacamole',\n",
        "    925: 'consomme',\n",
        "    926: 'hot pot, hotpot',\n",
        "    927: 'trifle',\n",
        "    928: 'ice cream, icecream',\n",
        "    929: 'ice lolly, lolly, lollipop, popsicle',\n",
        "    930: 'French loaf',\n",
        "    931: 'bagel, beigel',\n",
        "    932: 'pretzel',\n",
        "    933: 'cheeseburger',\n",
        "    934: 'hotdog, hot dog, red hot',\n",
        "    935: 'mashed potato',\n",
        "    936: 'head cabbage',\n",
        "    937: 'broccoli',\n",
        "    938: 'cauliflower',\n",
        "    939: 'zucchini, courgette',\n",
        "    940: 'spaghetti squash',\n",
        "    941: 'acorn squash',\n",
        "    942: 'butternut squash',\n",
        "    943: 'cucumber, cuke',\n",
        "    944: 'artichoke, globe artichoke',\n",
        "    945: 'bell pepper',\n",
        "    946: 'cardoon',\n",
        "    947: 'mushroom',\n",
        "    948: 'Granny Smith',\n",
        "    949: 'strawberry',\n",
        "    950: 'orange',\n",
        "    951: 'lemon',\n",
        "    952: 'fig',\n",
        "    953: 'pineapple, ananas',\n",
        "    954: 'banana',\n",
        "    955: 'jackfruit, jak, jack',\n",
        "    956: 'custard apple',\n",
        "    957: 'pomegranate',\n",
        "    958: 'hay',\n",
        "    959: 'carbonara',\n",
        "    960: 'chocolate sauce, chocolate syrup',\n",
        "    961: 'dough',\n",
        "    962: 'meat loaf, meatloaf',\n",
        "    963: 'pizza, pizza pie',\n",
        "    964: 'potpie',\n",
        "    965: 'burrito',\n",
        "    966: 'red wine',\n",
        "    967: 'espresso',\n",
        "    968: 'cup',\n",
        "    969: 'eggnog',\n",
        "    970: 'alp',\n",
        "    971: 'bubble',\n",
        "    972: 'cliff, drop, drop-off',\n",
        "    973: 'coral reef',\n",
        "    974: 'geyser',\n",
        "    975: 'lakeside, lakeshore',\n",
        "    976: 'promontory, headland, head, foreland',\n",
        "    977: 'sandbar, sand bar',\n",
        "    978: 'seashore, coast, seacoast, sea-coast',\n",
        "    979: 'valley, vale',\n",
        "    980: 'volcano',\n",
        "    981: 'ballplayer, baseball player',\n",
        "    982: 'groom, bridegroom',\n",
        "    983: 'scuba diver',\n",
        "    984: 'rapeseed',\n",
        "    985: 'daisy',\n",
        "    986: \"yellow lady's slipper, yellow lady-slipper, Cypripedium calceolus, Cypripedium parviflorum\",\n",
        "    987: 'corn',\n",
        "    988: 'acorn',\n",
        "    989: 'hip, rose hip, rosehip',\n",
        "    990: 'buckeye, horse chestnut, conker',\n",
        "    991: 'coral fungus',\n",
        "    992: 'agaric',\n",
        "    993: 'gyromitra',\n",
        "    994: 'stinkhorn, carrion fungus',\n",
        "    995: 'earthstar',\n",
        "    996: 'hen-of-the-woods, hen of the woods, Polyporus frondosus, Grifola frondosa',\n",
        "    997: 'bolete',\n",
        "    998: 'ear, spike, capitulum',\n",
        "    999: 'toilet tissue, toilet paper, bathroom tissue'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63332bcf-bb57-4d38-a497-f0ceab388030",
      "metadata": {
        "id": "63332bcf-bb57-4d38-a497-f0ceab388030"
      },
      "source": [
        "Now that we have our class labels, let's see how we can find which class label corresponded to the predicted class for our random tensor.  \n",
        "\n",
        "We use [*`torch.argmax()`*](https://pytorch.org/docs/stable/generated/torch.argmax.html) to find the index of the class with the highest score in the output.  \n",
        "\n",
        "The code below passes a random tensor through MobileNet V3 (like before), however this time it also computes the class label corresponding to the predicted class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47b88d21-4521-4ff1-bf80-17ff5a837089",
      "metadata": {
        "id": "47b88d21-4521-4ff1-bf80-17ff5a837089"
      },
      "outputs": [],
      "source": [
        "# Define a random tensor (including a batch dimension)\n",
        "random_tensor = torch.rand([1, 3, 224, 224], dtype=torch.float32)\n",
        "\n",
        "# Perform the forward pass through our mobilenet_v3 model\n",
        "with torch.no_grad():\n",
        "    output = mobilenet_v3(random_tensor)\n",
        "\n",
        "# Determine the index of the class with the highest score\n",
        "#   We need to specify the dimension to argmax so that we compute it per-element in our batch\n",
        "#   This will mean if the batch_size is N, we will get an (N, ) dimensional vector\n",
        "class_indexes = output.argmax(dim=1)\n",
        "\n",
        "# Extract just the first class index because our batch size = 1\n",
        "predicted_class_index = class_indexes[0]\n",
        "print(f'The predicted class index is: {predicted_class_index}')\n",
        "\n",
        "# Lookup the class label based on the class index producing the highest score\n",
        "#  predicted_class_index is a tensor. To convert it to an integer we need to use .item()\n",
        "print(f'The predicted label is: \"{IMAGENET_CLASSES[predicted_class_index.item()]}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daae8168-7e3a-4f2d-87e8-98ffae38ae43",
      "metadata": {
        "id": "daae8168-7e3a-4f2d-87e8-98ffae38ae43"
      },
      "source": [
        "After running the above code cell you should see a class index and class label printed out!  \n",
        "That means you've just successfully made a prediction with a neural network on some random data!  \n",
        "\n",
        "Before moving on, a couple of points worth noting:  \n",
        "* In our case we only had a single tensor we made a prediction on, so we could extract the first index into the `class_indexes` tensor. If your batch size is > 1, you would want to get the class label for each element in your `class_indexes` tensor (This is why specifying `dim=1` is so important in the call to *`argmax()`*).\n",
        "* After extracting the `predicted_class_index`, the type of `predicted_class_index` was still a tensor. Because we wanted to use this value as an index in a python list we needed to call [*`.item()`*](https://pytorch.org/docs/stable/generated/torch.Tensor.item.html) on the tensor. This converts the tensor into a standard python number, which we need to index into our dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eeee2eb6-6feb-4528-afd6-76b5c00a412c",
      "metadata": {
        "id": "eeee2eb6-6feb-4528-afd6-76b5c00a412c"
      },
      "source": [
        "### Classifying a Real Image\n",
        "Now we know how to make predictions on random data, the next step is to classify real image data!  \n",
        "\n",
        "Along with this lab you should have downloaded a `french_bulldog.png` file. In this section we see if we can get our network to correctly predict that our image is a French Bulldog! (This happens to be class index 245 in `IMAGENET_CLASSES`)\n",
        "\n",
        "Given we will be working with data directly from a file, we should be aware of some modifications we need to make to the input image before we are able to pass it through our network.  \n",
        "\n",
        "Along with loading it into a *numpy* array and converting to a *PyTorch* tensor, the pretrained network we are using requires that the input is resized to 224x224 pixels, the pixel values are between \\[0, 1], and the datatype is *float32*. Following the next code cell we will look into this criteria further, but for now be aware that these are a few things we will need to do.\n",
        "\n",
        "Now it's your turn! Let's try to classify the French Bulldog! Make sure you follow each step below, as there are a few things we need to do.\n",
        "\n",
        "**Task**: In the below code cell, load, preprocess and classify the `french_bulldog.png` image, following the steps outlined in the cell comments. You'll need to copy in your *`load_image_rgb()`* and *`display_image()`* functions from Lab 1.\n",
        "\n",
        "**NOTE:** The predicted class might not be what you expect. Also if you run your code cell multiple times you'll likely see different predictions. Read on to see why!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52cd5e1e-0a1b-461b-92de-56a39ba1c56e",
      "metadata": {
        "tags": [],
        "id": "52cd5e1e-0a1b-461b-92de-56a39ba1c56e"
      },
      "outputs": [],
      "source": [
        "# TODO: Copy in your load_image_rgb function from lab 1\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Copy in your display_image function from Lab 1\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Load the french_bulldog.png image (using load_image_rgb)\n",
        "# image = ...\n",
        "\n",
        "\n",
        "# TODO: Display the loaded image (using display_image)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Resize the image to 224x224 pixels using OpenCV (refer to Lab 1)\n",
        "# image = ...\n",
        "\n",
        "\n",
        "# TODO: Transpose the image to get dimensionality CxHxW\n",
        "# image = ...\n",
        "\n",
        "\n",
        "# TODO: Cast the image to a float32 datatype with pixels in the range [0, 1]\n",
        "image = image.astype(np.float32) / 255\n",
        "\n",
        "\n",
        "# TODO: Convert the image to a tensor\n",
        "# tensor_image = ...\n",
        "\n",
        "\n",
        "# TODO: Display the tensor (using display_tensor)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Create a batch dimension for your tensor\n",
        "# batched_tensor = ...\n",
        "\n",
        "\n",
        "# TODO: Pass your batched tensor through mobilenet_v3\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Print out the label corresponding to the class with the highest score\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task solution"
      ],
      "metadata": {
        "id": "RcfRm5mIcqPb"
      },
      "id": "RcfRm5mIcqPb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fb95e32-988e-4452-abec-8695dd7808f0",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "6fb95e32-988e-4452-abec-8695dd7808f0"
      },
      "outputs": [],
      "source": [
        "# TODO: Copy in your load_image_rgb function from lab 1\n",
        "def load_image_rgb(filepath):\n",
        "    image = cv2.imread(filepath)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    return image\n",
        "\n",
        "\n",
        "# TODO: Copy in your display_image function from Lab 1\n",
        "def display_image(image):\n",
        "    fig, axes = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    axes.imshow(image)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# TODO: Load the french_bulldog.png image (using load_image_rgb)\n",
        "image = load_image_rgb('french_bulldog.png')\n",
        "\n",
        "\n",
        "# TODO: Display the loaded image (using display_image)\n",
        "display_image(image)\n",
        "\n",
        "\n",
        "# TODO: Resize the image to 224x224 pixels using OpenCV (refer to Lab 1)\n",
        "image = cv2.resize(image, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "# TODO: Transpose the image to get dimensionality CxHxW\n",
        "image = np.transpose(image, (2, 0, 1))\n",
        "\n",
        "\n",
        "# TODO: Cast the image to a float32 datatype with pixels in the range [0, 1]\n",
        "image = image.astype(np.float32) / 255\n",
        "\n",
        "\n",
        "# TODO: Convert the image to a tensor\n",
        "tensor_image = torch.as_tensor(image)\n",
        "\n",
        "\n",
        "# TODO: Display the tensor (using display_tensor)\n",
        "display_tensor(tensor_image)\n",
        "\n",
        "\n",
        "# TODO: Create a batch dimension for your tensor\n",
        "batched_tensor = tensor_image.unsqueeze(dim=0)\n",
        "\n",
        "\n",
        "# TODO: Pass your batched tensor through mobilenet_v3\n",
        "with torch.no_grad():\n",
        "    outputs = mobilenet_v3(batched_tensor)\n",
        "\n",
        "\n",
        "# TODO: Print out the label corresponding to the class with the highest score\n",
        "class_indexes = outputs.argmax(dim=1)\n",
        "predicted_class_index = class_indexes[0]\n",
        "print(f'The predicted label is: \"{IMAGENET_CLASSES[predicted_class_index.item()]}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d269693d-fe42-41ac-ae14-4039f5a9c6b9",
      "metadata": {
        "id": "d269693d-fe42-41ac-ae14-4039f5a9c6b9"
      },
      "source": [
        "#### Evaluation mode\n",
        "\n",
        "Did you see the predicted class wasn't quite right?  \n",
        "Run your code cell 5-10 times, do you see different predictions?  \n",
        "\n",
        "If the second point above happened, that's a very big sign that something isn't quite right.  \n",
        "\n",
        "There are two ways we can configure our neural network, specifically ***train**ing* mode or ***eval**uation* mode.\n",
        "There are some layers in our network that behave very differently depending on which mode our network is in (e.g. BatchNorm, Dropout, etc.). If you saw different predictions each time you tried to make a prediction on your image, then this is a strong indicator that our network is in the wrong mode!  \n",
        "\n",
        "Whenever we want to predict on new data, we need to make sure our network is set to ***eval**uation* mode by calling its [*`eval()`* method](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval).  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a78be2df-0801-46d7-ba84-0d5ef226190d",
      "metadata": {
        "id": "a78be2df-0801-46d7-ba84-0d5ef226190d"
      },
      "source": [
        "**Task**: In the code cell below, make the call to set your `mobilenet_v3` model into eval mode at the top of the code cell then copy in your solution to the above code cell (no need to copy your function definitions).\n",
        "\n",
        "Run the code cell a few times to make sure your predictions are the same each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31e587e0-0552-4331-8056-3c159e6acf87",
      "metadata": {
        "id": "31e587e0-0552-4331-8056-3c159e6acf87"
      },
      "outputs": [],
      "source": [
        "# TODO: Set your MobileNet V3 into eval mode\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Copy your code from the above code cell here. Do not copy the function definitions\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task solution"
      ],
      "metadata": {
        "id": "JSt1ymEucCJM"
      },
      "id": "JSt1ymEucCJM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f64347e-06d8-4ae1-8def-0628001153d6",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "9f64347e-06d8-4ae1-8def-0628001153d6"
      },
      "outputs": [],
      "source": [
        "# TODO: Set your MobileNet V3 into eval mode\n",
        "mobilenet_v3.eval()\n",
        "\n",
        "\n",
        "# TODO: Copy your code from the above code cell here. Do not copy the function definitions\n",
        "# TODO: Load the french_bulldog.png image (using load_image_rgb)\n",
        "image = load_image_rgb('french_bulldog.png')\n",
        "\n",
        "\n",
        "# TODO: Display the loaded image (using display_image)\n",
        "display_image(image)\n",
        "\n",
        "\n",
        "# TODO: Resize the image to 224x224 pixels using OpenCV (refer to Lab 1)\n",
        "image = cv2.resize(image, dsize=(224, 224), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "# TODO: Transpose the image to get dimensionality CxHxW\n",
        "image = np.transpose(image, (2, 0, 1))\n",
        "\n",
        "\n",
        "# TODO: Cast the image to a float32 datatype with pixels in the range [0, 1]\n",
        "image = image.astype(np.float32) / 255\n",
        "\n",
        "\n",
        "# TODO: Convert the image to a tensor\n",
        "tensor_image = torch.as_tensor(image)\n",
        "\n",
        "\n",
        "# TODO: Display the tensor (using display_tensor)\n",
        "display_tensor(tensor_image)\n",
        "\n",
        "\n",
        "# TODO: Create a batch dimension for your tensor\n",
        "batched_tensor = tensor_image.unsqueeze(dim=0)\n",
        "\n",
        "\n",
        "# TODO: Pass your batched tensor through mobilenet_v3\n",
        "with torch.no_grad():\n",
        "    outputs = mobilenet_v3(batched_tensor)\n",
        "\n",
        "\n",
        "# TODO: Print out the label corresponding to the class with the highest score\n",
        "class_indexes = outputs.argmax(dim=1)\n",
        "predicted_class_index = class_indexes[0]\n",
        "print(f'The predicted label is: \"{IMAGENET_CLASSES[predicted_class_index.item()]}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08988e60-29ae-4a1c-9618-0a9facae4b03",
      "metadata": {
        "id": "08988e60-29ae-4a1c-9618-0a9facae4b03"
      },
      "source": [
        "### Data Preprocessing\n",
        "Awesome work so far! If everything went well, your code should have correctly predicted that the image was a French Bulldog!\n",
        "\n",
        "In the previous labs we had a taste of the possible preprocessing steps we can take to prepare images for classification. Since we are using a pre-trained model, we must ensure that we perform the same preprocessing steps on our image data as when it was trained, or else the model won't perform as well. Convolutional Neural Networks drastically reduce the amount of preprocessing required, so we've probably already covered most of the steps required.\n",
        "\n",
        "Looking at the [torchvision.models documentation](https://pytorch.org/vision/stable/models.html#:~:text=eval()%20for%20details.-,All%20pre-trained%20models%20expect%20input%20images%20normalized%20in%20the,using%20mean%20%3D%20%5B0.485%2C%200.456%2C%200.406%5D%20and%20std%20%3D%20%5B0.229%2C%200.224%2C%200.225%5D.,-You%20can%20use), we can see that it gives us some information about the expected format of image data when we use the pretrained models for classification.  \n",
        "\n",
        "That paragraph tells us:\n",
        "* We need mini-batches of RGB images (Bx3xHxW), with height and width of 224 *(We did this)*\n",
        "* Images should be loaded in to a range of [0,1] *(We did this)*\n",
        "* Images should be normalized with a given mean and standard deviation *(We didn't do this)*\n",
        "\n",
        "To make sure the classification results we get are as expected, we need to make sure our image data is in the appropriate format.  \n",
        "\n",
        "In the previous section we implemented the first two points manually and did not implement the third point.  \n",
        "\n",
        "To simplify the code we need to write, we can make use of the *torchvision* package to help perform these necessary preprocessing steps. The [torchvision.transforms documentation](https://pytorch.org/vision/stable/transforms.html) has a collection of transforms that we might want to make on our data, and specifically, can handle converting to tensor in the range [0,1], resizing our data, and normalizing our data.\n",
        "\n",
        "The code below shows examples of using *torchvision* to apply the three types of transforms mentioned above. Specifically we use the [functional transforms](https://pytorch.org/vision/stable/transforms.html#functional-transforms) imported as `tvtf` (named for **t**orch**v**ision.**t**ransforms.**f**unctional)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bad4fdb-e1fb-4221-8481-f8557f9bb1bb",
      "metadata": {
        "id": "4bad4fdb-e1fb-4221-8481-f8557f9bb1bb"
      },
      "outputs": [],
      "source": [
        "# Load the image into a numpy ndarray\n",
        "image = load_image_rgb('french_bulldog.png')\n",
        "print(f'Loaded image type: {type(image)}.\\nMin/Max pixel values: {image.min()}, {image.max()}.\\nDatatype: {image.dtype}\\nShape: {image.shape}')\n",
        "\n",
        "print('-' * 50)\n",
        "\n",
        "# Convert the image to a tensor using torchvision\n",
        "tensor_image = tvtf.to_tensor(image)\n",
        "print(f'Loaded image type: {type(tensor_image)}.\\nMin/Max pixel values: {tensor_image.min()}, {tensor_image.max()}.\\nDatatype: {tensor_image.dtype}\\nShape: {tensor_image.shape}')\n",
        "\n",
        "print('-' * 50)\n",
        "\n",
        "# Resize the image so that the smallest edge is 224 pixels\n",
        "tensor_image = tvtf.resize(tensor_image, 224)\n",
        "display_tensor(tensor_image)\n",
        "\n",
        "print('-' * 50)\n",
        "\n",
        "# Normalize the image using the mean/std given in the MobileNetV3 documentation\n",
        "tensor_image = tvtf.normalize(tensor_image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "print(f'Min/Max pixel values: {tensor_image.min()}, {tensor_image.max()}.\\nShape: {tensor_image.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a552f269-81ac-485a-98ee-99ac208fad41",
      "metadata": {
        "id": "a552f269-81ac-485a-98ee-99ac208fad41"
      },
      "source": [
        "As you can see, using *torchvision* greatly simplifies the code we otherwise need to write. The code to convert the *numpy* array to a *PyTorch* tensor can be performed in one line of code using *torchvision* vs. 3 lines of code when writing it manually.  \n",
        "\n",
        "You should be able to see the analogies between the custom code we needed to write vs. the code we write when using *torchvision*.\n",
        "\n",
        "Resizing our data is an important step to ensure that the spatial size of our image matches what our network expects. In the French Bulldog example, that image was square, meaning we had no issues when we resized it down to a resolution of 224x224. We will run into problems as soon as we encounter non-square images, where if we just resize the image we may end up skewing the image (creating a 'stretched' image).  \n",
        "\n",
        "One way we can overcome this is to resize the image so that the shortest edge is a bit larger than our desired size, then take a crop from the centre of the image. By doing this we will ideally capture the main contents of the image whilst still making sure we get the required resolution for our network.\n",
        "\n",
        "The `torchvision` package has transforms that can exactly perform these tasks for us, so in terms of code it just becomes a few function calls!\n",
        "\n",
        "To resize the image on the shortest edge we can use the [*`resize()`* function](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.functional.resize) and to take a crop from the centre of the image we can use the [*`center_crop()`* function](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.functional.center_crop).\n",
        "\n",
        "Pay attention to the expected types of arguments to the transform functions when looking at the documentation. In general, most transforms can be applied to a *PIL Image* or *Tensor*. Given our image data is stored in *numpy* arrays, we will first convert our data to a tensor then apply the transforms.\n",
        "\n",
        "Let's get some practice with this!\n",
        "\n",
        "**Task**: In the below code cell, load the `cat.jpg` image and perform all of the data preprocessing steps outlined in the cell comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e67c56e3-c162-4a5e-913b-133c615e3159",
      "metadata": {
        "id": "e67c56e3-c162-4a5e-913b-133c615e3159"
      },
      "outputs": [],
      "source": [
        "# TODO: Load the cat.png image (using load_image_rgb)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Display the loaded image (using display_image)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Convert the image to a tensor (using the torchvision transform)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Resize the tensor to 256px on the shortest edge (using the torchvision resize() transform)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Take a 224px centre crop of the tensor (using the torchvision center_crop() transform)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Display the tensor (using display_tensor)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task solution"
      ],
      "metadata": {
        "id": "a7g4AzdddXi_"
      },
      "id": "a7g4AzdddXi_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "361713b0-6c27-4cb0-a764-1047748189b0",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "361713b0-6c27-4cb0-a764-1047748189b0"
      },
      "outputs": [],
      "source": [
        "# TODO: Load the cat.png image (using load_image_rgb)\n",
        "image = load_image_rgb('cat.jpg')\n",
        "\n",
        "\n",
        "# TODO: Display the loaded image (using display_image)\n",
        "display_image(image)\n",
        "\n",
        "\n",
        "# TODO: Convert the image to a tensor (using the torchvision transform)\n",
        "tensor_image = tvtf.to_tensor(image)\n",
        "\n",
        "\n",
        "# TODO: Resize the tensor to 256px on the shortest edge (using the torchvision resize() transform)\n",
        "tensor_image = tvtf.resize(tensor_image, 256)\n",
        "\n",
        "\n",
        "# TODO: Take a 224px centre crop of the tensor (using the torchvision center_crop() transform)\n",
        "tensor_image = tvtf.center_crop(tensor_image, 224)\n",
        "\n",
        "\n",
        "# TODO: Display the tensor (using display_tensor)\n",
        "display_tensor(tensor_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Function"
      ],
      "metadata": {
        "id": "pgC4f-RUdRsq"
      },
      "id": "pgC4f-RUdRsq"
    },
    {
      "cell_type": "markdown",
      "id": "ef84bf2a-ea70-45ec-87c5-be502a90e8ee",
      "metadata": {
        "id": "ef84bf2a-ea70-45ec-87c5-be502a90e8ee"
      },
      "source": [
        "Great work!\n",
        "\n",
        "The `cat.jpg` image initially had a pixel resolution of 1327x913px, however with some transforms you were able to get it down to 224x224px whilst capturing the main part of the image and preserving the aspect ratio of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fef4d43-82be-4a4d-b632-1abc9a458009",
      "metadata": {
        "id": "4fef4d43-82be-4a4d-b632-1abc9a458009"
      },
      "source": [
        "**Task**: As a final step before moving on, let's write a function that will handle preprocessing an image for us.\n",
        "\n",
        "In the below code cell, write a function `preprocess_image` that:\n",
        "* Takes an image as a parameter\n",
        "* Converts the image to a tensor\n",
        "* Resizes the tensor to 256px on the shortest edge\n",
        "* Takes a 224px centre crop of the tensor\n",
        "* Normalizes the tensor (using the mean and std described above)\n",
        "* Creates a batch dimension for the tensor\n",
        "* Returns the processed tensor\n",
        "\n",
        "Make sure you use the *torchvision* transforms to implement this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aafdf8a-e121-46b0-839b-32c6c9ff6573",
      "metadata": {
        "id": "1aafdf8a-e121-46b0-839b-32c6c9ff6573"
      },
      "outputs": [],
      "source": [
        "# TODO: Write your function here\n",
        "\n",
        "\n",
        "\n",
        "cat_image = load_image_rgb('cat.jpg')\n",
        "cat_tensor = preprocess_image(cat_image)\n",
        "print(type(cat_tensor))      # Should be torch.Tensor\n",
        "print(cat_tensor.shape)      # Should be (1, 3, 224, 224)\n",
        "print(f'Min/Max pixel values: {tensor_image.min()}, {tensor_image.max()}.')    # Should be between [0, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task solution"
      ],
      "metadata": {
        "id": "SnHNGI3hdLUj"
      },
      "id": "SnHNGI3hdLUj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20ef40b9-9707-4cfe-8803-947f344986bd",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "20ef40b9-9707-4cfe-8803-947f344986bd"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(image):\n",
        "    # Convert image to tensor\n",
        "    image = tvtf.to_tensor(image)\n",
        "\n",
        "    # Resize to 256px on shortest edge\n",
        "    image = tvtf.resize(image, 256)\n",
        "\n",
        "    # 224px centre crop\n",
        "    image = tvtf.center_crop(image, 224)\n",
        "\n",
        "    # Normalise the image\n",
        "    image = tvtf.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    # Create a batch dimension\n",
        "    image = image.unsqueeze(dim=0)\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65a0045a-932b-4dbe-b335-954805ee8fa2",
      "metadata": {
        "id": "65a0045a-932b-4dbe-b335-954805ee8fa2"
      },
      "source": [
        "### Pulling Everything Together\n",
        "Now we have covered everything we need to be able to use our pretrained network to classify new unseen data.\n",
        "\n",
        "In this section let's create a function that can take a neural network model and image data (stored in a numpy array), and give us back some data on the predictions of the image!\n",
        "\n",
        "\n",
        "Now it's your turn! Let's try classify the French Bulldog! Make sure you follow each step below, as there are a few things we need to do.\n",
        "\n",
        "In the below code cell, your task is to write a function named *classify_image* that:\n",
        "* Takes a *model* and *image* as parameters\n",
        "* Sets the *model* to evaluation mode\n",
        "* Preprocesses the image, resulting in a tensor (Use the function you previously wrote)\n",
        "* Performs a foward pass of the batched data through the model\n",
        "* Returns a 3-tuple:\n",
        "    * The output 1000D vector (converted to a *numpy* array),\n",
        "    * The index of the class with the highest score (use `.item()` ), and\n",
        "    * The class label corresponding to that score\n",
        "\n",
        "At the bottom of the code cell is some code that will call your classify_image function with MobileNet V3 and the cat image. Use this to verify that your function works!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6f883cd-316e-426d-ad79-491989afbe7d",
      "metadata": {
        "tags": [],
        "id": "b6f883cd-316e-426d-ad79-491989afbe7d"
      },
      "outputs": [],
      "source": [
        "# TODO: Write your function here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Test your function!\n",
        "cat_image = load_image_rgb('cat.jpg')\n",
        "outputs, class_index, class_label = classify_image(mobilenet_v3, cat_image)\n",
        "print(f'Predicted to be: {class_label}')\n",
        "display_image(cat_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task solution"
      ],
      "metadata": {
        "id": "NQB9fihHdb6S"
      },
      "id": "NQB9fihHdb6S"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af65af58-af29-405d-af74-d55c0828b5fe",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "af65af58-af29-405d-af74-d55c0828b5fe"
      },
      "outputs": [],
      "source": [
        "# TODO: Write your function here\n",
        "def classify_image(model, image):\n",
        "    # Set model to eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Preprocess the image\n",
        "    image = preprocess_image(image)\n",
        "\n",
        "    # Perform the forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "\n",
        "    # Get the output vector, class index of highest score and class label\n",
        "    output = outputs[0]                             # Extract the 1000D vector\n",
        "    class_index = output.argmax(dim=0).item()       # Output is now a (1000,) dimensional vector, so we argmax over dim=0\n",
        "    class_label = IMAGENET_CLASSES[class_index]\n",
        "\n",
        "    # Convert the output vector to a numpy array\n",
        "    output = output.detach().cpu().numpy()\n",
        "\n",
        "    # Return the data\n",
        "    return output, class_index, class_label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "411e9cf5-03f6-4158-a175-bc7fbd5148f0",
      "metadata": {
        "id": "411e9cf5-03f6-4158-a175-bc7fbd5148f0"
      },
      "source": [
        "### More Predictions\n",
        "Now you've got a really powerful function that can classify new image data.  \n",
        "\n",
        "In the code cell below we have provided a function for you that can load image data from a URL into a *numpy* array.\n",
        "\n",
        "The code cell below defines the function and shows an example of using it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bea2b5d7-399a-4975-85c4-420190fe653e",
      "metadata": {
        "id": "bea2b5d7-399a-4975-85c4-420190fe653e"
      },
      "outputs": [],
      "source": [
        "def load_image_from_url(url):\n",
        "    \"\"\"Given a URL, loads the image into a numpy\n",
        "\n",
        "    Image loaded in RGB, with HWC channel ordering\n",
        "\n",
        "    Args:\n",
        "        url (str): The URL of the image to load\n",
        "\n",
        "    Returns:\n",
        "        (np.ndarray): The RGB, HWC ordered image\n",
        "    \"\"\"\n",
        "    with urlopen(url) as ur:\n",
        "        image = np.asarray(bytearray(ur.read()), dtype='uint8')\n",
        "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    return image\n",
        "\n",
        "# Load and display an image\n",
        "image = load_image_from_url('https://images.unsplash.com/photo-1543466835-00a7907e9de1?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1267&q=80')\n",
        "print(type(image), image.shape)\n",
        "display_image(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b80056df-e02b-43e5-8117-7dd04457f133",
      "metadata": {
        "id": "b80056df-e02b-43e5-8117-7dd04457f133"
      },
      "source": [
        "To further improve how we can present our classifications, let's try to improve the display_image function to accept a title for the image. Following classifying an image, when displaying the image we will set the title to the predicted label.\n",
        "\n",
        "You may already have done this if you completed Challenge Question 2 in Lab 1.\n",
        "\n",
        "**Task**: In the code cell below, write a function named *display_image* that:\n",
        "* Takes an image (in a *numpy* array) and a title (default value = None)\n",
        "* Displays the image using *matplotlib* (Use your code from Lab 1)\n",
        "* Before calling `plt.show()`, if the title is not None, sets the title of the plot to be the given title\n",
        "\n",
        "At the bottom of the code cell is some code that will call your display_image function. Use this to verify that your function works, you should see the title set to 'Dog'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a20db8a9-c455-432a-b754-5e1ee9b331cf",
      "metadata": {
        "id": "a20db8a9-c455-432a-b754-5e1ee9b331cf"
      },
      "outputs": [],
      "source": [
        "# TODO: Write your function here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Test your function!\n",
        "image = load_image_from_url('https://images.unsplash.com/photo-1543466835-00a7907e9de1?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1267&q=80')\n",
        "display_image(image, 'Dog')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task solution"
      ],
      "metadata": {
        "id": "wXnW1Hjke2N4"
      },
      "id": "wXnW1Hjke2N4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38429fa4-f46e-42af-965c-9fd210a6fc0f",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "38429fa4-f46e-42af-965c-9fd210a6fc0f"
      },
      "outputs": [],
      "source": [
        "def display_image(image, title=None):\n",
        "    fig, axes = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "    axes.imshow(image)\n",
        "\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Web Image Classification"
      ],
      "metadata": {
        "id": "9NIpvQV8e4x7"
      },
      "id": "9NIpvQV8e4x7"
    },
    {
      "cell_type": "markdown",
      "id": "a37da88e-31d9-418c-821a-61d65cf41b8d",
      "metadata": {
        "id": "a37da88e-31d9-418c-821a-61d65cf41b8d"
      },
      "source": [
        "Now we have a nicer displaying function, let's make a prediction on an image from a URL and display the predicted label along with the image!\n",
        "\n",
        "**Task**: In the code cell below load, classify and display the image from the given URL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "583377e1-0e43-462a-b842-b79305b5bac6",
      "metadata": {
        "id": "583377e1-0e43-462a-b842-b79305b5bac6"
      },
      "outputs": [],
      "source": [
        "url = 'https://images.unsplash.com/photo-1543466835-00a7907e9de1?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1267&q=80'\n",
        "\n",
        "# TODO: Load the image from the URL\n",
        "\n",
        "\n",
        "\n",
        "# Classify the image using MobileNet V3\n",
        "\n",
        "\n",
        "\n",
        "# Display the image with the title set to be the predicted label\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task solution"
      ],
      "metadata": {
        "id": "hgHaqBlCeSJN"
      },
      "id": "hgHaqBlCeSJN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73ea5796-db01-4999-8bb3-da1ba2c7235c",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "73ea5796-db01-4999-8bb3-da1ba2c7235c"
      },
      "outputs": [],
      "source": [
        "url = 'https://images.unsplash.com/photo-1543466835-00a7907e9de1?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1267&q=80'\n",
        "\n",
        "# TODO: Load the image from the URL\n",
        "image = load_image_from_url(url)\n",
        "\n",
        "\n",
        "# Classify the image using MobileNet V3\n",
        "_, _, label = classify_image(mobilenet_v3, image)\n",
        "\n",
        "\n",
        "# Display the image with the title set to be the predicted label\n",
        "display_image(image, label)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6f0cc83-0792-4f99-bc30-7edabe94fa4b",
      "metadata": {
        "id": "c6f0cc83-0792-4f99-bc30-7edabe94fa4b"
      },
      "source": [
        "#### Further Testing\n",
        "\n",
        "Awesome work!\n",
        "\n",
        "Feel free to play around with different URLs (or loading images from disk) to see differences in prediction!\n",
        "\n",
        "A good resource for finding sample images to classify is [Unsplash.com](https://unsplash.com/).  \n",
        "\n",
        "To get the URL for an image from Unsplash:\n",
        "1. Search for an image\n",
        "2. Click on the desired image to open it up in a larger window\n",
        "3. Right click on the image and select \"copy image address\"\n",
        "4. Paste the copied URL into the code cell and run your code!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14112c07-99eb-494e-a031-32649d51ee66",
      "metadata": {
        "id": "14112c07-99eb-494e-a031-32649d51ee66"
      },
      "source": [
        "## 1.4 Ensembling with a Single Model\n",
        "Before finishing up, let's see what the effects are if we first modify our image in some way before classifying it!\n",
        "\n",
        "To keep things simple, let's see what the effect of classifying an image of a cat is when we first apply a rotation to the cat image.\n",
        "\n",
        "The code cell below loads an image of a cat then classifies it, keeping track of the output vector, class index and class label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d13429d-a062-4cf9-9517-7889c3e757b9",
      "metadata": {
        "id": "4d13429d-a062-4cf9-9517-7889c3e757b9"
      },
      "outputs": [],
      "source": [
        "# Load the cat image\n",
        "url = 'https://images.unsplash.com/photo-1611242118897-f06ed77f6ab0?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1350&q=80'\n",
        "cat_image = load_image_from_url(url)\n",
        "\n",
        "# Degrade the image by clipping pixel values\n",
        "cat_image = np.clip(cat_image, 150, 205)\n",
        "\n",
        "# Classify and display the cat image\n",
        "cat_output, cat_class_index, cat_label = classify_image(mobilenet_v3, cat_image)\n",
        "display_image(cat_image, cat_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "038bd5c4-562c-4208-97d3-e14b44a534d2",
      "metadata": {
        "id": "038bd5c4-562c-4208-97d3-e14b44a534d2"
      },
      "source": [
        "Following that, you should see the cat predicted as a \"tabby, tabby cat\" or \"tiger cat\".  \n",
        "\n",
        "What if we were to rotate the image before classifying it? Let's have a look at how this changes our classification!\n",
        "\n",
        "**Task**: In the code cell below, apply rotations of +- 20 degrees to the cat image and visualize their classified labels. You'll need your *`rotate_image()`* function from Lab 1.\n",
        "\n",
        "**NOTE:** To classify multiple images at the same time we would usually combine them into a batch and do a single forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4335629c-eedc-4899-aa0f-a0722e7be58b",
      "metadata": {
        "id": "4335629c-eedc-4899-aa0f-a0722e7be58b"
      },
      "outputs": [],
      "source": [
        "# TODO: Copy in your rotate_image function from Lab 1\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Create a rotated cat image by rotating 20 degrees counter-clockwise\n",
        "# cat_ccw_rot_image = ...\n",
        "\n",
        "\n",
        "# Classify the rotated cat image\n",
        "cat_ccw_rot_output, cat_ccw_rot_class_index, cat_ccw_rot_label = classify_image(mobilenet_v3, cat_ccw_rot_image)\n",
        "\n",
        "\n",
        "# Display the rotated cat image along with its label\n",
        "display_image(cat_ccw_rot_image, cat_ccw_rot_label)\n",
        "\n",
        "\n",
        "# TODO: Create a rotated cat image by rotating 20 degrees clockwise\n",
        "# cat_cw_rot_image = ...\n",
        "\n",
        "\n",
        "# Classify the rotated cat image\n",
        "cat_cw_rot_output, cat_cw_rot_class_index, cat_cw_rot_label = classify_image(mobilenet_v3, cat_cw_rot_image)\n",
        "\n",
        "\n",
        "# Display the rotated cat image along with its label\n",
        "display_image(cat_cw_rot_image, cat_cw_rot_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task solution"
      ],
      "metadata": {
        "id": "DlitQUQXfKLJ"
      },
      "id": "DlitQUQXfKLJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb23d2b7-52b9-4c8c-a845-79d6a9693992",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "cb23d2b7-52b9-4c8c-a845-79d6a9693992"
      },
      "outputs": [],
      "source": [
        "# TODO: Copy in your rotate_image function from Lab 1\n",
        "def rotate_image(image, angle):\n",
        "    height, width = image.shape[:2]\n",
        "    M = cv2.getRotationMatrix2D((width // 2, height//2), angle, 1)\n",
        "    return cv2.warpAffine(image, M, (width, height))\n",
        "\n",
        "\n",
        "# TODO: Create a rotated cat image by rotating 20 degrees counter-clockwise\n",
        "cat_ccw_rot_image = rotate_image(cat_image, 20)\n",
        "\n",
        "\n",
        "# Classify the rotated cat image\n",
        "cat_ccw_rot_output, cat_ccw_rot_class_index, cat_ccw_rot_label = classify_image(mobilenet_v3, cat_ccw_rot_image)\n",
        "\n",
        "# Display the rotated cat image along with its label\n",
        "display_image(cat_ccw_rot_image, cat_ccw_rot_label)\n",
        "\n",
        "\n",
        "# Create a rotated cat image by rotating 20 degrees clockwise\n",
        "cat_cw_rot_image = rotate_image(cat_image, -20)\n",
        "\n",
        "\n",
        "# Classify the rotated cat image\n",
        "cat_cw_rot_output, cat_cw_rot_class_index, cat_cw_rot_label = classify_image(mobilenet_v3, cat_cw_rot_image)\n",
        "\n",
        "\n",
        "# Display the rotated cat image along with its label\n",
        "display_image(cat_cw_rot_image, cat_cw_rot_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4750cf41-ad0a-4837-b9c0-4eb402cee0ed",
      "metadata": {
        "id": "4750cf41-ad0a-4837-b9c0-4eb402cee0ed"
      },
      "source": [
        "#### Discussion\n",
        "You might see that the counter-clockwise rotation produced a different label to the clockwise rotation.    \n",
        "\n",
        "This shows that the network can produce different results depending on transformations applied to the input image. It also means in practice, if you took a photo of your cat on an angle the network may predict a different class.\n",
        "\n",
        "There are times where we can use this to our advantage! As long as the transformations we apply do not change the contents of the image (the class of the image is still obvious to us), then we can classify the same image (with different transformations applied) and then aggregate the classifications to get a final label. This can get us better performance at the cost of computer processing time.\n",
        "\n",
        "There are two common techniques we can use to do this, *Voting* on the class label or *Averaging the output vector*."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7836cd7-803b-4f7f-9aee-d82f08f1ad5a",
      "metadata": {
        "id": "d7836cd7-803b-4f7f-9aee-d82f08f1ad5a"
      },
      "source": [
        "### Voting\n",
        "When we use voting, we look at the collection of predictions we have made on the same image (with transformations applied), and then select the class that was most frequently predicted.\n",
        "\n",
        "When doing this we should take care of the case there are equally frequent classes predicted.\n",
        "\n",
        "An example of using Voting to aggregate the classifications in the previous example is found below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a2ae697-39bd-4984-8014-3c7b34ebf545",
      "metadata": {
        "id": "5a2ae697-39bd-4984-8014-3c7b34ebf545"
      },
      "outputs": [],
      "source": [
        "# Find the most common class index from the predicted class indexes\n",
        "predicted_class_indexes = [cat_class_index, cat_ccw_rot_class_index, cat_cw_rot_class_index]\n",
        "\n",
        "# This is a shortcut to find the mode of a list\n",
        "most_common_class_index = max(predicted_class_indexes, key=predicted_class_indexes.count)\n",
        "\n",
        "# Print out the class\n",
        "print(f'Most common class index: {most_common_class_index}')\n",
        "print(f'This corresponds to class: {IMAGENET_CLASSES[most_common_class_index]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f630b6d-d1b5-4b73-808b-c60a8922cc27",
      "metadata": {
        "id": "4f630b6d-d1b5-4b73-808b-c60a8922cc27"
      },
      "source": [
        "### Averaging Output Vectors\n",
        "Remembering that the output of our network is a score per-class, with the goal that the highest score corresponds to the predicted class.  \n",
        "\n",
        "Another way we can aggregate the different predictions is to find the average of the output vectors for each different image variation, then choose the class index directly from the averaged vector.\n",
        "\n",
        "An example of Averaging the Output Vector to aggregate the classifications in the previous example is found below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "864397ad-6c2d-4961-a657-1c92759d7679",
      "metadata": {
        "tags": [],
        "id": "864397ad-6c2d-4961-a657-1c92759d7679"
      },
      "outputs": [],
      "source": [
        "# Collect the different output vectors into a list\n",
        "predicted_output_vectors = [cat_output, cat_ccw_rot_output, cat_cw_rot_output]\n",
        "\n",
        "# Find the average of the output vectors, resulting in a single (1000,) dimensional vector\n",
        "averaged_output_vector = np.mean(predicted_output_vectors, axis=0)\n",
        "print(f'The size of the averaged vector is: {averaged_output_vector.shape}')\n",
        "\n",
        "# Find the class index of the highest average prediction\n",
        "highest_average_class_index = averaged_output_vector.argmax(axis=0)\n",
        "\n",
        "# Print out the class\n",
        "print(f'Class index of highest average: {highest_average_class_index}')\n",
        "print(f'This corresponds to class: {IMAGENET_CLASSES[highest_average_class_index]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f94015c-63d0-4ba9-8fe2-0c0398061814",
      "metadata": {
        "id": "3f94015c-63d0-4ba9-8fe2-0c0398061814"
      },
      "source": [
        "Following this you might see that we get different results based on the way we aggregated the predictions!  \n",
        "\n",
        "**Question**: Give an example of a circumstance where voting and averaging output vectors would give different answers\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "```python\n",
        "pred1 = [0.45, 0.55]\n",
        "pred2 = [0.45, 0.55]\n",
        "pred3 = [0.90, 0.10]\n",
        "    \n",
        "average = [(0.49*2+0.99)/3, (0.51*2+0.01)/3]\n",
        "# [0.6, 0.4]\n",
        "# So voting gives class 1, but averaging gives class 0\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9218da1e-f87c-458e-9ec6-97d7a7b1ad47",
      "metadata": {
        "id": "9218da1e-f87c-458e-9ec6-97d7a7b1ad47"
      },
      "source": [
        "# 2. Evaluation\n",
        "Evaluating how well neural networks perform is an extremely important task. To be able to use a neural network in practice, you need to be confident that the predictions it makes can generalize to new unseen data.\n",
        "\n",
        "In this section we will look at various evaluation metrics we can use to help determine how well out network is performing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67d92690-b0c6-4de9-aee1-330e41ebe721",
      "metadata": {
        "id": "67d92690-b0c6-4de9-aee1-330e41ebe721"
      },
      "source": [
        "## 2.1 Multiclass Classification vs. Binary Classification\n",
        "When performing classification, there are two types of classification tasks we can perform, multiclass or binary.\n",
        "\n",
        "`Multiclass Classification` refers to classification tasks where there are more than 2 classes to choose between.  \n",
        "`Binary Classification` refers to classification tasks where there are exactly 2 classes. Usually, these two classes are referred to as the *positive* and *negative* classes.\n",
        "\n",
        "It is common to see binary classification used on tasks where there are only 2 possible outcomes. For example, predicting whether a patient has a certain disease or not.\n",
        "\n",
        "In this lab we have been performing multiclass classification, given our model can predict 1000 different classes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b80ba6cb-c637-4b19-a427-112ac6cc7eeb",
      "metadata": {
        "id": "b80ba6cb-c637-4b19-a427-112ac6cc7eeb"
      },
      "source": [
        "## 2.2 Data Collection\n",
        "Before computing any evaluation metrics, let's first construct a small dataset that we can evaluate our MobileNet V3 network on and get predictions for each image in our dataset. For this we will use 4x images that belong to 5x different classes (20 images in total).\n",
        "\n",
        "The classes we will choose are: *lion*, *kite*, *magpie*, *toaster*, *eel*.\n",
        "\n",
        "In the code cell below, you'll see a list of `URLS` and `GROUND_TRUTH_LABELS`. These contain the image URLs we will consider as part of our dataset, and their corresponding class index labels.\n",
        "\n",
        "**Task**: Your task is to:\n",
        "* Create an empty list to store the predicted class indexes (called `predicted_labels`)\n",
        "* Loop through the list of urls, and:\n",
        "    * Load the URL into a *numpy* array\n",
        "    * Classify the image, getting back the predicted class index\n",
        "    * Append the predicted class index to the list of `predicted_labels`\n",
        "* **Optional:** If you are interested, you can also call the `display_image()` function to see what the image data looks like and see the ground truth/predicted labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cadfb58d-e861-49f2-a18c-73cb90d450de",
      "metadata": {
        "id": "cadfb58d-e861-49f2-a18c-73cb90d450de"
      },
      "outputs": [],
      "source": [
        "URLS = [\n",
        "    'https://images.unsplash.com/photo-1546182990-dffeafbe841d?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1340&q=80',\n",
        "    'https://images.unsplash.com/photo-1511216113906-8f57bb83e776?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=934&q=80',\n",
        "    'https://images.unsplash.com/photo-1590668468552-d87c3a011afb?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1352&q=80',\n",
        "    'https://images.unsplash.com/photo-1562512619-e5ed0e495c78?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1350&q=80',\n",
        "    'https://images.unsplash.com/photo-1596554817336-19fbecb23705?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=934&q=80',\n",
        "    'https://images.unsplash.com/photo-1604153741124-8e0eb40964ab?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=934&q=80',\n",
        "    'https://images.unsplash.com/photo-1588356294626-5c653a904228?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1350&q=80',\n",
        "    'https://images.unsplash.com/photo-1554234362-59a913f24b78?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1426&q=80',\n",
        "    'https://images.unsplash.com/photo-1598271597568-1df2e4470095?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1350&q=80',\n",
        "    'https://images.unsplash.com/photo-1580637065333-4fd06585fd8a?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1400&q=80',\n",
        "    'https://images.unsplash.com/photo-1586496567894-7d5556870fdf?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1350&q=80',\n",
        "    'https://images.unsplash.com/photo-1610759990825-5db0ab4220b0?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=934&q=80',\n",
        "    'https://images.unsplash.com/photo-1624209190904-aca680ededc1?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1025&q=80',\n",
        "    'https://images.unsplash.com/photo-1613221699807-4940ba9b83f4?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1277&q=80',\n",
        "    'https://images.unsplash.com/photo-1583729250536-d5fb10401671?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1350&q=80',\n",
        "    'https://images.unsplash.com/photo-1618506408870-64d8bec48248?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1350&q=80',\n",
        "    'https://images.unsplash.com/photo-1559897202-7fc939ce9db2?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1286&q=80',\n",
        "    'https://images.unsplash.com/photo-1540253236931-2a77e060b434?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1317&q=80',\n",
        "    'https://images.unsplash.com/photo-1516683169270-7514e272a5fc?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1352&q=80',\n",
        "    'https://images.unsplash.com/photo-1538180476225-ddaa78852f16?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1297&q=80'\n",
        "]\n",
        "GROUND_TRUTH_LABELS = [291, 291, 291, 291, 21, 21, 21, 21, 18, 18, 18, 18, 859, 859, 859, 859, 390, 390, 390, 390]\n",
        "ALL_CLASSES = [291, 21, 18, 859, 390]\n",
        "\n",
        "\n",
        "# TODO: Create an empty list to store the predicted class indexes in\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Loop through the list of URLs and append the predicted class index to the list of predicted class indexes\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "289dbd47-2fb0-48b7-8c7d-f5c8948dcbda",
      "metadata": {
        "id": "289dbd47-2fb0-48b7-8c7d-f5c8948dcbda"
      },
      "source": [
        "How were the predictions? If you also visualized the data you should have a rough idea about how good the model performed."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task solution"
      ],
      "metadata": {
        "id": "282JxNTtlFaM"
      },
      "id": "282JxNTtlFaM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "319660f0-9e48-41d1-89c7-679c17f87be2",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "319660f0-9e48-41d1-89c7-679c17f87be2"
      },
      "outputs": [],
      "source": [
        "URLS = [\n",
        "    'https://images.unsplash.com/photo-1546182990-dffeafbe841d?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1340&q=80',\n",
        "    'https://images.unsplash.com/photo-1511216113906-8f57bb83e776?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=934&q=80',\n",
        "    'https://images.unsplash.com/photo-1590668468552-d87c3a011afb?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1352&q=80',\n",
        "    'https://images.unsplash.com/photo-1562512619-e5ed0e495c78?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1350&q=80',\n",
        "    'https://images.unsplash.com/photo-1596554817336-19fbecb23705?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=934&q=80',\n",
        "    'https://images.unsplash.com/photo-1604153741124-8e0eb40964ab?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=934&q=80',\n",
        "    'https://images.unsplash.com/photo-1588356294626-5c653a904228?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1350&q=80',\n",
        "    'https://images.unsplash.com/photo-1554234362-59a913f24b78?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1426&q=80',\n",
        "    'https://images.unsplash.com/photo-1598271597568-1df2e4470095?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1350&q=80',\n",
        "    'https://images.unsplash.com/photo-1580637065333-4fd06585fd8a?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1400&q=80',\n",
        "    'https://images.unsplash.com/photo-1586496567894-7d5556870fdf?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1350&q=80',\n",
        "    'https://images.unsplash.com/photo-1610759990825-5db0ab4220b0?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=934&q=80',\n",
        "    'https://images.unsplash.com/photo-1624209190904-aca680ededc1?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1025&q=80',\n",
        "    'https://images.unsplash.com/photo-1613221699807-4940ba9b83f4?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1277&q=80',\n",
        "    'https://images.unsplash.com/photo-1583729250536-d5fb10401671?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1350&q=80',\n",
        "    'https://images.unsplash.com/photo-1618506408870-64d8bec48248?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1350&q=80',\n",
        "    'https://images.unsplash.com/photo-1559897202-7fc939ce9db2?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1286&q=80',\n",
        "    'https://images.unsplash.com/photo-1540253236931-2a77e060b434?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1317&q=80',\n",
        "    'https://images.unsplash.com/photo-1516683169270-7514e272a5fc?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1352&q=80',\n",
        "    'https://images.unsplash.com/photo-1538180476225-ddaa78852f16?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1297&q=80'\n",
        "]\n",
        "GROUND_TRUTH_LABELS = [291, 291, 291, 291, 21, 21, 21, 21, 18, 18, 18, 18, 859, 859, 859, 859, 390, 390, 390, 390]\n",
        "ALL_CLASSES = [291, 21, 18, 859, 390]\n",
        "\n",
        "\n",
        "# TODO: Create an empty list to store the predicted class indexes in\n",
        "predicted_labels = []\n",
        "\n",
        "# TODO: Loop through the list of URLs and append the predicted class index to the list of predicted class indexes\n",
        "# NOTE: Here we also loop through the ground truth labels to display the predictions with their\n",
        "#       ground truth and predicted label\n",
        "for url, label in zip(URLS, GROUND_TRUTH_LABELS):\n",
        "    # Load the image\n",
        "    image = load_image_from_url(url)\n",
        "\n",
        "    # Classify the image\n",
        "    _, class_index, class_label = classify_image(mobilenet_v3, image)\n",
        "\n",
        "    # Append the class index to the list of predicted labels\n",
        "    predicted_labels.append(class_index)\n",
        "\n",
        "    # Display the image (Here we show the title and predicted labels)\n",
        "    title = f'Ground Truth: {IMAGENET_CLASSES[label]}\\nPrediction: {class_label}'\n",
        "    display_image(image, title)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5df236d-e81d-41c3-bfb3-33813ddcc1b2",
      "metadata": {
        "id": "f5df236d-e81d-41c3-bfb3-33813ddcc1b2"
      },
      "source": [
        "## 2.3 TP/TN/FP/FN\n",
        "Before jumping into computing metrics for our dataset, let's briefly talk about TP/TN/FP/FN. When discussing TP/TN/FP/FN, we need to introduce the notion of what a 'positive' and 'negative' class means.  \n",
        "\n",
        "In `binary classification` this is quite clear - There are only two classes and we assign one the 'positive' class and the other the 'negative' class.\n",
        "\n",
        "In `multiclass classification` it is not as clear. We have multiple classes, so which one is considered the 'positive' class?  \n",
        "When considering multiclass classification, we look at TP/TN/FP/FN per-class. That is, take turns considering 1 class to be the 'positive' class and the rest to be the 'negative' class, until all classes have been considered the 'positive' class.  \n",
        "\n",
        "As an example: if we have 3 classes: {A, B, C}, then considering class A we would say class A is the 'positive' class and classes B and C are the 'negative' classes, then when considering class B, class B would be considered the 'positive' class and classes A and C the 'negative' classes (and so-on for class C). Another way to think about it is that the 'class of interest' is the 'positive' class.  \n",
        "\n",
        "Descriptions of TP/TN/FP/FN can be found below. Keep in mind for multiclass classification when considering a 'positive' class, we are considering a single class at a time (That is, we find the number of TP/TN/FP/FN for each class in multiclass classification).\n",
        "\n",
        "**True Positive (TP)**  \n",
        "Refers to the number of predictions where the classifier correctly predicts the positive class as positive.\n",
        "\n",
        "**True Negative (TN)**  \n",
        "Refers to the number of predictions where the classifier correctly predicts the negative class as negative.  \n",
        "In the case of multiclass classification, if we have 3 classes: {A, B, C} and class A is considered the 'positive' class, then an example from class B classified as class C is still considered a True Negative (because the classifier correctly predicted the negative class as negative).  \n",
        "\n",
        "**False Positive (FP)**  \n",
        "Refers to the number of predictions where the classifier incorrectly predicts the negative class as positive.\n",
        "\n",
        "**False Negative (FN)**  \n",
        "Refers to the number of predictions where the classifier incorrectly predicts the positive class as negative.\n",
        "\n",
        "We can very quickly visualize this in a confusion matrix, which we can make use of *scikit-learn* to generate for us.\n",
        "\n",
        "**Artificial Dataset** In the code cell below we create an artificial dataset from a 3 class problem. This dataset is simply a list of ground truth 'labels' and predicted 'labels'. We will be using this artificial dataset throughout this section to get experience manually computing some metrics.  \n",
        "\n",
        "In the below code cell we create a confusion matrix with the artificial dataset. Once you've run the code cell, make sure to answer the comprehension questions before moving on. Don't be concerned with the plotting code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b41e25a7-48e2-47aa-94ea-260af4d5f178",
      "metadata": {
        "id": "b41e25a7-48e2-47aa-94ea-260af4d5f178"
      },
      "outputs": [],
      "source": [
        "# Define artificial ground truth and predicted labels\n",
        "#    (there is an uneven amount of ground truth examples per-class)\n",
        "ground_truth = [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2]\n",
        "predictions =  [1, 2, 0, 2, 1, 1, 0, 0, 0, 1, 2, 2]\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_true=ground_truth, y_pred=predictions)\n",
        "\n",
        "# Display the confusion matrix\n",
        "cm_disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "cm_disp = cm_disp.plot(include_values=True, cmap='plasma_r', ax=None, xticks_rotation='horizontal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd669ba7-d7e1-4bad-baad-7cbf8e4bda00",
      "metadata": {
        "id": "cd669ba7-d7e1-4bad-baad-7cbf8e4bda00"
      },
      "source": [
        "### Comprehension Questions\n",
        "\n",
        "**Question 1**\n",
        "\n",
        "Considering class 0 as the 'positive' class, what are the values of TP/TN/FP/FN?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "$TP=1$\n",
        "    \n",
        "    \n",
        "$TN=3$    \n",
        "    \n",
        "    \n",
        "$FP=3$    \n",
        "    \n",
        "    \n",
        "$FN=5$    \n",
        "    \n",
        "    \n",
        "</details>\n",
        "\n",
        "**Question 2**\n",
        "\n",
        "Considering class 1 as the 'positive' class, what are the values of TP/TN/FP/FN?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "$TP=1$\n",
        "    \n",
        "    \n",
        "$TN=5$    \n",
        "    \n",
        "    \n",
        "$FP=3$    \n",
        "    \n",
        "    \n",
        "$FN=3$    \n",
        "    \n",
        "    \n",
        "</details>\n",
        "\n",
        "**Question 3**\n",
        "\n",
        "Considering class 2 as the 'positive' class, what are the values of TP/TN/FP/FN?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "$TP=2$\n",
        "    \n",
        "    \n",
        "$TN=8$    \n",
        "    \n",
        "    \n",
        "$FP=2$    \n",
        "    \n",
        "    \n",
        "$FN=0$    \n",
        "    \n",
        "    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14620746-1fd6-46c0-8ee9-a03c852ea6d9",
      "metadata": {
        "id": "14620746-1fd6-46c0-8ee9-a03c852ea6d9"
      },
      "source": [
        "## 2.4 Metrics\n",
        "Now we have a real and artificial dataset to work with, and we understand TP/TN/FP/FN notation, we can start looking at specific evaluation metrics and calculate them!\n",
        "\n",
        "In each of the following sections, we will show code to compute the metric on our image dataset, and then to test your understanding you will be asked to manually compute the metric on the artificial dataset presented in the TP/TN/FP/FN section above."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c8832fe-d6e2-4277-a2c2-2658783cd674",
      "metadata": {
        "id": "9c8832fe-d6e2-4277-a2c2-2658783cd674"
      },
      "source": [
        "### Accuracy\n",
        "One of the simplest evaluation metrics we can compute is the accuracy. Accuracy tells us overall how often the model is making a correct prediction.  \n",
        "\n",
        "The formula for computing accuracy is shown below:  \n",
        "<center>$accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$</center>\n",
        "\n",
        "To compute the accuracy score, we will use the [*scikit-learn* *`accuracy_score()`* function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) which will tell us the accuracy across our dataset.\n",
        "\n",
        "In the below code cell we compute the accuracy score on our image dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72aefd0e-b11c-4a1d-807a-5f46b08e9bf5",
      "metadata": {
        "id": "72aefd0e-b11c-4a1d-807a-5f46b08e9bf5"
      },
      "outputs": [],
      "source": [
        "dataset_accuracy = accuracy_score(y_true=GROUND_TRUTH_LABELS, y_pred=predicted_labels)\n",
        "print(f'Accuracy: {dataset_accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a4b5af4-709d-4c2e-bb98-8ec23f139c94",
      "metadata": {
        "id": "4a4b5af4-709d-4c2e-bb98-8ec23f139c94"
      },
      "source": [
        "#### Comprehension Questions\n",
        "\n",
        "**Question 1**  \n",
        "What is the accuracy of the artificial dataset described above?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "You can compute the accuracy considering any of the classes as the 'positive' class. This solution shows considering class 0 as the positive class:  \n",
        "\n",
        "<center>$accuracy=\\frac{1 + 3}{1 + 3 + 3 + 5} = \\frac{4}{12} \\approx 0.33 \\space (33\\%)$</center>\n",
        "    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "255b996c-73ed-435c-b52b-5b698fcb8305",
      "metadata": {
        "id": "255b996c-73ed-435c-b52b-5b698fcb8305"
      },
      "source": [
        "### Recall (Sensitivity)\n",
        "Recall is a metric that quantifies the number of correct positive predictions made out of all positive examples. Recall tells you: when the model is presented with a positive example, what is the chance that the model will predict 'positive'? Recall is also referred to as sensitivity.\n",
        "\n",
        "The formula for computing recall is shown below:  \n",
        "<center>$recall = \\frac{TP}{TP + FN}$</center>\n",
        "\n",
        "Given we are dealing multiclass classification, we will generate a recall score *per-class*.\n",
        "\n",
        "To compute the recall score, we will use the [*scikit-learn* *`recall_score()`* function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc9cbcd5-03e7-4746-aa36-5b8536412ea8",
      "metadata": {
        "id": "fc9cbcd5-03e7-4746-aa36-5b8536412ea8"
      },
      "outputs": [],
      "source": [
        "# We pass in the list of imagenet class label indexes to recall_score so it is aware of all classes in our dataset\n",
        "# This will return back a (1000,) dimensional numpy array, with each value corresponding to an ImageNet class\n",
        "# The values in this array correspond to the recall score for that class\n",
        "recall_scores = recall_score(y_true=GROUND_TRUTH_LABELS, y_pred=predicted_labels,\n",
        "                             labels=list(IMAGENET_CLASSES.keys()), average=None, zero_division=0)\n",
        "\n",
        "# We are only interested in seeing the recall score for classes that we have ground truth data for\n",
        "# Here we print out the recall score for each class defined in ALL_CLASSES\n",
        "for class_idx in ALL_CLASSES:\n",
        "    print(f'Class: {IMAGENET_CLASSES[class_idx]}. Recall score: {recall_scores[class_idx]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87950f09-e077-420e-bdad-536296c8b1a9",
      "metadata": {
        "id": "87950f09-e077-420e-bdad-536296c8b1a9"
      },
      "source": [
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Expand for Discussion</u></summary>\n",
        "\n",
        "A few conclusions we can make:\n",
        "\n",
        "Our classifier was unable to correctly predict any of the kite examples. This should be concerning to us if predicting kites was important.    \n",
        "Our classifier was able to correctly predict every eel example.\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fec69938-0463-4be5-9dac-2cb1387d5dca",
      "metadata": {
        "id": "fec69938-0463-4be5-9dac-2cb1387d5dca"
      },
      "source": [
        "#### Comprehension Questions\n",
        "\n",
        "**Question 1**\n",
        "\n",
        "In the artificial dataset described above, considering class 0 as the 'positive' class, what is the recall score for this class?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "<center>$recall = \\frac{1}{1 + 5} = \\frac{1}{6} \\approx 0.17$</center>\n",
        "    \n",
        "</details>\n",
        "\n",
        "**Question 2**\n",
        "\n",
        "In the artificial dataset described above, considering class 1 as the 'positive' class, what is the recall score for this class?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "<center>$recall = \\frac{1}{1 + 3} = \\frac{1}{4} = 0.25$</center>\n",
        "    \n",
        "</details>\n",
        "\n",
        "**Question 3**\n",
        "\n",
        "In the artificial dataset described above, considering class 2 as the 'positive' class, what is the recall score for this class?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "<center>$recall = \\frac{2}{2 + 0} = \\frac{2}{2} = 1$</center>    \n",
        "    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca5bf99e-fa08-41f1-9fd1-2016e0fde2fb",
      "metadata": {
        "id": "ca5bf99e-fa08-41f1-9fd1-2016e0fde2fb"
      },
      "source": [
        "### Precision\n",
        "Precision is a metric that quantifies the number of correct positive predictions made out of all predictions made to the positive class. Precision tells you: when a model has predicted 'positive', what is the chance that you actually have a positive example?\n",
        "\n",
        "The formula for computing precision is shown below:  \n",
        "<center>$precision = \\frac{TP}{TP + FP}$</center>\n",
        "\n",
        "Given we are dealing multiclass classification, we will generate a precision score *per-class*.\n",
        "\n",
        "To compute the precision score, we will use the [*scikit-learn* *`precision_score()`* function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44c92772-18e8-49a4-80e0-652ec133e014",
      "metadata": {
        "id": "44c92772-18e8-49a4-80e0-652ec133e014"
      },
      "outputs": [],
      "source": [
        "# We pass in the list of imagenet class label indexes to precision_score so it is aware of all classes in our dataset\n",
        "# This will return back a (1000,) dimensional numpy array, with each value corresponding to an ImageNet class\n",
        "# The values in this array correspond to the precision score for that class\n",
        "precision_scores = precision_score(y_true=GROUND_TRUTH_LABELS, y_pred=predicted_labels,\n",
        "                                   labels=list(IMAGENET_CLASSES.keys()), average=None, zero_division=0)\n",
        "\n",
        "# We are only interested in seeing the precision score for classes that we have ground truth data for\n",
        "# Here we print out the precision score for each class defined in ALL_CLASSES\n",
        "for class_idx in ALL_CLASSES:\n",
        "    print(f'Class: {IMAGENET_CLASSES[class_idx]}. Precision score: {precision_scores[class_idx]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69c0e0fb-38fd-4b7e-983d-d914b661f3f8",
      "metadata": {
        "id": "69c0e0fb-38fd-4b7e-983d-d914b661f3f8"
      },
      "source": [
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Expand for Discussion</u></summary>\n",
        "\n",
        "The precision scores for this dataset are quite boring, but that is expected.  \n",
        "    \n",
        "For a class to not have a perfect precision score, we would need one of the examples in our dataset to be predicted as one of the other classes we are interested in (This is very unlikely, given that there are 1000 possible classes our model could predict).\n",
        "    \n",
        "The reason we see the *kite* class with a precision score of 0 is because none of the examples were predicted as the *kite* class.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74f8be3a-3de2-45d1-9fa7-09cb45dfd39e",
      "metadata": {
        "id": "74f8be3a-3de2-45d1-9fa7-09cb45dfd39e"
      },
      "source": [
        "#### Comprehension Questions\n",
        "\n",
        "**Question 1**\n",
        "\n",
        "In the artificial dataset described above, considering class 0 as the 'positive' class, what is the precision score for this class?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "<center>$precision = \\frac{1}{1 + 3} = \\frac{1}{4} = 0.25$</center>\n",
        "    \n",
        "</details>\n",
        "\n",
        "**Question 2**\n",
        "\n",
        "In the artificial dataset described above, considering class 1 as the 'positive' class, what is the precision score for this class?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "<center>$precision = \\frac{1}{1 + 3} = \\frac{1}{4} = 0.25$</center>\n",
        "    \n",
        "</details>\n",
        "\n",
        "**Question 3**\n",
        "\n",
        "In the artificial dataset described above, considering class 2 as the 'positive' class, what is the precision score for this class?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "<center>$precision = \\frac{2}{2 + 2} = \\frac{2}{4} = 0.5$</center>    \n",
        "    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46dd4334-bd77-4e54-a564-db37a23ee4e7",
      "metadata": {
        "id": "46dd4334-bd77-4e54-a564-db37a23ee4e7"
      },
      "source": [
        "### F1 Score\n",
        "The F1 score is a metric that is a weighted average of both the precision and recall, with the relative contribution of precision and recall both being equal.\n",
        "\n",
        "The formula for computing the F1 score is shown below:  \n",
        "<center>$F1 \\space score = \\frac{2 \\cdot (precision \\cdot recall)}{precision + recall}$</center>\n",
        "\n",
        "Given we are dealing multiclass classification, we will generate an F1 score score *per-class*.\n",
        "\n",
        "To compute the F1 score, we will use the [*scikit-learn* *`f1_score()`* function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89f0d230-e9de-4d83-b87f-ce023dc1b18e",
      "metadata": {
        "id": "89f0d230-e9de-4d83-b87f-ce023dc1b18e"
      },
      "outputs": [],
      "source": [
        "# We pass in the list of imagenet class label indexes to f1_score so it is aware of all classes in our dataset\n",
        "# This will return back a (1000,) dimensional numpy array, with each value corresponding to an ImageNet class\n",
        "# The values in this array correspond to the F1 score for that class\n",
        "f1_scores = f1_score(y_true=GROUND_TRUTH_LABELS, y_pred=predicted_labels,\n",
        "                     labels=list(IMAGENET_CLASSES.keys()), average=None, zero_division=0)\n",
        "\n",
        "# We are only interested in seeing the F1 score for classes that we have ground truth data for\n",
        "# Here we print out the F1 score for each class defined in ALL_CLASSES\n",
        "for class_idx in ALL_CLASSES:\n",
        "    print(f'Class: {IMAGENET_CLASSES[class_idx]}. F1 score: {f1_scores[class_idx]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3a6c4c1-26f5-45cf-9a78-22682ae07ce9",
      "metadata": {
        "id": "e3a6c4c1-26f5-45cf-9a78-22682ae07ce9"
      },
      "source": [
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Expand for Discussion</u></summary>\n",
        "\n",
        "As we should expect, the F1 score for the eel class is perfect. This was because we saw perfect recall and precision for that class. This shows we can be happy that our classifier is doing a great job at predicting eels and not misclassifying other classes as eels. (With that said, we only have a *very small* amount of data we used for evaluation. To properly validate this claim we would need to evaluate on a larger dataset).  \n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cbfb5f1-595b-484e-9565-1d9b3d43a2ac",
      "metadata": {
        "id": "9cbfb5f1-595b-484e-9565-1d9b3d43a2ac"
      },
      "source": [
        "#### Comprehension Questions\n",
        "\n",
        "**Question 1**\n",
        "\n",
        "In the artificial dataset described above, considering class 0 as the 'positive' class, what is the F1 score for this class?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "<center>$F1 \\space score = \\frac{2 * (0.25 * 0.17)}{0.25 + 0.17} = \\frac{0.085}{0.42} \\approx 0.20$</center>\n",
        "    \n",
        "</details>\n",
        "\n",
        "**Question 2**\n",
        "\n",
        "In the artificial dataset described above, considering class 1 as the 'positive' class, what is the F1 score for this class?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "<center>$F1 \\space score = \\frac{2 * (0.25 * 0.25)}{0.25 + 0.25} = \\frac{0.125}{0.5} = 0.25$</center>\n",
        "    \n",
        "</details>\n",
        "\n",
        "**Question 3**\n",
        "\n",
        "In the artificial dataset described above, considering class 2 as the 'positive' class, what is the F1 score for this class?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "<center>$F1 \\space score = \\frac{2 * (0.5 * 1)}{0.5 + 1} = \\frac{1}{1.5} \\approx 0.67$</center>    \n",
        "    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db9b175e-1613-480a-a7db-5728b0fbc7ba",
      "metadata": {
        "id": "db9b175e-1613-480a-a7db-5728b0fbc7ba"
      },
      "source": [
        "### Metrics on Artificial Dataset\n",
        "You've now seen how we can compute these various metrics using `scikit-learn`. Your task now is to perform the same computations but with the artificial dataset we introduced above.  \n",
        "\n",
        "Given you have answered the comprehension questions, you should be able to validate that your answer matches what `scikit-learn` gives.\n",
        "\n",
        "**Task**: In the code cell below, using the artificial dataset described above:\n",
        "* Compute (and print) the accuracy for the whole artificial dataset\n",
        "* Compute (and print) the recall score, precision score and F1 score per-class\n",
        "\n",
        "**NOTE:** Given our `ground_truth` and `predictions` contain examples from each class, there is no need to pass the `labels` or `zero_division` arguments to any of the *`recall_score()`*, *`precision_score()`* or *`f1_score()`* functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0c7ecbc-163d-4886-bee9-0045b91baacf",
      "metadata": {
        "id": "a0c7ecbc-163d-4886-bee9-0045b91baacf"
      },
      "outputs": [],
      "source": [
        "# TODO: Compute (and print) the accuracy\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Compute (and print) the recall score per-class\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Compute (and print) the precision score per-class\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Compute (and print) the F1 score per-class\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task solution"
      ],
      "metadata": {
        "id": "LmHngJVwlYcI"
      },
      "id": "LmHngJVwlYcI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3f1cc79-df88-4b62-aa4b-01c41d80413a",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "e3f1cc79-df88-4b62-aa4b-01c41d80413a"
      },
      "outputs": [],
      "source": [
        "# TODO: Compute (and print) the accuracy\n",
        "accuracy = accuracy_score(y_true=ground_truth, y_pred=predictions)\n",
        "print(f'Accuracy: {dataset_accuracy * 100:.2f}%')\n",
        "print('-' * 50)\n",
        "\n",
        "# TODO: Compute (and print) the recall score per-class\n",
        "recall_scores = recall_score(y_true=ground_truth, y_pred=predictions, average=None)\n",
        "for class_idx, recall in enumerate(recall_scores):\n",
        "    print(f'Class: {class_idx}. Recall score: {recall}')\n",
        "print('-' * 50)\n",
        "\n",
        "# TODO: Compute (and print) the precision score per-class\n",
        "precision_scores = precision_score(y_true=ground_truth, y_pred=predictions, average=None)\n",
        "for class_idx, precision in enumerate(precision_scores):\n",
        "    print(f'Class: {class_idx}. Precision score: {precision}')\n",
        "print('-' * 50)\n",
        "\n",
        "# TODO: Compute (and print) the F1 score per-class\n",
        "f1_scores = f1_score(y_true=ground_truth, y_pred=predictions, average=None)\n",
        "for class_idx, f1 in enumerate(f1_scores):\n",
        "    print(f'Class: {class_idx}. F1 score: {f1}')\n",
        "print('-' * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6549c2e-4241-47a8-9090-f90896365f6c",
      "metadata": {
        "id": "f6549c2e-4241-47a8-9090-f90896365f6c"
      },
      "source": [
        "# 3. Challenge Tasks\n",
        "These tasks are meant to help pull together everything you have covered in this lab or extend on other exercises previously covered. It is highly recommended that you give these tasks a go, but only try to once you've finished the Lab Exercises section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dbeffb5-becc-4b55-8e6f-b7928a19896d",
      "metadata": {
        "id": "4dbeffb5-becc-4b55-8e6f-b7928a19896d"
      },
      "source": [
        "## Challenge 1 - Comparing Architectures\n",
        "In the evaluation part of this lab we only computed evaluation metrics using the MobileNet V3 network.  \n",
        "\n",
        "Your task is to create a pretrained [`ResNext` network](https://pytorch.org/vision/stable/models.html#id28), evaluate it on the same image data you did in the evaluation section, then compute all metrics that you calculated for MobileNet V3 in the evaluation section.  \n",
        "\n",
        "Once you have the metrics for ResNext, compare them against what you computed for MobileNet V3. Did one perform better than the other?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4366b478-9ec9-4083-98fc-af74e98d9b29",
      "metadata": {
        "id": "4366b478-9ec9-4083-98fc-af74e98d9b29"
      },
      "outputs": [],
      "source": [
        "# TODO: Write your solution here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e02c1b1a-3e79-47e5-9eb0-ee55f5fdc88f",
      "metadata": {
        "id": "e02c1b1a-3e79-47e5-9eb0-ee55f5fdc88f"
      },
      "source": [
        "## Challenge 2 - Trying Other Architectures\n",
        "\n",
        "This is an extension on Challenge question 1. After you have tried out ResNext, take a look at the [torchvision.models documentation](https://pytorch.org/vision/stable/models.html) and choose another network that you might want to try out.   \n",
        "\n",
        "Play around with the different networks, try different input images (from disk or from a URL), evaluate them using the metrics we covered above. Can you identify which network gives you the best results?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4b4d820-e474-4fd9-a06d-d31bc8ad1dc8",
      "metadata": {
        "id": "c4b4d820-e474-4fd9-a06d-d31bc8ad1dc8"
      },
      "outputs": [],
      "source": [
        "# TODO: Write your solution here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58c18baf-1d90-418b-9364-9081480e724f",
      "metadata": {
        "id": "58c18baf-1d90-418b-9364-9081480e724f"
      },
      "source": [
        "# Summary\n",
        "In this lab, we learned how to use a CNN to classify image data using *PyTorch*. We also saw some common evaluation metrics and learned how we can apply them to multiclass classification tasks."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "toc-autonumbering": false,
    "toc-showcode": false,
    "toc-showmarkdowntxt": false,
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1iKZFHNjagLG",
        "7wm-zCX7avMy",
        "RcfRm5mIcqPb",
        "JSt1ymEucCJM",
        "a7g4AzdddXi_",
        "SnHNGI3hdLUj",
        "NQB9fihHdb6S",
        "wXnW1Hjke2N4",
        "hgHaqBlCeSJN",
        "DlitQUQXfKLJ",
        "282JxNTtlFaM",
        "LmHngJVwlYcI"
      ],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}